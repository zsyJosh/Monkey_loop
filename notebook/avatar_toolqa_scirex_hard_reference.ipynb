{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USR_NAME = 'shirwu'\n",
    "TOOL_QA_ROOT = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'hard'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "# dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) \n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    "    max_iters=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Disable all INFO logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Silence all loggers that might be chatty\n",
    "loggers_to_silence = [\n",
    "    \"httpx\",\n",
    "    \"httpcore\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"dspy\",\n",
    "    \"langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"tiktoken\",\n",
    "    \"asyncio\",\n",
    "    \"faiss\",\n",
    "    \"anthropic\"\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_silence:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable tokenizer parallelism warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Which method achieves the highest PCK score on Leeds_Sports_Poses dataset for Pose_Estimation task?', 'answer': 'Pyramid_Residual_Modules__PRMs_'}) (input_keys={'paper_id', 'question'})\n",
      "physics | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APICallMetrics:\n",
    "    timestamp: datetime\n",
    "    tool_name: str\n",
    "    tokens_in: int = 0\n",
    "    tokens_out: int = 0\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AvatarMetrics:\n",
    "    total_calls: int = 0\n",
    "    total_tokens_in: int = 0\n",
    "    total_tokens_out: int = 0\n",
    "    total_execution_time: float = 0.0\n",
    "    calls_by_tool: Dict[str, int] = field(default_factory=dict)\n",
    "    api_call_history: List[APICallMetrics] = field(default_factory=list)\n",
    "    \n",
    "    def add_call(self, metrics: APICallMetrics):\n",
    "        self.total_calls += 1\n",
    "        self.total_tokens_in += metrics.tokens_in\n",
    "        self.total_tokens_out += metrics.tokens_out\n",
    "        self.total_execution_time += metrics.execution_time\n",
    "        self.calls_by_tool[metrics.tool_name] = self.calls_by_tool.get(metrics.tool_name, 0) + 1\n",
    "        self.api_call_history.append(metrics)\n",
    "    \n",
    "    def merge(self, other: 'AvatarMetrics'):\n",
    "        \"\"\"Merge another AvatarMetrics instance into this one\"\"\"\n",
    "        self.total_calls += other.total_calls\n",
    "        self.total_tokens_in += other.total_tokens_in\n",
    "        self.total_tokens_out += other.total_tokens_out\n",
    "        self.total_execution_time += other.total_execution_time\n",
    "        for tool, count in other.calls_by_tool.items():\n",
    "            self.calls_by_tool[tool] = self.calls_by_tool.get(tool, 0) + count\n",
    "        self.api_call_history.extend(other.api_call_history)\n",
    "\n",
    "    def estimate_cost(self, model_name: str = \"gpt-4\") -> float:\n",
    "        pricing = {\n",
    "            \"gpt-4\": {\"input\": 2.5, \"output\": 10.0},\n",
    "        }\n",
    "        if model_name not in pricing:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        rates = pricing[model_name]\n",
    "        input_cost = (self.total_tokens_in / 1000000) * rates[\"input\"]\n",
    "        output_cost = (self.total_tokens_out / 1000000) * rates[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "class AvatarWithMetrics(Avatar):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metrics = AvatarMetrics()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error counting tokens: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _wrapped_tool_call(self, tool, input_text: str) -> str:\n",
    "        start_time = time.time()\n",
    "        tokens_in = self._count_tokens(input_text)\n",
    "        \n",
    "        try:\n",
    "            result = tool.run(input_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tool execution error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            execution_time = time.time() - start_time\n",
    "            tokens_out = self._count_tokens(str(result))\n",
    "            \n",
    "            metrics = APICallMetrics(\n",
    "                timestamp=datetime.now(),\n",
    "                tool_name=tool.name,\n",
    "                tokens_in=tokens_in,\n",
    "                tokens_out=tokens_out,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self.metrics.add_call(metrics)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        metrics = APICallMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            tool_name=\"main_llm\",\n",
    "            tokens_in=self._count_tokens(str(args) + str(kwargs)),\n",
    "            tokens_out=self._count_tokens(str(result)),\n",
    "            execution_time=total_time\n",
    "        )\n",
    "        self.metrics.add_call(metrics)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for example in test_set:\n",
    "            def process_with_metrics(example=example):\n",
    "                try:\n",
    "                    avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "                    prediction = avatar(**example.inputs().toDict())\n",
    "                    return metric(example, prediction), avatar.metrics\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return 0, AvatarMetrics()\n",
    "\n",
    "            futures.append(executor.submit(process_with_metrics))\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            score, metrics = future.result()\n",
    "            total_score += score\n",
    "            # Only combine token counts and call counts, not execution times\n",
    "            combined_metrics.total_calls += metrics.total_calls\n",
    "            combined_metrics.total_tokens_in += metrics.total_tokens_in\n",
    "            combined_metrics.total_tokens_out += metrics.total_tokens_out\n",
    "            for tool, count in metrics.calls_by_tool.items():\n",
    "                combined_metrics.calls_by_tool[tool] = combined_metrics.calls_by_tool.get(tool, 0) + count\n",
    "            combined_metrics.api_call_history.extend(metrics.api_call_history)\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    combined_metrics.total_execution_time = total_execution_time\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        try:\n",
    "            avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "            prediction = avatar(**example.inputs().toDict())\n",
    "            score = metric(example, prediction)\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in avatar.metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def format_metrics_report(metrics: AvatarMetrics, model_name: str = \"gpt-4\") -> str:\n",
    "    cost = metrics.estimate_cost(model_name)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Avatar Execution Metrics Report\n",
    "==============================\n",
    "Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "Total API Calls: {metrics.total_calls}\n",
    "Total Tokens: {metrics.total_tokens_in + metrics.total_tokens_out:,} ({metrics.total_tokens_in:,} in, {metrics.total_tokens_out:,} out)\n",
    "Estimated Cost: ${cost:.4f}\n",
    "\n",
    "Average Time per Call: {metrics.total_execution_time / metrics.total_calls:.2f} seconds\n",
    "\n",
    "Tool Usage Breakdown:\n",
    "-------------------\n",
    "\"\"\"\n",
    "    for tool, count in sorted(metrics.calls_by_tool.items()):\n",
    "        report += f\"{tool}: {count} calls\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, as mentioned in the context of various research papers discussing reinforcement learning methods applied to Atari games. | Atari_2600_Chopper_Command => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The method EASE achieves the highest Recall@50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "LiteFlowNet achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The Paragraph_vector method for the Question Answering task has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The current state-of-the-art method achieving the highest Mean_IoU score on the CamVid dataset for Semantic Segmentation is SERNet-Former, although specific Mean_IoU scores were not found in the search results. | PSPNet => 0.0\n",
      "The search did not yield specific datasets for the Deep_Speech method evaluation in Speech Recognition. Further detailed search or specific papers might be needed to find this information. | Switchboard___Hub500 => 0.0\n",
      "The method that achieves the highest Score score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes a variety of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The method that achieves the highest error score on the Yelp Binary classification dataset for the Sentiment Analysis task is a shallow-and-wide network with word inputs, which outperforms deep models such as DenseNet. | Char-level_CNN => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [01:00<59:12, 60.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current state-of-the-art on Atari 2600 Name This Game is MuZero. | IQN => 0.0\n",
      "The method achieving the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism for regularizing neural language models, which achieved a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB+Fisher+CH is evaluated on the Switchboard and Hub500 datasets for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The specific evaluation metrics for the Ann_PAT_MT method on the CoNLL-2014_A2 dataset for Grammatical Error Detection were not found in the available resources. It may require accessing specific research papers or datasets that detail this method's evaluation. | F0_5 => 0.0\n",
      "The RNN (Featured) model achieves the highest Train Accuracy score of 96.52% on the SNLI dataset for the Natural Language Inference task. | __Unigram_and_bigram_features => 0.0\n",
      "The current state-of-the-art method on the SNLI dataset for Natural Language Inference is Neural Tree Indexers for Text Understanding. However, specific details about the highest Parameters score are not readily available from the search results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and IFC (Information Fidelity Criterion). | PSNR, SSIM => 0.5\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The FDNet method is evaluated on the WIDER_FACE Easy dataset for the Face Detection task using metrics such as precision and recall, as indicated by its performance on the validation set where it achieved 95.9% on the easy set. | AP => 0.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Transformer method is evaluated on the WMT 2014 English-to-German and English-to-French translation tasks for the Machine Translation task. | IWSLT2015_English-German => 0.0\n",
      "The SVDCNN method for text classification is evaluated on several large-scale datasets, including AG's News Corpus, Sogou News Corpus, DBPedia Ontology Dataset, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both human and noop start settings. | Atari_2600_River_Raid => 0.0\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using the area under the PCK-over-alpha curve as a function of the number of training annotations. | Mean_PCK => 0.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in this method is shown to improve performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The available searches did not provide specific information about the dataset on which the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further detailed research or access to specific papers or datasets might be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The Snips method is evaluated on the TIMIT dataset for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The DPN-131 method is evaluated on several datasets for the Image Classification task, including the OSIE dataset, RVL-CDIP dataset, Tobacco-3482 dataset, and Places365-Standard dataset. | ImageNet => 0.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is referred to as 'accuracy@', where a pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The available resources did not provide the specific dataset on which the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further detailed research or access to specific experimental results might be required to obtain this information. | Atari_2600_Video_Pinball => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question_Answering task using accuracy as the primary metric. The performance is measured by the proportion of test cases where the ground truth is among the top answers proposed by the model. | CNN, Daily_Mail => 0.5\n",
      "The CRN method for the Image-to-Image Translation task does not have specific datasets mentioned in the retrieved documents. Further specific information about datasets used for CRN in this context might not be readily available in the sources accessed. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as Exact Match (EM) and F1 score. | MAP, MRR => 0.0\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image Generation task were not directly found in the retrieved documents. However, common metrics for evaluating image generation tasks on datasets like CIFAR-10 include Inception Score (IS) and Fréchet Inception Distance (FID). These metrics assess the quality and diversity of generated images. | NLL_Test => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as accuracy and boundary F1-measure (BF). These metrics are used to assess the performance of segmentation architectures, focusing on both region accuracies and boundary precision. | Mean_IoU => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII dataset. | FLIC_Elbows => 0.0\n",
      "The DRCN method is evaluated on the Set5 dataset for 4x upscaling using metrics such as PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). | MOS, PSNR, SSIM => 0.67\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task were not found in the available resources. It is recommended to consult the original research paper or supplementary materials for specific evaluation metrics. | Score => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the performance of the model in terms of semantic segmentation and pixel-level adaptation. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as Set5 and SuperTexture, as mentioned in the retrieved results. However, specific datasets for video super-resolution were not explicitly mentioned in the results. The SRCNN method is generally used for single image super-resolution, and its application to video super-resolution may involve similar datasets used for image super-resolution tasks. | Vid4_-_4x_upscaling => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:47<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method evaluation metrics on the Bing_News dataset for the Click-Through Rate Prediction task are not explicitly mentioned in the retrieved documents. Further specific details might be found in the original research paper or supplementary materials related to the PNN method. | AUC, Log_Loss => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score, metrics = multi_thread_executor(toolqa_test, ToolQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.20\n",
      "\n",
      "Avatar Execution Metrics Report\n",
      "==============================\n",
      "Execution Time: 108.61 seconds\n",
      "Total API Calls: 60\n",
      "Total Tokens: 92,638 (1,702 in, 90,936 out)\n",
      "Estimated Cost: $0.9136\n",
      "\n",
      "Average Time per Call: 1.81 seconds\n",
      "\n",
      "Tool Usage Breakdown:\n",
      "-------------------\n",
      "main_llm: 60 calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average Score on ArxivQA before opitmization: {aqa_score:.2f}\")\n",
    "print(f\"Test Score: {score:.2f}\")\n",
    "print(format_metrics_report(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "iterative_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=2,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:09<06:07,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 87.0 F1, achieved by the span-based model presented in the paper \"A Span Selection Model for Semantic Role Labeling\" by Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. | Li_et_al_ => 0.0\n",
      "The IQN method is evaluated on 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The A3C-CTS method is evaluated on the whole Atari 2600 suite, including Montezuma's Revenge and Bellemare et al.'s set of hard exploration games with sparse rewards. | Atari_2600_Venture => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The method PTSR (Patch Translator for Image Super-Resolution) achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models. | PFF => 0.0\n",
      "The Bi-LSTM trained on FCE method achieves the highest F0.5 score on the FCE dataset for the Grammatical Error Detection task, as indicated by the result from the paper by Masahiro Kaneko and Mamoru Komachi. | CoNLL-2014_A2 => 0.0\n",
      "The X-Transformer obtained the highest BLEU score of 46.63 on the WMT 2014 English-German dataset for Machine Translation. | Weighted_Transformer__large_ => 0.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the retrieved information. The available papers discuss DCCL in the context of Generalized Category Discovery and Unsupervised Domain Adaptation, but not specifically for Machine Translation. | IWSLT2015_German-English => 0.0\n",
      "The method that achieves the highest MRR score on the FB15k dataset for the Link Prediction task is AutoKGE with an MRR of 0.861. | TuckER => 0.0\n",
      "The Frustum_PointNets method is evaluated on the KITTI dataset for the Object_Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The highest Validation_perplexity score on the Penn Treebank Word Level dataset for Language Modelling is achieved by OpenAI's GPT-3 with a word-level perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on the CoNLL_2005 dataset, with scores above 97 F1. | CoNLL_2005 => 1.0\n",
      "The method that achieves the highest Score score on the Atari_2600_Road_Runner dataset for the Atari_Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 metric. | F1 => 1.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset for the Weakly Supervised Object Detection task using the evaluation metrics of Average Precision (AP) and mean Average Precision (mAP), following the standard PASCAL VOC protocol. | MAP => 1.0\n",
      "The method achieving the highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is not clearly identified in the available resources. The Renovating Parsing R-CNN is a notable method, but specific AP_0_5 scores on this dataset are not provided in the search results. | NAN => 1.0\n",
      "The IDE____CamStyle method is evaluated on the PRID2011, iLIDS-VID, and VIPeR datasets for the Person Re-Identification task. | DukeMTMC-reID => 0.0\n",
      "The Transformer method is evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using the BLEU metric. The evaluation reports tokenized BLEU using the \"multi-bleu.perl\" script. | BLEU_score => 1.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric. | Percentage_error => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:52<18:21, 28.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Ms__Pacman => 0.0\n",
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Time_Pilot => 0.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. | F-Measure => 0.5\n",
      "The TARNet method is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both a randomized and a non-randomized component, for the Causal Inference task. | IDHP => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  10%|█         | 4/40 [00:53<06:47, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__noop method is evaluated on 57 Atari games. | Atari_2600_Berzerk => 0.0\n",
      "The DeepFM method achieves the highest Log_Loss score for the Click-Through Rate Prediction task on the Criteo dataset. The Criteo dataset is a well-known ad tech industry benchmarking dataset used for evaluating CTR prediction models. | Criteo => 1.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using the NDCG@100 metric for the Collaborative Filtering task. | Recall_20, Recall_50 => 0.0\n",
      "The BiDAF___Self_Attention__single_model_ method is evaluated on the SQuAD and CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n",
      "CornerNet-Squeeze is evaluated on the COCO dataset for the Real-Time_Object_Detection task. | COCO => 1.0\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on the miniImageNet and Fewshot-CIFAR100 (FC100) datasets. | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The PFF method for Image Super-Resolution is evaluated on the Set5 and Set14 datasets. | Set14_-_4x_upscaling => 0.5\n",
      "The ResNet_ELU method is evaluated on the CIFAR-100 dataset using the test error percentage as a metric. The ELU networks achieved a test error of 24.28%, which is among the best results reported for CIFAR-100. | Percentage_correct => 0.5\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains, and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The LapSRN method is evaluated on the Urban100 - 4x upscaling dataset using two evaluation metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). | PSNR => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  12%|█▎        | 5/40 [01:00<05:47,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for the Atari Games task is GDI-H3 with a score of 9620.33%. | Ape-X => 0.0\n",
      "The Duel_hs method is evaluated on the 57 Atari games dataset, which includes a variety of games used for benchmarking in reinforcement learning research. | Atari_2600_Video_Pinball => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using metrics such as Accuracy and F1 score. | Matched, Mismatched => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  18%|█▊        | 7/40 [01:05<03:32,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__hs method is evaluated on the Atari 2600 games dataset for the Atari_Games task. | Atari_2600_Assault => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:21<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MTGAE method evaluation metrics on the Pubmed dataset for the Link_Prediction task are not explicitly found in the available resources. It is recommended to refer to the original research paper or supplementary materials for detailed evaluation metrics. | Accuracy => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.2875\n",
      "Generated new instruction: New Instruction: \n",
      "\n",
      "To effectively accomplish the `Goal` using the provided `Tools`, begin by carefully analyzing the user query to determine the most suitable tool for the task. Retain the flexibility to use no tools if the answer can be directly provided. When selecting a tool, prioritize `ARXIV_SEARCH` or `RETRIEVE` for queries that involve specific datasets or evaluation metrics, as these tools are more aligned with academic and structured data retrieval. For broader queries, where information might be dispersed across various sources, consider using `WEB_SEARCH`. Ensure that the input queries to these tools are well-constructed, incorporating key terms and context to enhance specificity and relevance.\n",
      "\n",
      "Refine your computational logic by ensuring that the tool selection is aligned with the nature of the query. For instance, if the query involves academic or technical information, `ARXIV_SEARCH` should be prioritized. Construct input queries with precision, avoiding vague or overly broad terms. Break down complex queries into simpler, more focused components that can be effectively handled by the tools. This approach will help in extracting the needed information more accurately and efficiently.\n",
      "\n",
      "Adopt an iterative approach to tool usage. After receiving initial outputs from the selected tool, review the results to assess their relevance and completeness. If necessary, refine the queries based on the initial findings and conduct subsequent searches to ensure a more targeted and successful retrieval of information. This iterative process will help in addressing any gaps in the initial results and improve the overall performance on negative inputs, leading to more consistent and accurate outcomes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ByteNet method is evaluated on the English-to-German WMT translation task for the Machine Translation task. | WMT2014_English-French => 0.0\n",
      "The IQN method is evaluated on the 57 Atari 2600 games in the ALE (Arcade Learning Environment). | Atari_2600_Kung-Fu_Master => 0.0\n",
      "The method achieving the highest validation perplexity score on the Penn Treebank Word Level dataset for language modeling is Dynamic Evaluation, with a perplexity of 51.1. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The BiDAF Self Attention single model method is evaluated on the Stanford Question Answering Dataset (SQuAD). | SQuAD1_1 => 0.5\n",
      "The method achieving the highest F1 score on the OntoNotes dataset for the Semantic Role Labeling task is HeSyFu with an F1 score of 88.59. | Li_et_al_ => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "Frustum_PointNets is evaluated on the KITTI and SUN RGB-D 3D detection benchmarks for the Object Localization task. | KITTI_Cars_Hard => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:26<17:12, 26.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The method PTSR: Patch Translator for Image Super-Resolution achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models. | PFF => 0.0\n",
      "The method GDI-H3 achieves the highest score on the Atari 2600 Road Runner dataset for the Atari Games task, with a score of 999999. | Duel_noop => 0.0\n",
      "The MTGAE method is evaluated on the MRR (Mean Reciprocal Rank) metric for the Pubmed dataset in the Link Prediction task. | Accuracy => 0.0\n",
      "The OICR-Ens___FRCNN method for Weakly Supervised Object Detection on the PASCAL VOC 2012 dataset is typically evaluated using the mean Average Precision (mAP) metric. | MAP => 1.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using metrics such as Precision, Recall, and F-measure. | F-Measure => 0.5\n",
      "The DeepFM method achieves the highest Log_Loss score for Click-Through Rate Prediction on the Criteo dataset. | Criteo => 1.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task based on the available information. | IWSLT2015_German-English => 0.0\n",
      "TARNet is evaluated on synthetic and real-world datasets for causal inference tasks. | IDHP => 0.0\n",
      "The highest BLEU score achieved on the WMT2014 English-German translation task is 28.4. | Weighted_Transformer__large_ => 0.0\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 score as the main evaluation metric. | F1 => 1.0\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains, and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The Transformer method is typically evaluated using metrics such as BLEU and METEOR on the IWSLT2015 German-English dataset for the Machine Translation task. | BLEU_score => 0.5\n",
      "The LapSRN method is evaluated on the Urban100 - 4x upscaling dataset using the metrics PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). | PSNR => 0.5\n",
      "The Bi-LSTM trained on the FCE dataset achieves the highest F0.5 score for the Grammatical Error Detection task on the FCE dataset itself. | CoNLL-2014_A2 => 0.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on the CoNLL-2005 dataset, with scores above 97 F1 on both in-domain datasets. | CoNLL_2005 => 1.0\n",
      "CornerNet-Squeeze is evaluated on the MS COCO dataset for Real-Time Object Detection tasks. | COCO => 1.0\n",
      "The DDQN__tuned__noop method's evaluation datasets for the Atari Games task could not be found in the available resources. It is recommended to check specific research papers or repositories related to DDQN and Atari Games for detailed information. | Atari_2600_Berzerk => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric, specifically on the matched and mismatched datasets. | Matched, Mismatched => 1.0\n",
      "The specific evaluation metrics for the ResNet_ELU method on the CIFAR-100 dataset for image classification were not found in the available resources. Common metrics for image classification tasks on CIFAR-100 typically include accuracy, precision, recall, and F1-score, but specific details for ResNet_ELU were not retrieved. | Percentage_correct => 0.0\n",
      "The specific datasets used for evaluating the Duel_noop method on the Atari Games task were not found in the search results. It appears that the information is not readily available in the sources accessed. | Atari_2600_Ms__Pacman => 0.0\n",
      "The datasets on which the PFF method is evaluated for the Image Super-Resolution task could not be found in the available resources. | Set14_-_4x_upscaling => 0.0\n",
      "The highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is achieved by the NAN method with a score of 59.70%.I couldn't find specific datasets on which the Sample_Clustering method is evaluated for the Few-Shot Image Classification task. | CUB-200_-_0-Shot_Learning => 0.0\n",
      " | NAN => 0.0\n",
      "The IDE____CamStyle method for Person Re-Identification is evaluated on the Market-1501 and DukeMTMC-reID datasets. | DukeMTMC-reID => 0.5\n",
      "The A3C-CTS method is evaluated on Atari 2600 games, including Beamrider, Breakout, Pong, Q*bert, and Space Invaders. | Atari_2600_Venture => 0.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated using the Word Error Rate (WER) metric on the swb_hub_500_WER_fullSWBCH dataset for the Speech Recognition task. | Percentage_error => 1.0\n",
      "The DDQN__tuned__hs method is evaluated on datasets from the Atari 2600 games, which typically include a variety of games from the Atari Learning Environment (ALE). However, specific datasets or games for the DDQN__tuned__hs method were not explicitly identified in the search results. | Atari_2600_Assault => 0.0\n",
      "The Duel_hs method evaluation datasets for the Atari Games task were not found in the search results. | Atari_2600_Video_Pinball => 0.0\n",
      "The Mult-DAE method is typically evaluated using metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) on collaborative filtering tasks, including those involving the Netflix dataset. However, specific details on the evaluation metrics for Mult-DAE on the Netflix dataset were not found in the search results. | Recall_20, Recall_50 => 0.0\n",
      "The method achieving the highest MRR score on the FB15k dataset for the Link Prediction task is ANALOGY, with an MRR of 0.725. | TuckER => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [01:02<20:14, 31.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on the Atari 2600 Games task, which involves training an agent to achieve high game scores across various Atari games. However, specific datasets or games used for evaluation were not explicitly mentioned in the search results. | Atari_2600_Time_Pilot => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:33<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method achieving the highest Medium Human-Normalized Score on the Atari-57 dataset is not clearly identified in the available data. The search results frequently mention Agent57 as a significant performer, but specific details about the highest Medium Human-Normalized Score are missing. | Ape-X => 0.0\n",
      "Average Score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new instruction: New Instruction: To effectively accomplish the `Goal` using the provided `Tools`, begin by carefully analyzing the user query to determine the most suitable tool for the task. Retain the flexibility to use no tools if the answer can be directly provided. When selecting a tool, prioritize `ARXIV_SEARCH` or `RETRIEVE` for queries that involve specific datasets or evaluation metrics, as these tools are more aligned with academic and structured data retrieval. For broader queries, where information might be dispersed across various sources, consider using `WEB_SEARCH`. Ensure that the input queries to these tools are well-constructed, incorporating key terms and context to enhance specificity and relevance. Refine your computational logic by ensuring that the tool selection is aligned with the nature of the query. For instance, if the query involves academic or technical information, `ARXIV_SEARCH` should be prioritized. Construct input queries with precision, avoiding vague or overly broad terms. Break down complex queries into simpler, more focused components that can be effectively handled by the tools. This approach will help in extracting the needed information more accurately and efficiently.\n",
      "\n",
      "To improve performance on negative inputs, focus on enhancing pattern recognition and contrast. Successful queries often involve specific datasets and evaluation metrics, with tool selection aligning well with the query's nature. For negative inputs, ensure that the tool selection is consistently aligned with the query's nature. For academic or technical queries, prioritize `ARXIV_SEARCH` or `RETRIEVE`. Construct input queries with precision, incorporating key terms and context to enhance specificity and relevance. Break down complex queries into simpler components that can be effectively handled by the tools. This will help in addressing any gaps in the initial results and improve the overall performance on negative inputs.\n",
      "\n",
      "Adopt an iterative approach to tool usage. After receiving initial outputs from the selected tool, review the results to assess their relevance and completeness. If necessary, refine the queries based on the initial findings and conduct subsequent searches to ensure a more targeted and successful retrieval of information. By implementing these strategies, the performance on negative inputs can be improved, leading to more consistent and accurate outcomes.\n",
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 33.34 seconds\n",
      "                Total API Calls: 4\n",
      "                - Comparator calls: 2\n",
      "                - Feedback instruction calls: 2\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 92,386\n",
      "                - Input tokens: 90,848\n",
      "                - Output tokens: 1,538\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $2.8177\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "result = iterative_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization cost: $2.8177\n",
      "Final score achieved: 0.287\n",
      "\n",
      "Iteration 0:\n",
      "Score: 0.287\n",
      "Comparator tokens in: 26861\n",
      "Comparator tokens out: 466\n",
      "Feedback tokens in: 606\n",
      "Feedback tokens out: 308\n",
      "Execution time: 98.40s\n",
      "\n",
      "Iteration 1:\n",
      "Score: 0.250\n",
      "Comparator tokens in: 62703\n",
      "Comparator tokens out: 337\n",
      "Feedback tokens in: 678\n",
      "Feedback tokens out: 427\n",
      "Execution time: 111.64s\n"
     ]
    }
   ],
   "source": [
    "optimized_actor_agent = result[\"agent\"]\n",
    "optimization_metrics = result[\"metrics\"]\n",
    "\n",
    "# Now you can process the metrics\n",
    "print(f\"Total optimization cost: ${optimization_metrics['total_cost']:.4f}\")\n",
    "print(f\"Final score achieved: {optimization_metrics['final_score']:.3f}\")\n",
    "\n",
    "# Analyze per-iteration performance\n",
    "for iteration in optimization_metrics['iteration_details']:\n",
    "    print(f\"\\nIteration {iteration['iteration']}:\")\n",
    "    print(f\"Score: {iteration['score']:.3f}\")\n",
    "    print(f\"Comparator tokens in: {iteration['comparator_metrics']['tokens_in']}\")\n",
    "    print(f\"Comparator tokens out: {iteration['comparator_metrics']['tokens_out']}\")\n",
    "    print(f\"Feedback tokens in: {iteration['feedback_metrics']['tokens_in']}\")\n",
    "    print(f\"Feedback tokens out: {iteration['feedback_metrics']['tokens_out']}\")\n",
    "    print(f\"Execution time: {iteration['execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U-Net method for skin cancer segmentation is evaluated on benchmark datasets such as the ISIC-2017 and ISIC-2018 datasets. | Kaggle_Skin_Lesion_Segmentation => 0.5\n",
      "The TANDA method achieves the highest MAP score on the WikiQA dataset for the Question Answering task, with a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The shallow word model achieves the highest performance on the Yelp Binary classification dataset with an accuracy of 95.9%. | Char-level_CNN => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies for the node classification task on the Cora dataset. | GCN => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, with a MAE of 3.71. | 3DDFA => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:38<37:47, 38.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task with a score of 157177.85. | IQN => 0.0\n",
      "The FRCN (Fast Region-based Convolutional Networks) method for object detection is evaluated on datasets such as PASCAL VOC and Microsoft COCO. | PASCAL_VOC_2007 => 0.5\n",
      "The MemNNs ensemble method is evaluated on the bAbI and NLVR datasets for the Question Answering task. | CNN___Daily_Mail => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using geometric error metrics, which measure the difference between reconstructed meshes and the ground truth 3D scans. | Mean_NME_ => 0.0\n",
      "The IQN method achieves the highest Score score on the baseline dataset of 57 Atari 2600 games in the ALE. | Atari_2600_Atlantis => 0.0\n",
      "MuZero achieves the highest score on the Atari 2600 Robotank dataset for the Atari Games task with a score of 131.13. | Bootstrapped_DQN => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:46<19:41, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The PNN method is evaluated using the AUC (Area Under the ROC Curve) and logloss metrics on the Bing News dataset for the Click-Through Rate Prediction task. | AUC, Log_Loss => 1.0\n",
      "The specific evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found in the available resources. | Score => 0.0\n",
      "SERNet-Former achieves the highest Mean IoU score of 84.62% on the CamVid dataset for the Semantic Segmentation task. | PSPNet => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest accuracy on the CIFAR-100 dataset for image classification with an accuracy of 96.08%. | Res2NeXt-29 => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | CIFAR-10 => 0.5\n",
      "The Deep Speech method for speech recognition is commonly evaluated on datasets such as LibriSpeech and TIMIT Acoustic-Phonetic Continuous Speech Corpus. | Switchboard___Hub500 => 0.0\n",
      "I couldn't find specific datasets on which the CRN method is evaluated for the Image-to-Image Translation task. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The FDNet method evaluation metrics on the WIDER Face Easy dataset for the Face Detection task are not explicitly found in the available resources. Further specific details might be available in the original FDNet publication or related technical documentation. | AP => 0.0\n",
      "The SRCNN method is evaluated on the Manga109 - 4x upscaling dataset using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics. | PSNR, SSIM => 1.0\n",
      "The S-Norm method's evaluation datasets for the Question Answering task were not found in the available search results. | TriviaQA => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA Fall-to-Winter dataset for the Image-to-Image Translation task using the mean intersection over union (mIoU) metric. | Per-pixel_Accuracy, fwIOU, mIoU => 0.33\n",
      "The DQN_hs method is evaluated on the Atari 2600 games dataset. | Atari_2600_Chopper_Command => 0.5\n",
      "The IDE+CamStyle+Random Erasing method for Person Re-Identification is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets. | Market-1501 => 0.5\n",
      "The Inception_V2 model is typically evaluated on the ImageNet dataset using standard image classification metrics such as accuracy, precision, recall, and F1-score. However, specific metrics for Inception_V2 on ImageNet were not found in the search results. | Top_1_Accuracy, Top_5_Accuracy => 0.0\n",
      "The specific dataset on which the SVDCNN method achieves the highest error score for the Sentiment Analysis task was not found in the available search results. | Yelp_Fine-grained_classification => 0.0\n",
      "The current state-of-the-art on WikiText-2 for language modeling, in terms of the number of parameters, is SparseGPT with 175 billion parameters and 50% sparsity. | AWD-LSTM-DOC => 0.0\n",
      "The SVDCNN method for text classification was evaluated on datasets such as AG News and Yelp reviews, as inferred from the context of related text classification evaluations. | AG_News => 0.5\n",
      "The NICE method for image generation on the CIFAR-10 dataset is typically evaluated using metrics such as the Inception Score (IS) and the Fréchet Inception Distance (FID). These metrics assess the quality and diversity of the generated images. | NLL_Test => 0.0\n",
      "The Transformer method for the Machine Translation task has been evaluated on several datasets, including WMT'14, WMT'17, and various domain-specific datasets such as those used in the AdapMT Shared Task ICON 2020 for English-Hindi translation. Additionally, Persian-English parallel corpora have been used for evaluation. | IWSLT2015_English-German => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and F1 score. However, specific evaluation metrics for the Impatient_Reader method were not found in the search results. | CNN, Daily_Mail => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on datasets such as the Fluent Speech Commands and Snips SmartLights datasets, as well as the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The Paragraph Vector method has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA for the Question Answering task. | WikiQA => 0.5\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | STL-10 => 0.5\n",
      "The AWD-LSTM-DOC method is typically evaluated using the perplexity metric on the WikiText-2 dataset for the Language Modelling task. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The dataset on which the Stacked Hourglass Networks method achieves the highest PCK_0_2 score for the Pose Estimation task is not explicitly mentioned in the available search results. | FLIC_Elbows => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset for the Face Detection task. | WIDER_Face__Easy_ => 0.0\n",
      "The current state-of-the-art on the SNLI dataset for Natural Language Inference is not clearly identified in terms of the highest Parameters score from the available search results. The searches did not yield specific information about the highest Parameters score for 2023. Further detailed research in academic papers or specific repositories like 'Papers with Code' might be necessary to find the exact model and its parameters. | 300D_Residual_stacked_encoders => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The DPN-131 method is evaluated on the ImageNet-1k dataset for the Image Classification task. | ImageNet => 1.0\n",
      "The DQN_noop method is typically evaluated on the Atari 2600 games, which include a standard set of 57 games in the Arcade Learning Environment (ALE). | Atari_2600_River_Raid => 0.5\n",
      "The DeepLab-LargeFOV method is typically evaluated using metrics such as mean Intersection over Union (mIoU), accuracy, precision, and recall for the Scene Segmentation task on the SUN-RGBD dataset. However, specific metrics for DeepLab-LargeFOV on this dataset were not found in the search results. | Mean_IoU => 0.5\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using metrics related to matching accuracy and possibly other standard metrics for correspondence estimation, but specific metrics were not found in the search results. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [01:18<24:25, 25.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spynet method for Optical Flow Estimation is evaluated on the MPI-Sintel and KITTI datasets. | Sintel-final => 0.5\n",
      "The highest Train_Accuracy score on the SNLI dataset for the Natural Language Inference task is not explicitly available in the current search results. However, state-of-the-art models like Neural Tree Indexers and DR-BiLSTM have been mentioned in various sources. For the most accurate and up-to-date information, checking the latest publications or repositories like Papers With Code might be necessary. | __Unigram_and_bigram_features => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [01:19<14:50, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model_trained_on_SWB_Fisher_CH__N-gram___RNNLM_language_model_trained_on_Switchboard_Fisher_Gigaword_Broadcast method is evaluated on the Switchboard and CallHome portions of the NIST 2000 evaluation set for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO Visual Question Answering (VQA) dataset. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the retrieved documents. However, the CoNLL-2014 shared task typically uses metrics like precision, recall, and F0.5 score for evaluation. | F0_5 => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score on the Sintel final pass dataset for the Optical Flow Estimation task. | Sintel-final => 1.0\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, ROUGE, and PARENT. | BLEU, ROUGE => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  23%|██▎       | 14/60 [01:25<02:13,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using metrics such as detection accuracy and regression loss. These metrics assess the precision of keypoint localization and the effectiveness of the ConvNet features in performing tasks that require correspondence and fine localization. | Mean_PCK => 0.0\n",
      "The SRCNN method for video super-resolution is typically evaluated on datasets such as Set5, which is commonly used for image super-resolution tasks. However, specific datasets for video super-resolution using SRCNN are not clearly identified in the available resources. | Vid4_-_4x_upscaling => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  35%|███▌      | 21/60 [01:25<01:01,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific evaluation metrics for the Paragraph_vector__lexical_overlap___dist_output_ method on the QASent dataset for the Question Answering task could not be found in the available resources. | MAP, MRR => 0.0\n",
      "Bootstrapped DQN is evaluated on the Atari benchmark, which includes a diverse selection of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The highest Recall_50 score for the Collaborative Filtering task on the Million Song Dataset is not explicitly available in the retrieved data. The search did not yield specific results for Recall_50 scores, indicating that this metric might not be commonly reported for this dataset or task. | Mult-VAE_PR => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is typically evaluated on datasets like CIFAR-10, CIFAR-100, and SVHN. These datasets are commonly used in the field for evaluating semi-supervised learning methods. | CIFAR-10__4000_Labels => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  67%|██████▋   | 40/60 [01:33<00:16,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task on the Atari_2600_Video_Pinball dataset. | Atari_2600_Video_Pinball => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:39<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DRCN method for Image Super-Resolution on the Set5 dataset with 4x upscaling is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). These metrics are standard for assessing the quality of super-resolved images. | MOS, PSNR, SSIM => 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U-Net method for skin cancer segmentation is evaluated on benchmark datasets such as the ISIC-2017 and ISIC-2018 datasets. | Kaggle_Skin_Lesion_Segmentation => 0.5\n",
      "The TANDA method achieves the highest MAP score on the WikiQA dataset for the Question Answering task, with a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "SERNet-Former achieves the highest Mean IoU score of 84.62% on the CamVid dataset for the Semantic Segmentation task. | PSPNet => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, with a MAE of 3.71. | 3DDFA => 0.0\n",
      "The shallow word model achieves the highest performance on the Yelp Binary classification dataset with an accuracy of 95.9%. | Char-level_CNN => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:14<14:02, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task with a score of 157177.85. | IQN => 0.0\n",
      "The IQN method achieves the highest Score score on the baseline dataset of 57 Atari 2600 games in the ALE. | Atari_2600_Atlantis => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | CIFAR-10 => 0.5\n",
      "The MemNNs ensemble method is evaluated on the bAbI and NLVR datasets for the Question Answering task. | CNN___Daily_Mail => 0.0\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08% on the CIFAR-100 dataset for Image Classification. | Res2NeXt-29 => 0.0\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using the Percentage of Correct Keypoints (PCK) metric. | Mean_PCK => 0.5\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | STL-10 => 0.5\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies for the node classification task on the Cora dataset. | GCN => 0.0\n",
      "The FRCN (Fast Region-based Convolutional Networks) method for object detection is evaluated on datasets such as PASCAL VOC and Microsoft COCO. | PASCAL_VOC_2007 => 0.5\n",
      "The method that achieves the highest score on the Atari 2600 Robotank dataset for the Atari Games task is MuZero. | Bootstrapped_DQN => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task primarily using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The IDE CamStyle Random Erasing method for Person Re-Identification is evaluated on datasets such as Market-1501, CUHK03, and DukeMTMC-reID. | Market-1501 => 0.5\n",
      "The Transformer method for the Machine Translation task has been evaluated on several datasets, including WMT'14, WMT'17, and various domain-specific datasets such as those used in the AdapMT Shared Task ICON 2020 for English-Hindi translation. Additionally, Persian-English parallel corpora have been used for evaluation. | IWSLT2015_English-German => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using geometric error metrics, which measure the difference between reconstructed meshes and the ground truth. | Mean_NME_ => 0.0\n",
      "The Paragraph Vector method has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA for the Question Answering task. | WikiQA => 0.5\n",
      "The Deep Speech method for speech recognition is commonly evaluated on datasets such as TIMIT and LibriSpeech. These datasets are standard benchmarks in the field of automatic speech recognition (ASR). | Switchboard___Hub500 => 0.0\n",
      "The S-Norm method evaluation datasets for the Question Answering task could not be found in the available resources. | TriviaQA => 0.0\n",
      "The SRCNN method is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) for the Image Super-Resolution task on datasets like Manga109 with 4x upscaling. | PSNR, SSIM => 1.0\n",
      "SparseGPT (175B, 50% Sparsity) achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task. | AWD-LSTM-DOC => 0.5\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, Yelp Polarity, and Yelp Review. | AG_News => 0.5\n",
      "The specific evaluation metrics for the DeepLab-LargeFOV method on the SUN-RGBD dataset for the Scene Segmentation task were not found in the search results. Typically, semantic segmentation tasks are evaluated using metrics such as mean Intersection over Union (mIoU), pixel accuracy, and class accuracy. However, for precise metrics used in a specific study, consulting the original research paper or dataset documentation would be necessary. | Mean_IoU => 0.5\n",
      "The information about the SVDCNN method achieving the highest error score on a specific dataset for the Sentiment Analysis task is not available in the current search results. | Yelp_Fine-grained_classification => 0.0\n",
      "The FDNet method evaluation metrics on the WIDER Face Easy dataset for the Face Detection task could not be found in the available resources. It is possible that the specific evaluation metrics for FDNet on this dataset are not publicly documented or available in the searched sources. | AP => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset for the Face Detection task. | WIDER_Face__Easy_ => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games dataset. | Atari_2600_Chopper_Command => 0.5\n",
      "The Snips method for Speech Recognition is evaluated on datasets such as the Fluent Speech Commands and Snips SmartLights datasets, as well as the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The specific dataset on which the Stacked Hourglass Networks method achieves the highest PCK_0_2 score for the Pose Estimation task is not clearly identified in the available search results. | FLIC_Elbows => 0.0\n",
      "The CRN method for Image-to-Image Translation does not have specific datasets or evaluation metrics clearly identified in the available literature or search results. The searches did not yield direct information about the datasets used for evaluating the CRN method in this context. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "Bootstrapped DQN is evaluated on the Atari benchmark, which includes a diverse selection of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The DPN-131 method is evaluated on the ImageNet-1k dataset for the Image Classification task. | ImageNet => 1.0\n",
      "The current state-of-the-art on the SNLI dataset for Natural Language Inference is achieved by Neural Tree Indexers for Text Understanding. However, specific details about the highest Parameters score in 2023 are not readily available from the search results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The DQN_noop method is typically evaluated on the Atari 2600 games using the Arcade Learning Environment (ALE) as a benchmark. This includes a variety of games, often up to 60 different Atari games, to assess the performance of reinforcement learning algorithms. | Atari_2600_River_Raid => 0.0\n",
      "Inception_V2 is evaluated on the ImageNet dataset using top-1 and top-5 error rates as metrics for the Image Classification task. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The NICE method for image generation on the CIFAR-10 dataset is typically evaluated using metrics such as the Inception Score (IS) and the Fréchet Inception Distance (FID). These metrics assess the quality and diversity of the generated images. | NLL_Test => 0.0\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, ROUGE, and PARENT. | BLEU, ROUGE => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and F1 score, although specific metrics for this method were not found in the search results. | CNN, Daily_Mail => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [01:21<43:56, 45.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated using the AUC (Area Under the ROC Curve) and logloss metrics on the Bing News dataset for the Click-Through Rate Prediction task. | AUC, Log_Loss => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [01:21<23:33, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest Train_Accuracy score on the SNLI dataset for the Natural Language Inference task is not explicitly available from the current search results. The state-of-the-art models mentioned include Neural Tree Indexers and DR-BiLSTM, but specific Train_Accuracy scores are not provided. | __Unigram_and_bigram_features => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score on the Sintel final pass dataset for the Optical Flow Estimation task. | Sintel-final => 1.0\n",
      "The Spynet method for Optical Flow Estimation is evaluated on the MPI-Sintel and KITTI datasets. | Sintel-final => 0.5\n",
      "The SRCNN method for Video Super-Resolution is typically evaluated on datasets like REDS VTSR, which is used in challenges such as the AIM 2019 Challenge on Video Temporal Super-Resolution. | Vid4_-_4x_upscaling => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as semantic segmentation performance, typically measured by mean Intersection over Union (mIoU). | Per-pixel_Accuracy, fwIOU, mIoU => 0.33\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the available resources. Common metrics for such tasks typically include precision, recall, and F0.5 score, which weights precision twice as much as recall. However, specific details for Ann_PAT_MT were not found. | F0_5 => 0.5\n",
      "The DeepMatching method is evaluated on the HPatches dataset for the Dense Pixel Correspondence Estimation task using metrics such as Average Endpoint Error (AEE) and Percentage of Correct Keypoints (PCK). These metrics assess the accuracy of pixel correspondences and the quality of the estimated dense flow field. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.0\n",
      "The DRCN method for Image Super-Resolution on the Set5 dataset with 4x upscaling is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | MOS, PSNR, SSIM => 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [01:31<17:29, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VGG_Resnet_LACE_BiLSTM acoustic model trained on SWB, Fisher, and CH datasets is evaluated on the Switchboard and CallHome portions of the NIST 2000 evaluation set for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 1.0\n",
      "The specific evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found in the available resources. | Score => 0.0\n",
      "The highest Recall_50 score for the Million Song Dataset in the context of collaborative filtering is not readily available from the current search results. The available information primarily focuses on other metrics or does not specify Recall_50 scores. | Mult-VAE_PR => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is typically evaluated on datasets like CIFAR-10, CIFAR-100, and SVHN. These datasets are commonly used in the field for benchmarking semi-supervised learning methods. | CIFAR-10__4000_Labels => 0.5\n",
      "The DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task on the Atari_2600_Video_Pinball dataset. | Atari_2600_Video_Pinball => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:42<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific evaluation metrics for the Paragraph_vector__lexical_overlap___dist_output_ method on the QASent dataset for the Question Answering task could not be found in the available resources. It is possible that this information is not publicly documented or is part of proprietary research. Further investigation in specialized academic databases or contacting the authors of related works might be necessary. | MAP, MRR => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U-Net method for skin cancer segmentation is evaluated on benchmark datasets such as the ISIC-2017 and ISIC-2018 datasets. | Kaggle_Skin_Lesion_Segmentation => 0.5\n",
      "The TANDA method achieves the highest MAP score on the WikiQA dataset for the Question Answering task, with a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | CIFAR-10 => 0.5\n",
      "The PNN method for Click-Through Rate Prediction on the Bing News dataset is evaluated using the metrics AUC (Area Under the ROC Curve) and logloss. | AUC, Log_Loss => 1.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for Image Classification. | Res2NeXt-29 => 0.0\n",
      "The IQN method achieves the highest Score score on the baseline dataset of 57 Atari 2600 games in the ALE. | Atari_2600_Atlantis => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, with a MAE of 3.71. | 3DDFA => 0.0\n",
      "The MemNNs ensemble method is evaluated on the bAbI and NLVR datasets for the Question Answering task. | CNN___Daily_Mail => 0.0\n",
      "SERNet-Former achieves the highest Mean IoU score of 84.62% on the CamVid dataset for the Semantic Segmentation task. | PSPNet => 0.0\n",
      "The shallow word model achieves the highest performance on the Yelp Binary classification dataset with an accuracy of 95.9%. | Char-level_CNN => 0.0\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The FRCN (Fast Region-based Convolutional Networks) method for object detection is evaluated on datasets such as PASCAL VOC and Microsoft COCO. | PASCAL_VOC_2007 => 0.5\n",
      "The IDE + CamStyle + Random Erasing method for Person Re-Identification is evaluated on the Market-1501 and DukeMTMC-reID datasets. | Market-1501 => 0.5\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies for the node classification task on the Cora dataset. | GCN => 0.0\n",
      "MuZero achieves the highest score on the Atari 2600 Robotank dataset for the Atari Games task with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The SRCNN method is typically evaluated using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics for the Manga109 dataset in the 4x upscaling Image Super-Resolution task. | PSNR, SSIM => 1.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D face reconstruction using geometric error metrics, which measure the difference between reconstructed meshes and the ground truth 3D scans. | Mean_NME_ => 0.0\n",
      "The specific evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found in the available resources. It is recommended to consult the original research paper or related documentation for detailed information. | Score => 0.0\n",
      "The DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task on the Atari_2600_Video_Pinball dataset. | Atari_2600_Video_Pinball => 1.0\n",
      "The search did not yield specific information about the datasets used for evaluating the CRN method in Image-to-Image Translation tasks. It seems that the CRN method might not be widely documented or recognized in the context of Image-to-Image Translation, or the information is not readily available in the searched sources. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The Paragraph Vector method has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA for the Question Answering task. | WikiQA => 0.5\n",
      "The FDNet method's evaluation metrics on the WIDER Face Easy dataset for the face detection task are not explicitly found in the search results. The searches did not yield specific information about FDNet's evaluation metrics on this dataset. | AP => 0.0\n",
      "The Transformer method for the Machine Translation task has been evaluated on several datasets, including WMT'14, WMT'17, and various domain-specific datasets such as those used in the AdapMT Shared Task ICON 2020 for English-Hindi translation. Additionally, Persian-English parallel corpora have been used for evaluation. | IWSLT2015_English-German => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task primarily using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The information about the dataset on which the SVDCNN method achieves the highest error score for the Sentiment Analysis task is not available in the current search results. | Yelp_Fine-grained_classification => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and F1 score. However, specific evaluation metrics for the Impatient_Reader method were not found in the search results. | CNN, Daily_Mail => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:56<55:10, 56.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task with a score of 157177.85. | IQN => 0.0\n",
      "The Stacked Hourglass Networks method achieves the highest PCK_0_2 score for the Pose Estimation task on the MPII Human Pose dataset. | FLIC_Elbows => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "SparseGPT (175B, 50% Sparsity) is the current state-of-the-art on WikiText-2 for language modeling. | AWD-LSTM-DOC => 0.0\n",
      "The S-Norm method evaluation datasets for the Question Answering task were not found in the search results. | TriviaQA => 0.0\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News and Yelp reviews, as inferred from the context of typical text classification evaluations, although specific datasets for SVDCNN were not explicitly mentioned in the retrieved documents. | AG_News => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The highest Recall_50 score for the Million Song Dataset in the context of collaborative filtering could not be found in the available resources. The search did not yield specific results for Recall_50 scores, indicating a potential gap in the documented evaluations or the need for more targeted research in this area. | Mult-VAE_PR => 0.0\n",
      "The SRCNN method for Video Super-Resolution is typically evaluated on datasets such as the REDS VTSR dataset, which is used for video temporal super-resolution tasks. However, specific datasets for SRCNN in the context of video super-resolution are not consistently mentioned across sources, indicating that it might be evaluated on various datasets depending on the specific implementation or study. | Vid4_-_4x_upscaling => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [01:03<16:28, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Snips method for Speech Recognition is evaluated on datasets such as the Fluent Speech Commands and Snips SmartLights datasets, as well as the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The highest Train_Accuracy score on the SNLI dataset for the Natural Language Inference task is not explicitly available in the current search results. However, models like ESIM have achieved state-of-the-art results with test accuracies around 88.0% on the SNLI dataset. For the most recent and specific Train_Accuracy scores, consulting the latest papers or benchmark platforms like Papers with Code might be necessary. | __Unigram_and_bigram_features => 0.0\n",
      "The DPN-131 method is evaluated on the ImageNet-1k dataset for the Image Classification task. | ImageNet => 1.0\n",
      "The DQN_hs method evaluation datasets for the Atari Games task were not specifically identified in the search results. The searches did not yield direct information about the datasets used for evaluating the DQN_hs method on Atari Games. | Atari_2600_Chopper_Command => 0.0\n",
      "The Deep Speech method is commonly evaluated on datasets such as the Wall Street Journal (WSJ) and LibriSpeech for the Speech Recognition task. | Switchboard___Hub500 => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score on the Sintel final pass dataset for the Optical Flow Estimation task. | Sintel-final => 1.0\n",
      "The Spynet method for Optical Flow Estimation is evaluated on the MPI-Sintel and KITTI datasets. | Sintel-final => 0.5\n",
      "The evaluation metrics for the DeepLab-LargeFOV method on the SUN-RGBD dataset for the Scene Segmentation task are not explicitly found in the search results. Typically, semantic segmentation tasks are evaluated using metrics such as mean Intersection over Union (mIoU), pixel accuracy, and class accuracy. However, specific metrics for DeepLab-LargeFOV on this dataset were not retrieved. | Mean_IoU => 0.5\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, ROUGE, and PARENT. | BLEU, ROUGE => 0.5\n",
      "The current state-of-the-art on the SNLI dataset for Natural Language Inference is achieved by Neural Tree Indexers for Text Understanding. However, specific details about the highest Parameters score are not readily available from the search results. | 300D_Residual_stacked_encoders => 0.0\n",
      "Inception_V2 is evaluated on the ImageNet dataset using top-1 and top-5 error rates as metrics for the Image Classification task. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The ACF-WIDER method's highest AP score for the Face Detection task on a specific dataset could not be determined from the available search results. | WIDER_Face__Easy_ => 0.0\n",
      "Bootstrapped DQN is evaluated on the Atari benchmark, which includes a diverse selection of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on datasets such as STL-10, CIFAR-10, and CIFAR-100 for the Image Classification task. | STL-10 => 0.5\n",
      "The DeepMatching method is evaluated on the HPatches dataset for the Dense Pixel Correspondence Estimation task using metrics such as the Average End-Point Error (AEPE), which measures the Euclidean distance between the ground-truth and estimated flow. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.5\n",
      "The NICE method is evaluated on the CIFAR-10 dataset for Image Generation using metrics such as the Inception Score, which measures the image quality and diversity of generated images. | NLL_Test => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the available resources. The search did not yield specific results for this method and dataset combination. | F0_5 => 0.0\n",
      "The DQN_noop method is typically evaluated on the Atari 2600 games, which include a wide range of games such as Pong, Boxing, and others within the Atari Learning Environment (ALE). The DQN Replay Dataset, which includes data from all 60 Atari 2600 games, is often used for evaluation. | Atari_2600_River_Raid => 0.5\n",
      "The evaluation metrics for the Paragraph_vector__lexical_overlap___dist_output_ method on the QASent dataset for the Question Answering task could not be found in the available resources. | MAP, MRR => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [01:25<17:37, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VGG_Resnet_LACE_BiLSTM acoustic model, trained on SWB+Fisher+CH with an N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast, is evaluated on the NIST 2000 Switchboard task for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The CyCADA method is typically evaluated using metrics such as semantic segmentation accuracy, mean intersection over union (mIoU), and pixel-level accuracy. However, specific metrics for the SYNTHIA Fall-to-Winter dataset in the context of Image-to-Image Translation were not found in the available resources. | Per-pixel_Accuracy, fwIOU, mIoU => 0.5\n",
      "The DRCN method is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) on the Set5 dataset for 4x upscaling in the Image Super-Resolution task. | MOS, PSNR, SSIM => 0.67\n",
      "The VAT_EntMin method for semi-supervised image classification is evaluated on datasets such as CIFAR-10, CIFAR-100, and SVHN. | CIFAR-10__4000_Labels => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:33<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is typically evaluated using metrics such as detection accuracy and regression loss. Additionally, the area under the PCK (Percentage of Correct Keypoints)-over-alpha curve is often reported as a function of the number of training annotations. | Mean_PCK => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 4 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U-Net method for skin cancer segmentation is evaluated on benchmark datasets such as the ISIC-2017 and ISIC-2018 datasets. | Kaggle_Skin_Lesion_Segmentation => 0.5\n",
      "The TANDA method achieves the highest MAP score on the WikiQA dataset for the Question Answering task, with a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "SERNet-Former achieves the highest Mean IoU score of 84.62% on the CamVid dataset for the Semantic Segmentation task. | PSPNet => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, with a MAE of 3.71. | 3DDFA => 0.0\n",
      "The IQN method achieves the highest Score score on the baseline dataset of 57 Atari 2600 games in the ALE. | Atari_2600_Atlantis => 0.0\n",
      "The FRCN (Fast Region-based Convolutional Networks) method for object detection is evaluated on datasets such as PASCAL VOC and Microsoft COCO. | PASCAL_VOC_2007 => 0.5\n",
      "The shallow word model achieves the highest performance on the Yelp Binary classification dataset with an accuracy of 95.9%. | Char-level_CNN => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies for the node classification task on the Cora dataset. | GCN => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08% on the CIFAR-100 dataset for Image Classification. | Res2NeXt-29 => 0.0\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The MemNNs ensemble method is evaluated on the bAbI and NLVR datasets for the Question Answering task. | CNN___Daily_Mail => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task primarily using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D face reconstruction using geometric error metrics, which measure the difference between reconstructed meshes and the ground truth 3D scans. | Mean_NME_ => 0.0\n",
      "The Deep Speech method for speech recognition is commonly evaluated on datasets such as LibriSpeech and TIMIT Acoustic-Phonetic Continuous Speech Corpus. | Switchboard___Hub500 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:37<36:43, 37.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task with a score of 157177.85. | IQN => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | STL-10 => 0.5\n",
      "The FDNet method evaluation metrics on the WIDER Face Easy dataset for the Face Detection task could not be found in the available resources. It is possible that the specific evaluation metrics for FDNet on this dataset are not publicly documented or available in the searched sources. | AP => 0.0\n",
      "The SRCNN method is evaluated on the Manga109 - 4x upscaling dataset using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics. | PSNR, SSIM => 1.0\n",
      "The Transformer method for the Machine Translation task has been evaluated on several datasets, including WMT'14, WMT'17, and various domain-specific datasets such as those used in the AdapMT Shared Task ICON 2020 for English-Hindi translation. Additionally, Persian-English parallel corpora have been used for evaluation. | IWSLT2015_English-German => 0.0\n",
      "The Paragraph Vector method has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA for the Question Answering task. | WikiQA => 0.5\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10, CIFAR-10, and CIFAR-100 datasets for the Image Classification task. | CIFAR-10 => 0.5\n",
      "The DQN_hs method evaluation datasets for the Atari Games task are not explicitly mentioned in the search results. The searches did not yield specific datasets associated with the DQN_hs method. | Atari_2600_Chopper_Command => 0.0\n",
      "The S-Norm method evaluation datasets for the Question Answering task could not be identified from the available search results. Further specific information about the S-Norm method and its evaluation datasets is needed to provide an accurate answer. | TriviaQA => 0.0\n",
      "The specific evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found in the available resources. | Score => 0.0\n",
      "The specific dataset on which the SVDCNN method achieves the highest error score for the Sentiment Analysis task could not be determined from the available information. | Yelp_Fine-grained_classification => 0.0\n",
      "The highest Recall_50 score for the Million Song Dataset in the context of collaborative filtering could not be found in the available resources. The searches did not yield specific results for Recall_50 scores. Further investigation in specialized databases or contacting authors of relevant papers might be necessary. | Mult-VAE_PR => 0.0\n",
      "The dataset on which the Stacked Hourglass Networks method achieves the highest PCK_0_2 score for the Pose Estimation task is not explicitly mentioned in the available search results. Further specific research or access to detailed experimental results from relevant papers may be required to determine this information. | FLIC_Elbows => 0.0\n",
      "The IDE+CamStyle+Random Erasing method for Person Re-Identification is evaluated on the Market-1501 dataset, achieving a rank-1 accuracy of 89.4% and a mean Average Precision (mAP) of 71.5%. | Market-1501 => 1.0\n",
      "MuZero achieves the highest score on the Atari_2600_Robotank dataset for the Atari_Games task with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER Face dataset for the Face Detection task. | WIDER_Face__Easy_ => 0.5\n",
      "Inception_V2 is typically evaluated on the ImageNet dataset using metrics such as accuracy, precision, recall, and F1-score for the image classification task. However, specific metrics for Inception_V2 on ImageNet were not found in the search results. | Top_1_Accuracy, Top_5_Accuracy => 0.0\n",
      "The DQN_noop method is typically evaluated on the Atari 2600 games dataset, which includes a variety of games used for benchmarking reinforcement learning algorithms. However, specific datasets for DQN_noop evaluations were not explicitly found in the search results. | Atari_2600_River_Raid => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [01:03<29:43, 30.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated using the AUC (Area Under the ROC Curve) and logloss metrics on the Bing News dataset for the Click-Through Rate Prediction task. | AUC, Log_Loss => 1.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for the Dense Pixel Correspondence Estimation task using metrics such as Average End-Point Error (AEPE) and Percentage of Correct Keypoints (PCK). Lower AEPE and higher PCK values indicate better performance. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.5\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, Yelp Polarity, and Yelp Review. These datasets are commonly used for evaluating text classification models. | AG_News => 0.5\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score on the Sintel final pass dataset for the Optical Flow Estimation task. | Sintel-final => 1.0\n",
      "The current state-of-the-art on the SNLI dataset for Natural Language Inference is achieved by Neural Tree Indexers for Text Understanding. However, specific details about the highest Parameters score are not readily available from the search results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on datasets such as CIFAR-10, SVHN, and MNIST. | CIFAR-10__4000_Labels => 0.5\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, ROUGE, and PARENT. | BLEU, ROUGE => 0.5\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using metrics such as detection accuracy and regression loss. These metrics assess the precision of keypoint localization and the effectiveness of the ConvNet features in performing tasks that require correspondence and fine localization. | Mean_PCK => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO Visual Question Answering (VQA) dataset. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the available resources. The search did not yield specific results for this method and dataset combination. | F0_5 => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on datasets such as the Fluent Speech Commands and Snips SmartLights datasets, as well as the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [01:12<19:35, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spynet method for Optical Flow Estimation is evaluated on the MPI-Sintel and KITTI datasets. | Sintel-final => 0.5\n",
      "The highest Train_Accuracy score on the SNLI dataset for the Natural Language Inference task in 2023 is not explicitly available from the current search results. The search results primarily focus on state-of-the-art models and their performance, but specific Train_Accuracy scores are not detailed. It is recommended to check the latest research papers or repositories like 'Papers with Code' for the most up-to-date and specific performance metrics. | __Unigram_and_bigram_features => 0.0\n",
      "The current state-of-the-art on WikiText-2 is SparseGPT with 175 billion parameters and 50% sparsity. | AWD-LSTM-DOC => 0.0\n",
      "Bootstrapped DQN is evaluated on the Atari benchmark, which includes a diverse selection of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The DPN-131 method for Image Classification is evaluated on the ImageNet-1k and Places365-Standard datasets. | ImageNet => 0.5\n",
      "The SRCNN method for Video Super-Resolution is typically evaluated on datasets like REDS VTSR, which is used in video temporal super-resolution challenges. However, specific datasets for SRCNN in video super-resolution are not consistently mentioned in the available literature. | Vid4_-_4x_upscaling => 0.0\n",
      "The DeepLab-LargeFOV method is typically evaluated using metrics such as mean Intersection over Union (mIoU) and pixel accuracy for scene segmentation tasks on datasets like SUN-RGBD. However, specific evaluation metrics for DeepLab-LargeFOV on the SUN-RGBD dataset were not found in the search results. | Mean_IoU => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using accuracy as a metric, with a reported performance of 63.8%. | CNN, Daily_Mail => 0.5\n",
      "The CRN method for Image-to-Image Translation has not been specifically evaluated on any particular datasets as per the available search results. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as accuracy and F1 score. However, specific evaluation metrics for this method on the QASent dataset were not found in the available resources. | MAP, MRR => 0.0\n",
      "The DRCN method for Image Super-Resolution on the Set5 dataset with 4x upscaling is typically evaluated using the PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index) metrics. | MOS, PSNR, SSIM => 0.67\n",
      "The CyCADA method is evaluated on the SYNTHIA Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as semantic segmentation performance, typically measured by mean Intersection over Union (mIoU). | Per-pixel_Accuracy, fwIOU, mIoU => 0.33\n",
      "The DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task on the Atari_2600_Video_Pinball dataset. | Atari_2600_Video_Pinball => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [01:26<16:55, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The VGG_Resnet_LACE_BiLSTM acoustic model, trained on SWB, Fisher, and CH datasets, with an N-gram and RNNLM language model trained on Switchboard, Fisher, Gigaword, and Broadcast, is evaluated on the Switchboard and CallHome portions of the NIST 2000 evaluation set for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:36<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NICE method for image generation on the CIFAR-10 dataset is typically evaluated using metrics such as the Inception Score, which measures the quality and diversity of generated images. | NLL_Test => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)\n",
    "batch_num = 4\n",
    "iterative_monkey.thread_safe_evaluator_batch(toolqa_test, optimized_actor_agent,batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
