{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USR_NAME = 'shirwu'\n",
    "TOOL_QA_ROOT = '/zfs/projects/students/junze-inference/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SERPER_API_KEY'] = 'c6657f1d036706c608af42ce15bf562abb314914'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'hard'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "# dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) \n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** Since DSPy 2.5.16+, TypedPredictors are now deprecated, underperform, and are about to be removed! ***\n",
      "Please use standard predictors, e.g. dspy.Predict and dspy.ChainOfThought.\n",
      "They now support type annotations and other features of TypedPredictors and tend to work much better out of the box.\n",
      "Please let us know if you face any issues: https://github.com/stanfordnlp/dspy/issues\n"
     ]
    }
   ],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    "    max_iters=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Disable all INFO logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Silence all loggers that might be chatty\n",
    "loggers_to_silence = [\n",
    "    \"httpx\",\n",
    "    \"httpcore\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"dspy\",\n",
    "    \"langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"tiktoken\",\n",
    "    \"asyncio\",\n",
    "    \"faiss\",\n",
    "    \"anthropic\"\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_silence:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable tokenizer parallelism warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client GPT3, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Which method achieves the highest PCK score on Leeds_Sports_Poses dataset for Pose_Estimation task?', 'answer': 'Pyramid_Residual_Modules__PRMs_'}) (input_keys={'question', 'paper_id'})\n",
      "physics | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APICallMetrics:\n",
    "    timestamp: datetime\n",
    "    tool_name: str\n",
    "    tokens_in: int = 0\n",
    "    tokens_out: int = 0\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AvatarMetrics:\n",
    "    total_calls: int = 0\n",
    "    total_tokens_in: int = 0\n",
    "    total_tokens_out: int = 0\n",
    "    total_execution_time: float = 0.0\n",
    "    calls_by_tool: Dict[str, int] = field(default_factory=dict)\n",
    "    api_call_history: List[APICallMetrics] = field(default_factory=list)\n",
    "    \n",
    "    def add_call(self, metrics: APICallMetrics):\n",
    "        self.total_calls += 1\n",
    "        self.total_tokens_in += metrics.tokens_in\n",
    "        self.total_tokens_out += metrics.tokens_out\n",
    "        self.total_execution_time += metrics.execution_time\n",
    "        self.calls_by_tool[metrics.tool_name] = self.calls_by_tool.get(metrics.tool_name, 0) + 1\n",
    "        self.api_call_history.append(metrics)\n",
    "    \n",
    "    def merge(self, other: 'AvatarMetrics'):\n",
    "        \"\"\"Merge another AvatarMetrics instance into this one\"\"\"\n",
    "        self.total_calls += other.total_calls\n",
    "        self.total_tokens_in += other.total_tokens_in\n",
    "        self.total_tokens_out += other.total_tokens_out\n",
    "        self.total_execution_time += other.total_execution_time\n",
    "        for tool, count in other.calls_by_tool.items():\n",
    "            self.calls_by_tool[tool] = self.calls_by_tool.get(tool, 0) + count\n",
    "        self.api_call_history.extend(other.api_call_history)\n",
    "\n",
    "    def estimate_cost(self, model_name: str = \"gpt-4\") -> float:\n",
    "        pricing = {\n",
    "            \"gpt-4\": {\"input\": 2.5, \"output\": 10.0},\n",
    "        }\n",
    "        if model_name not in pricing:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        rates = pricing[model_name]\n",
    "        input_cost = (self.total_tokens_in / 1000000) * rates[\"input\"]\n",
    "        output_cost = (self.total_tokens_out / 1000000) * rates[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "class AvatarWithMetrics(Avatar):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metrics = AvatarMetrics()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error counting tokens: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _wrapped_tool_call(self, tool, input_text: str) -> str:\n",
    "        start_time = time.time()\n",
    "        tokens_in = self._count_tokens(input_text)\n",
    "        \n",
    "        try:\n",
    "            result = tool.run(input_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tool execution error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            execution_time = time.time() - start_time\n",
    "            tokens_out = self._count_tokens(str(result))\n",
    "            \n",
    "            metrics = APICallMetrics(\n",
    "                timestamp=datetime.now(),\n",
    "                tool_name=tool.name,\n",
    "                tokens_in=tokens_in,\n",
    "                tokens_out=tokens_out,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self.metrics.add_call(metrics)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        metrics = APICallMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            tool_name=\"main_llm\",\n",
    "            tokens_in=self._count_tokens(str(args) + str(kwargs)),\n",
    "            tokens_out=self._count_tokens(str(result)),\n",
    "            execution_time=total_time\n",
    "        )\n",
    "        self.metrics.add_call(metrics)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for example in test_set:\n",
    "            def process_with_metrics(example=example):\n",
    "                try:\n",
    "                    avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "                    prediction = avatar(**example.inputs().toDict())\n",
    "                    return metric(example, prediction), avatar.metrics\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return 0, AvatarMetrics()\n",
    "\n",
    "            futures.append(executor.submit(process_with_metrics))\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            score, metrics = future.result()\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        try:\n",
    "            avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "            prediction = avatar(**example.inputs().toDict())\n",
    "            score = metric(example, prediction)\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in avatar.metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def format_metrics_report(metrics: AvatarMetrics, model_name: str = \"gpt-4\") -> str:\n",
    "    cost = metrics.estimate_cost(model_name)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Avatar Execution Metrics Report\n",
    "==============================\n",
    "Total Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "Total API Calls: {metrics.total_calls}\n",
    "Total Tokens: {metrics.total_tokens_in + metrics.total_tokens_out:,} ({metrics.total_tokens_in:,} in, {metrics.total_tokens_out:,} out)\n",
    "Estimated Cost: ${cost:.4f}\n",
    "\n",
    "Total Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "\n",
    "Tool Usage Breakdown:\n",
    "-------------------\n",
    "\"\"\"\n",
    "    for tool, count in sorted(metrics.calls_by_tool.items()):\n",
    "        report += f\"{tool}: {count} calls\\n\"\n",
    "\n",
    "    report += \"\\nTotal calling time per API:\\n\"\n",
    "    api_call_total_time = {}\n",
    "    for call in metrics.api_call_history:\n",
    "        api_call_total_time[call.tool_name] = api_call_total_time.get(call.tool_name, 0) + call.execution_time\n",
    "    for tool, total_time in api_call_total_time.items():\n",
    "        report += f\"{tool}: {total_time:.2f} seconds\\n\"\n",
    "\n",
    "        \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a Mean IoU of 84.62. | PSPNet => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method EASE achieves the highest Recall@50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism proposed by Dilin Wang, Chengyue Gong, and Qiang Liu, which achieves a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is the one that establishes the state of the art with an impressive MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The RNN (Featured) model achieves the highest Train Accuracy on the SNLI dataset for the Natural Language Inference task with an accuracy of 96.52%. | __Unigram_and_bigram_features => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on the Snips SmartLights dataset and the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for Head Pose Estimation, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, DBpedia, Yelp Review Polarity, and Yahoo Answers. | AG_News => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:49<48:12, 49.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "I couldn't find the specific method that achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task. You may need to consult the latest research papers or specific benchmarks related to Atari game scores for the most accurate information. | IQN => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method for Natural Language Inference is evaluated on the Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset, and Quora Question Pairs dataset. | SNLI => 0.5\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel dataset. | Sintel-final => 0.0\n",
      "The Paragraph_vector method for Question Answering tasks has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari game dataset implemented by OpenAI Gym, which includes 49 classic Atari games as part of the Arcade Learning Environment. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The AWD-LSTM-DOC method is typically evaluated on the WikiText-2 dataset for the Language Modelling task using metrics such as perplexity, cross entropy, and bits-per-character (BPC). | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The Transformer method for Machine Translation is evaluated on various datasets, including nine document-level datasets and two sentence-level datasets across six languages, as mentioned in the paper \"Rethinking Document-level Neural Machine Translation.\" | IWSLT2015_English-German => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The Deep_Speech method for speech recognition is evaluated on several datasets, including AN4, TEDLIUM, Voxforge, Common Voice, and LibriSpeech. | Switchboard___Hub500 => 0.0\n",
      "MuZero achieves the highest score on the Atari_2600_Robotank dataset for the Atari_Games task with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The DPN-131 method for Image Classification is evaluated on the Places365-Standard dataset, PASCAL-S, OSIE, and MIT1003 datasets. | ImageNet => 0.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and other image quality metrics like IFC (Information Fidelity Criterion) and NQM (Noise Quality Measure). | PSNR, SSIM => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH N-gram RNNLM language model is evaluated on datasets derived from the Switchboard and Fisher corpora for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mIoU (mean Intersection over Union), fwIoU (frequency weighted Intersection over Union), and Pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the following datasets for the Person Re-Identification task: Market-1501, DukeMTMC-reID, CUHK03, PRID2011, iLIDS-VID, and VIPeR. | Market-1501 => 0.5\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question_Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and ensemble averaging. The evaluation involves reporting results for the best single model as well as the average of accuracies for the top-performing models on validation data. Ensemble models are also evaluated using simple averaging of the answer probabilities predicted by ensemble members. | CNN, Daily_Mail => 1.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in the model boosts performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image_Generation task were not found in the retrieved documents. It is possible that the specific metrics used for evaluating the NICE method on this task are not widely documented or available in the sources accessed. | NLL_Test => 0.0\n",
      "The available resources did not provide specific information on the dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further investigation or access to specific research papers or datasets might be required to obtain this information. | Atari_2600_Video_Pinball => 0.0\n",
      "The current state-of-the-art method on the Yelp Binary classification dataset for Sentiment Analysis is XLNet. However, specific error scores are not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The available resources did not provide information on the dataset where the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further specific research or access to the original paper discussing SVDCNN might be necessary to obtain this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset using metrics such as lexical overlap and distribution output. However, specific evaluation metrics for this method on the QASent dataset were not found in the retrieved documents. | MAP, MRR => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games. | Atari_2600_River_Raid => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face__Easy_ dataset for the Face Detection task using metrics such as detection accuracy. Specifically, FDNet1.0 achieved a detection accuracy of 95.9% on the easy set of the WIDER FACE validation dataset. | AP => 0.0\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for detailed evaluation metrics. | Score => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as the REDS VTSR dataset and other popular video benchmarks like Set5 and SuperTexture. | Vid4_-_4x_upscaling => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII Human Pose dataset. | FLIC_Elbows => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as mean Intersection over Union (mIoU) and boundary metrics. These metrics help in assessing the segmentation performance, particularly in challenging indoor scenes with high variability and numerous object classes. | Mean_IoU => 0.5\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as detection accuracy and regression loss. However, specific metrics for Pascal3D were not found in the retrieved documents. | Mean_PCK => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with fewer parameters than previous models. | 300D_Residual_stacked_encoders => 0.0\n",
      "The DeepMatching_ method is evaluated on the HPatches dataset for the Dense Pixel Correspondence Estimation task using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is defined as 'accuracy@', which measures the proportion of 'correct' pixels from the first image with respect to the total number of pixels. A pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, which include a wide range of games such as Montezuma's Revenge and others in the Atari suite. The evaluations typically involve testing the trained DQN agent on specific games and measuring performance in terms of average scores achieved. | Atari_2600_Chopper_Command => 0.0\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset using the PSNR (Peak Signal-to-Noise Ratio) metric. The 30-layer DRCN network achieves higher PSNR compared to other methods like CSCN. | MOS, PSNR, SSIM => 0.5\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Pong dataset, where it reaches a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the available resources. The search did not yield specific results for Ann_PAT_MT. It is possible that the method is evaluated using common metrics for grammatical error detection, such as precision, recall, and F1-score, but this cannot be confirmed without more specific information. | F0_5 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [02:08<1:04:33, 66.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated on the Bing_News dataset for Click-Through Rate Prediction using metrics such as Logloss, which is a common metric for evaluating the performance of models in CTR prediction tasks. However, specific metrics for the Bing_News dataset were not found in the retrieved information. | AUC, Log_Loss => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:09<00:00,  2.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CRN method for Image-to-Image Translation is evaluated on datasets such as Cityscapes and Horse2Zebra. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score, metrics = multi_thread_executor(toolqa_test, ToolQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.18\n",
      "\n",
      "Avatar Execution Metrics Report\n",
      "==============================\n",
      "Total Execution Time: 4957.93 seconds\n",
      "Total API Calls: 60\n",
      "Total Tokens: 90,571 (1,702 in, 88,869 out)\n",
      "Estimated Cost: $5.3832\n",
      "\n",
      "Total Execution Time: 4957.93 seconds\n",
      "\n",
      "Tool Usage Breakdown:\n",
      "-------------------\n",
      "main_llm: 60 calls\n",
      "\n",
      "Total calling time per API:\n",
      "main_llm: 4957.93 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average Score on ArxivQA before opitmization: {aqa_score:.2f}\")\n",
    "print(f\"Test Score: {score:.2f}\")\n",
    "print(format_metrics_report(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dspy.teleprompt import AvatarOptimizer\n",
    "\n",
    "# teleprompter = AvatarOptimizer(\n",
    "#     metric=metric,\n",
    "#     max_iters=3,\n",
    "#     max_negative_inputs=10,\n",
    "#     max_positive_inputs=10,\n",
    "#     lower_bound=0.5,\n",
    "#     upper_bound=0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_actor_agent = teleprompter.compile(\n",
    "#     student=actor_agent,\n",
    "#     trainset=toolqa_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "iterative_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=0,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 0.00 seconds\n",
      "                Total API Calls: 0\n",
      "                - Comparator calls: 0\n",
      "                - Feedback instruction calls: 0\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 0\n",
      "                - Input tokens: 0\n",
      "                - Output tokens: 0\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $0.0000\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "result = iterative_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization cost: $0.0000\n",
      "Final score achieved: -999.000\n"
     ]
    }
   ],
   "source": [
    "optimized_actor_agent = result[\"agent\"]\n",
    "optimization_metrics = result[\"metrics\"]\n",
    "\n",
    "# Now you can process the metrics\n",
    "print(f\"Total optimization cost: ${optimization_metrics['total_cost']:.4f}\")\n",
    "print(f\"Final score achieved: {optimization_metrics['final_score']:.3f}\")\n",
    "\n",
    "# Analyze per-iteration performance\n",
    "for iteration in optimization_metrics['iteration_details']:\n",
    "    print(f\"\\nIteration {iteration['iteration']}:\")\n",
    "    print(f\"Score: {iteration['score']:.3f}\")\n",
    "    print(f\"Comparator tokens in: {iteration['comparator_metrics']['tokens_in']}\")\n",
    "    print(f\"Comparator tokens out: {iteration['comparator_metrics']['tokens_out']}\")\n",
    "    print(f\"Feedback tokens in: {iteration['feedback_metrics']['tokens_in']}\")\n",
    "    print(f\"Feedback tokens out: {iteration['feedback_metrics']['tokens_out']}\")\n",
    "    print(f\"Execution time: {iteration['execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a Mean IoU of 84.62. | PSPNet => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism proposed by Dilin Wang, Chengyue Gong, and Qiang Liu, which achieves a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The AWD-LSTM-DOC method is typically evaluated on the WikiText-2 dataset for the Language Modelling task using metrics such as perplexity, cross entropy, and bits-per-character (BPC). | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on the Snips SmartLights dataset and the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel dataset. | Sintel-final => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method for Natural Language Inference is evaluated on the Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset, and Quora Question Pairs dataset. | SNLI => 0.5\n",
      "The Paragraph_vector method for Question Answering tasks has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The current state-of-the-art method for achieving the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA-DeBERTa-V3-Large + ALL, with a MAP score of 88.7%. | Key-Value_Memory_Network => 0.0\n",
      "The Transformer method for Machine Translation is evaluated on various datasets, including nine document-level datasets and two sentence-level datasets across six languages, as mentioned in the paper \"Rethinking Document-level Neural Machine Translation.\" | IWSLT2015_English-German => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, DBpedia, Yelp Review Polarity, and Yahoo Answers. | AG_News => 0.5\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for Head Pose Estimation, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:50<50:04, 50.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find the specific method that achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task. You may need to consult the latest research papers or specific benchmarks related to Atari game scores for the most accurate information. | IQN => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with significantly fewer parameters than previous models. | 300D_Residual_stacked_encoders => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The DPN-131 method for Image Classification has been evaluated on the Places365-Standard dataset, PASCAL-S, OSIE, and MIT1003 datasets. | ImageNet => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the standard benchmark metric, which is the geometric error between reconstructed meshes and the ground truth. | Mean_NME_ => 0.5\n",
      "MuZero achieves the highest score of 131.13 on the Atari_2600_Robotank dataset for the Atari_Games task. | Bootstrapped_DQN => 0.0\n",
      "The Deep_Speech method for speech recognition is evaluated on several datasets, including the LibriSpeech Corpus, TIMIT Acoustic-Phonetic Continuous Speech Corpus, AN4, TEDLIUM, Voxforge, and Common Voice. | Switchboard___Hub500 => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari game dataset implemented by OpenAI Gym, which includes a variety of classic Atari games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH N-gram RNNLM language model is evaluated on datasets derived from the Switchboard and Fisher corpora for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question_Answering task. | CNN___Daily_Mail => 0.5\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the following datasets for the Person Re-Identification task: Market-1501, DukeMTMC-reID, CUHK03, PRID2011, iLIDS-VID, and VIPeR. | Market-1501 => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mIoU (mean Intersection over Union), fwIoU (frequency weighted Intersection over Union), and Pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The current state-of-the-art method on the Yelp Binary classification dataset for Sentiment Analysis is XLNet. However, specific error scores are not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and other image quality metrics like IFC (Information Fidelity Criterion) and NQM (Noise Quality Measure). | PSNR, SSIM => 0.5\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and ensemble averaging. The evaluation involves reporting results for the best single model as well as the average of accuracies for the top-performing models on validation data. Ensemble models are also evaluated using simple averaging of the answer probabilities predicted by ensemble members. | CNN, Daily_Mail => 1.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset using metrics such as lexical overlap and distribution output. However, specific evaluation metrics for this method on the QASent dataset were not found in the retrieved documents. | MAP, MRR => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games. | Atari_2600_River_Raid => 0.0\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for detailed evaluation metrics. | Score => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, which include a wide range of games such as Montezuma's Revenge and others in the Atari suite. The evaluations typically involve testing the trained DQN agent on specific games and measuring performance in terms of average scores achieved. | Atari_2600_Chopper_Command => 0.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in the model boosts performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Atari 2600 dataset. | Atari_2600_Atlantis => 0.5\n",
      "The available tools did not provide the specific dataset on which the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further research or access to specific academic papers or datasets may be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face__Easy_ dataset for the Face Detection task using metrics such as detection accuracy. Specifically, FDNet achieved a detection accuracy of 95.9% on the easy set of the WIDER FACE validation dataset. | AP => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the available resources. The CoNLL-2014 shared task generally uses precision, recall, and F1-score as evaluation metrics for grammatical error detection tasks, but specific details for Ann_PAT_MT are not found. | F0_5 => 0.0\n",
      "The available information does not specify the dataset on which the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further specific details might be found in the original research papers or datasets related to Atari Games and DDQN methods. | Atari_2600_Video_Pinball => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII Human Pose dataset. | FLIC_Elbows => 0.0\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image_Generation task were not found in the retrieved documents. It is possible that the specific metrics used for evaluating the NICE method on this dataset are not publicly documented or are not available in the sources accessed. | NLL_Test => 0.0\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as Average Precision (AP) and Average Viewpoint Precision (AVP). | Mean_PCK => 0.0\n",
      "The CRN method for Image-to-Image Translation is evaluated on datasets that include semantic layouts and synthesized images from models like ForenSynths. However, specific datasets used for CRN were not clearly identified in the search results. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The DeepMatching_ method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a performance metric based on the number of correctly matched pixels compared to the overall number of pixels. This is defined as 'accuracy@', which measures the proportion of 'correct' pixels from the first image with respect to the total number of pixels. A pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for the Scene Segmentation task using metrics such as accuracy and boundary F1-measure (BF). | Mean_IoU => 0.0\n",
      "The DR-BiLSTM (Single) model achieves the highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task, outperforming other models including ensemble models when preprocessing is applied. | __Unigram_and_bigram_features => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on popular video benchmarks such as Set5 and SuperTexture, as well as other datasets used in frame-based super-resolution experiments. | Vid4_-_4x_upscaling => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset using the PSNR (Peak Signal-to-Noise Ratio) metric. The DRCN method's deeper network achieves better performance compared to other methods, with significant improvements in PSNR values. | MOS, PSNR, SSIM => 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:02<00:00,  2.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated on the Bing_News dataset for Click-Through Rate Prediction using metrics such as Area Under the Curve (AUC) and Logloss. | AUC, Log_Loss => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is the one that establishes the state of the art with an impressive MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a Mean IoU of 84.62. | PSPNet => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The RNN (Featured) model achieves the highest Train Accuracy on the SNLI dataset for the Natural Language Inference task with an accuracy of 96.52%. | __Unigram_and_bigram_features => 0.0\n",
      "The Transformer method for Machine Translation is evaluated on various datasets, including nine document-level datasets and two sentence-level datasets across six languages, as mentioned in the paper \"Rethinking Document-level Neural Machine Translation.\" | IWSLT2015_English-German => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on the Snips SmartLights dataset and the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel dataset. | Sintel-final => 0.0\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, DBpedia, Yelp Review Polarity, and Yahoo Answers. | AG_News => 0.5\n",
      "The Paragraph_vector method for Question Answering tasks has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism proposed by Dilin Wang, Chengyue Gong, and Qiang Liu, which achieves a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari game dataset implemented by OpenAI Gym, which includes 49 classic Atari games as part of the Arcade Learning Environment. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for Head Pose Estimation, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method for Natural Language Inference is evaluated on the Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset, and Quora Question Pairs dataset. | SNLI => 0.5\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [01:13<1:11:57, 73.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find the specific method that achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task. You may need to consult the latest research papers or specific benchmarks related to Atari game scores for the most accurate information. | IQN => 0.0\n",
      "The method that achieves the highest score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for the 3D Face Reconstruction task using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.5\n",
      "The Deep_Speech method for Speech Recognition is evaluated on datasets such as LibriSpeech, TIMIT, AN4, TEDLIUM, Voxforge, and Common Voice. | Switchboard___Hub500 => 0.0\n",
      "The DPN-131 method for image classification has been evaluated on the Places365-Standard, PASCAL-S, OSIE, and MIT1003 datasets. | ImageNet => 0.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and other image quality metrics like IFC (Information Fidelity Criterion) and NQM (Noise Quality Measure). | PSNR, SSIM => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, which include a wide range of games such as Montezuma's Revenge and others in the Atari suite. The evaluations typically involve testing the trained DQN agent on specific games and measuring performance in terms of average scores achieved. | Atari_2600_Chopper_Command => 0.0\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image_Generation task were not found in the retrieved documents. It is possible that the specific metrics used for evaluating the NICE method on this task are not widely documented or available in the sources accessed. | NLL_Test => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mIoU (mean Intersection over Union), fwIoU (frequency weighted Intersection over Union), and Pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The available resources did not provide information on the dataset where the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further specific research or access to the original paper discussing SVDCNN might be necessary to obtain this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset using the PSNR (Peak Signal-to-Noise Ratio) metric. The 30-layer DRCN network achieves higher PSNR compared to other methods like CSCN. | MOS, PSNR, SSIM => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the following datasets for the Person Re-Identification task: Market-1501, DukeMTMC-reID, CUHK03, PRID2011, iLIDS-VID, and VIPeR. | Market-1501 => 0.5\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for detailed evaluation metrics. | Score => 0.0\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as detection accuracy and regression loss. However, specific metrics for Pascal3D were not found in the retrieved documents. | Mean_PCK => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face__Easy_ dataset for the Face Detection task using metrics such as detection accuracy. Specifically, FDNet achieved a detection accuracy of 95.9% on the easy set of the WIDER FACE validation dataset. | AP => 0.0\n",
      "The available information does not specify the dataset on which the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further specific details might be found in the original research papers or datasets related to Atari Games and DDQN methods. | Atari_2600_Video_Pinball => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for the Scene Segmentation task using metrics such as accuracy and boundary F1-measure (BF). | Mean_IoU => 0.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question_Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the available resources. The CoNLL-2014 shared task generally uses precision, recall, and F1-score as evaluation metrics for grammatical error detection tasks, but specific details for Ann_PAT_MT are not found. | F0_5 => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII dataset. | FLIC_Elbows => 0.0\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Atari 2600 Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with fewer parameters than previous models. | 300D_Residual_stacked_encoders => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and ensemble averaging. The evaluation involves reporting results for the best single model and the average of accuracies for the top 20% of models with the best performance on validation data. Ensemble models are evaluated using simple averaging of the answer probabilities predicted by ensemble members. | CNN, Daily_Mail => 1.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in the model boosts performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset using metrics such as lexical overlap and distribution output. However, specific evaluation metrics for this method on the QASent dataset were not found in the retrieved documents. | MAP, MRR => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games. | Atari_2600_River_Raid => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH N-gram RNNLM language model is evaluated on datasets derived from the Switchboard and Fisher corpora for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on popular video benchmarks such as Set5 and SuperTexture, as well as other datasets used for frame-based super-resolution experiments. | Vid4_-_4x_upscaling => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. Dynamic evaluation is used to improve the state-of-the-art perplexity on WikiText-2, providing significant improvements over other methods. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The DeepMatching_ method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is referred to as 'accuracy@', which measures the proportion of 'correct' pixels from the first image with respect to the total number of pixels. A pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The current state-of-the-art on the Yelp Binary classification dataset for Sentiment Analysis is XLNet. However, specific information about the highest Error score is not readily available from the searches conducted. | Char-level_CNN => 0.0\n",
      "The CRN method for Image-to-Image Translation is evaluated on datasets such as Cityscapes and GTA5. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:31<00:00,  2.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method evaluation metrics on the Bing_News dataset for the Click-Through Rate Prediction task are not explicitly mentioned in the retrieved documents or web search results. However, common evaluation metrics for CTR prediction tasks include metrics like AUC (Area Under the Curve), Logloss, accuracy, precision, recall, and F1-score. It is likely that similar metrics are used for evaluating the PNN method on the Bing_News dataset. | AUC, Log_Loss => 0.0\n",
      "Processing batch 3 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a Mean IoU of 84.62. | PSPNet => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 300D_NTI-SLSTM-LSTM_encoders method for Natural Language Inference is evaluated on the Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset, and Quora Question Pairs dataset. | SNLI => 0.5\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism proposed by Dilin Wang, Chengyue Gong, and Qiang Liu, which achieves a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The RNN (Featured) model achieves the highest Train Accuracy on the SNLI dataset for the Natural Language Inference task with an accuracy of 96.52%. | __Unigram_and_bigram_features => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:18<18:30, 18.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Paragraph_vector method for Question Answering tasks has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "I couldn't find the specific method that achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task. You may need to consult the latest research papers or specific benchmarks related to Atari game scores for the most accurate information. | IQN => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The method that achieves the highest score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero, with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The current state-of-the-art method achieving the highest MAP score on the WikiQA dataset is TANDA-DeBERTa-V3-Large + ALL, with an impressive MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on the Snips SmartLights dataset and the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The Transformer method for Machine Translation is evaluated on various datasets, including nine document-level datasets and two sentence-level datasets across six languages, as mentioned in the paper \"Rethinking Document-level Neural Machine Translation.\" | IWSLT2015_English-German => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with significantly fewer parameters than previous models. | 300D_Residual_stacked_encoders => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel dataset. | Sintel-final => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for Head Pose Estimation, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, DBpedia, Yelp Review Polarity, and Yahoo Answers. | AG_News => 0.5\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari game dataset implemented by OpenAI Gym, which includes a variety of classic Atari games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The Deep_Speech method for Speech Recognition is evaluated on datasets such as LibriSpeech, TIMIT, AN4, TEDLIUM, Voxforge, and Common Voice. | Switchboard___Hub500 => 0.0\n",
      "The DPN-131 method for Image Classification is evaluated on the Places365-Standard, PASCAL-S, OSIE, and MIT1003 datasets. | ImageNet => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and other image quality metrics like IFC (Information Fidelity Criterion) and NQM (Noise Quality Measure). | PSNR, SSIM => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mIoU (mean Intersection over Union), fwIoU (frequency weighted Intersection over Union), and Pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question_Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and ensemble averaging. The evaluation involves reporting results for the best single model as well as the average of accuracies for the top-performing models on validation data. Ensemble models are also evaluated using simple averaging of the answer probabilities predicted by ensemble members. | CNN, Daily_Mail => 1.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH N-gram RNNLM language model is evaluated on datasets derived from the Switchboard and Fisher corpora for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the following datasets for the Person Re-Identification task: Market-1501, DukeMTMC-reID, CUHK03, PRID2011, iLIDS-VID, and VIPeR. | Market-1501 => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image_Generation task were not found in the retrieved documents. It is possible that the specific metrics used for evaluating the NICE method on this task are not widely documented or available in the sources accessed. | NLL_Test => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as mean Intersection over Union (mIoU) and boundary metrics. These metrics help in assessing the segmentation performance, particularly in challenging indoor scenes with high variability and numerous object classes. | Mean_IoU => 0.5\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The current state-of-the-art method on the Yelp Binary classification dataset for Sentiment Analysis is XLNet. However, specific error scores are not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the available resources. The CoNLL-2014 shared task generally uses precision, recall, and F1-score as evaluation metrics for grammatical error detection tasks, but specific details for Ann_PAT_MT are not found. | F0_5 => 0.0\n",
      "The available information does not specify the dataset on which the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further specific details might be found in the original research papers or datasets related to Atari Games and DDQN methods. | Atari_2600_Video_Pinball => 0.0\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Atari 2600 Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for detailed evaluation metrics. | Score => 0.0\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as detection accuracy and regression loss. However, specific metrics for Pascal3D were not found in the retrieved documents. | Mean_PCK => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset using metrics such as lexical overlap and distribution output. However, specific evaluation metrics for this method on the QASent dataset were not found in the retrieved documents. | MAP, MRR => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games. | Atari_2600_River_Raid => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face__Easy_ dataset for the Face Detection task using metrics such as detection accuracy. Specifically, FDNet achieved a detection accuracy of 95.9% on the easy set of the WIDER FACE validation dataset. | AP => 0.0\n",
      "The available resources did not provide information on the dataset where the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further specific research or access to detailed experimental results from relevant studies may be required to obtain this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in the model boosts performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The DQN_hs method for the Atari Games task is evaluated on the full suite of 57 Atari 2600 games, as indicated by the evaluation protocols and comparisons with other algorithms across these games. | Atari_2600_Chopper_Command => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on popular video benchmarks such as Set5 and SuperTexture, as well as other datasets used for frame-based super-resolution experiments. | Vid4_-_4x_upscaling => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. Dynamic evaluation is used to improve the state-of-the-art perplexity on this dataset. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII Human Pose dataset. | FLIC_Elbows => 0.0\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [02:01<1:05:54, 68.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CRN method for Image-to-Image Translation is evaluated on datasets such as Cityscapes and GTA5. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The PNN method is evaluated on the Bing_News dataset for Click-Through Rate Prediction using metrics such as Area Under the Curve (AUC) and Logloss. | AUC, Log_Loss => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  80%|████████  | 48/60 [02:02<00:20,  1.74s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DRCN method is evaluated on the Set5 4x upscaling dataset for Image Super-Resolution using the PSNR (Peak Signal-to-Noise Ratio) metric. The DRCN method's deeper networks achieve better performance, with the 30-layer network exceeding the second-best method, CSCN, by 0.47dB on the 4x scale. | MOS, PSNR, SSIM => 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:02<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DeepMatching_ method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is referred to as 'accuracy@', which measures the proportion of 'correct' pixels from the first image with respect to the total number of pixels. A pixel is considered correct if its match in the second image is closer than a specified threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "Processing batch 4 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is the one that establishes the state of the art with an impressive MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a Mean IoU of 84.62. | PSPNet => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism proposed by Dilin Wang, Chengyue Gong, and Qiang Liu, which achieves a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The Paragraph_vector method for Question Answering tasks has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The DPN-131 method for image classification has been evaluated on the Places365-Standard, PASCAL-S, OSIE, and MIT1003 datasets. | ImageNet => 0.0\n",
      "MuZero achieves the highest score on the Atari_2600_Robotank dataset for the Atari_Games task with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The AWD-LSTM-DOC method is typically evaluated using metrics such as perplexity, cross entropy, and bits-per-character (BPC) on the WikiText-2 dataset for the Language Modelling task. | Number_of_params, Test_perplexity, Validation_perplexity => 0.33\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The Transformer method for Machine Translation is evaluated on various datasets, including nine document-level datasets and two sentence-level datasets across six languages, as mentioned in the paper \"Rethinking Document-level Neural Machine Translation.\" | IWSLT2015_English-German => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method for Natural Language Inference is evaluated on the Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset, and Quora Question Pairs dataset. | SNLI => 0.5\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for Head Pose Estimation, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel dataset. | Sintel-final => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari game dataset implemented by OpenAI Gym, which includes a variety of classic Atari games. | Atari_2600_Montezuma_s_Revenge => 0.0\n",
      "The method 'Discriminative Unsupervised Feature Learning with Convolutional Neural Networks' is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The Deep_Speech method for Speech Recognition is evaluated on datasets such as LibriSpeech, TIMIT, AN4, TEDLIUM, Voxforge, and Common Voice. | Switchboard___Hub500 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [01:57<1:55:19, 117.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find the specific method that achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task. You may need to consult the latest research papers or specific benchmarks related to Atari game scores for the most accurate information. | IQN => 0.0\n",
      "The SVDCNN method for text classification is evaluated on datasets such as AG News, DBpedia, Yelp Review Polarity, and Yahoo Answers. | AG_News => 0.5\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and other image quality metrics like IFC (Information Fidelity Criterion) and NQM (Noise Quality Measure). | PSNR, SSIM => 0.5\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in the model boosts performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as accuracy and ensemble averaging. The evaluation involves reporting results for the best single model as well as the average of accuracies for the top-performing models on validation data. Ensemble models are also evaluated using simple averaging of the answer probabilities predicted by ensemble members. | CNN, Daily_Mail => 1.0\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as detection accuracy and regression loss. However, specific metrics for Pascal3D were not found in the retrieved documents. | Mean_PCK => 0.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question_Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the available resources. The CoNLL-2014 shared task generally uses precision, recall, and F1-score as evaluation metrics for grammatical error detection tasks, but specific details for Ann_PAT_MT are not found. | F0_5 => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH N-gram RNNLM language model is evaluated on datasets derived from the Switchboard and Fisher corpora for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as accuracy and boundary F1-measure (BF). | Mean_IoU => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on the Snips SmartLights dataset and the SNIPS Audio dataset. | LibriSpeech_test-clean => 0.0\n",
      "The available resources did not provide information on the dataset where the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further specific research or access to detailed experimental results may be required to answer this question. | Yelp_Fine-grained_classification => 0.0\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the following datasets for the Person Re-Identification task: Market-1501, DukeMTMC-reID, CUHK03, PRID2011, iLIDS-VID, and VIPeR. | Market-1501 => 0.5\n",
      "The current state-of-the-art method on the Yelp Binary classification dataset for Sentiment Analysis is XLNet. However, specific error scores are not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The CRN method for Image-to-Image Translation is evaluated on datasets that include semantic layouts and synthesized images from models like ForenSynths. However, specific datasets used for CRN were not clearly identified in the search results. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for detailed evaluation metrics. | Score => 0.0\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Atari 2600 Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mIoU (mean Intersection over Union), fwIoU (frequency weighted Intersection over Union), and Pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, which include a wide range of games such as Montezuma's Revenge and others in the Atari suite. The evaluations typically involve testing the trained DQN agent on specific games and measuring performance in terms of average scores achieved. | Atari_2600_Chopper_Command => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII Human Pose dataset. | FLIC_Elbows => 0.0\n",
      "The DR-BiLSTM (Single) model achieves the highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task, outperforming other models including ensemble models when preprocessing is applied. | __Unigram_and_bigram_features => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games.The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset using metrics such as lexical overlap and distribution output. However, specific evaluation metrics for this method on the QASent dataset were not found in the retrieved documents. | MAP, MRR => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with fewer parameters than previous models. | 300D_Residual_stacked_encoders => 0.0\n",
      " | Atari_2600_River_Raid => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as the REDS VTSR dataset and other popular video benchmarks like Set5 and SuperTexture. | Vid4_-_4x_upscaling => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face__Easy_ dataset for the Face Detection task using metrics such as detection accuracy. Specifically, FDNet1.0 achieves a detection accuracy of 95.9% on the easy set of the WIDER FACE validation dataset. | AP => 0.0\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [02:46<1:14:25, 77.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated using the metrics AUC (Area Under the ROC Curve) and logloss for the Click-Through Rate Prediction task on the Bing_News dataset. | AUC, Log_Loss => 1.0\n",
      "The DeepMatching_ method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using the metric of 'accuracy@', which measures the proportion of correctly matched pixels compared to the total number of pixels. A pixel is considered correct if its match in the second image is closer than a specified threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The DRCN method is evaluated on the Set5 - 4x upscaling dataset for Image Super-Resolution using the PSNR (Peak Signal-to-Noise Ratio) metric. The DRCN method's performance is compared to other methods, and it achieves higher PSNR values, indicating better image quality. | MOS, PSNR, SSIM => 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:46<00:00,  2.78s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image_Generation task are not explicitly mentioned in the retrieved documents. However, common metrics for evaluating image generation tasks on CIFAR-10 include Inception Score and Fréchet Inception Distance (FID). | NLL_Test => 0.0\n",
      "The available resources did not provide specific information on the dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further investigation or access to specific research papers or datasets may be required to obtain this information. | Atari_2600_Video_Pinball => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19716666666666666"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)\n",
    "batch_num = 4\n",
    "iterative_monkey.thread_safe_evaluator_batch(toolqa_test, optimized_actor_agent,batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
