{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following variables to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USR_NAME = 'shirwu'\n",
    "TOOL_QA_ROOT = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'easy'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "# dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d9e1becf484072bfef425d60052113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023ab8936a174abb9cd359dd4c77565d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e39e90051a94df083ec60b33417e4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d9fbb381f24f058eeeaba350de883a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d48277a58a54f81a90209661a7bbfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Disable all INFO logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Silence all loggers that might be chatty\n",
    "loggers_to_silence = [\n",
    "    \"httpx\",\n",
    "    \"httpcore\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"dspy\",\n",
    "    \"langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"tiktoken\",\n",
    "    \"asyncio\",\n",
    "    \"faiss\",\n",
    "    \"anthropic\"\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_silence:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable tokenizer parallelism warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'What is the corresponding Medium_Human-Normalized_Score score of the Ape-X method on Atari-57 dataset for Atari_Games task?', 'answer': '434.1%'}) (input_keys={'question', 'paper_id'})\n",
      "physics | 434.1% => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_example(example, signature):\n",
    "    try:\n",
    "        avatar = Avatar(\n",
    "            signature,\n",
    "            tools=tools,\n",
    "            verbose=False,\n",
    "        )\n",
    "        prediction = avatar(**example.inputs().toDict())\n",
    "\n",
    "        return metric(example, prediction)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "# process_example(tool_qa[0], ToolQASignature)\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(process_example, example, signature) for example in test_set]\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            total_score += future.result()\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        total_score += process_example(example, signature)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APICallMetrics:\n",
    "    timestamp: datetime\n",
    "    tool_name: str\n",
    "    tokens_in: int = 0\n",
    "    tokens_out: int = 0\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AvatarMetrics:\n",
    "    total_calls: int = 0\n",
    "    total_tokens_in: int = 0\n",
    "    total_tokens_out: int = 0\n",
    "    total_execution_time: float = 0.0\n",
    "    calls_by_tool: Dict[str, int] = field(default_factory=dict)\n",
    "    api_call_history: List[APICallMetrics] = field(default_factory=list)\n",
    "    \n",
    "    def add_call(self, metrics: APICallMetrics):\n",
    "        self.total_calls += 1\n",
    "        self.total_tokens_in += metrics.tokens_in\n",
    "        self.total_tokens_out += metrics.tokens_out\n",
    "        self.total_execution_time += metrics.execution_time\n",
    "        self.calls_by_tool[metrics.tool_name] = self.calls_by_tool.get(metrics.tool_name, 0) + 1\n",
    "        self.api_call_history.append(metrics)\n",
    "    \n",
    "    def merge(self, other: 'AvatarMetrics'):\n",
    "        \"\"\"Merge another AvatarMetrics instance into this one\"\"\"\n",
    "        self.total_calls += other.total_calls\n",
    "        self.total_tokens_in += other.total_tokens_in\n",
    "        self.total_tokens_out += other.total_tokens_out\n",
    "        self.total_execution_time += other.total_execution_time\n",
    "        for tool, count in other.calls_by_tool.items():\n",
    "            self.calls_by_tool[tool] = self.calls_by_tool.get(tool, 0) + count\n",
    "        self.api_call_history.extend(other.api_call_history)\n",
    "\n",
    "    def estimate_cost(self, model_name: str = \"gpt-4\") -> float:\n",
    "        pricing = {\n",
    "            \"gpt-4\": {\"input\": 2.5, \"output\": 10.0},\n",
    "        }\n",
    "        if model_name not in pricing:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        rates = pricing[model_name]\n",
    "        input_cost = (self.total_tokens_in / 1000000) * rates[\"input\"]\n",
    "        output_cost = (self.total_tokens_out / 1000000) * rates[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "class AvatarWithMetrics(Avatar):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metrics = AvatarMetrics()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error counting tokens: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _wrapped_tool_call(self, tool, input_text: str) -> str:\n",
    "        start_time = time.time()\n",
    "        tokens_in = self._count_tokens(input_text)\n",
    "        \n",
    "        try:\n",
    "            result = tool.run(input_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tool execution error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            execution_time = time.time() - start_time\n",
    "            tokens_out = self._count_tokens(str(result))\n",
    "            \n",
    "            metrics = APICallMetrics(\n",
    "                timestamp=datetime.now(),\n",
    "                tool_name=tool.name,\n",
    "                tokens_in=tokens_in,\n",
    "                tokens_out=tokens_out,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self.metrics.add_call(metrics)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        metrics = APICallMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            tool_name=\"main_llm\",\n",
    "            tokens_in=self._count_tokens(str(args) + str(kwargs)),\n",
    "            tokens_out=self._count_tokens(str(result)),\n",
    "            execution_time=total_time\n",
    "        )\n",
    "        self.metrics.add_call(metrics)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for example in test_set:\n",
    "            def process_with_metrics(example=example):\n",
    "                try:\n",
    "                    avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "                    prediction = avatar(**example.inputs().toDict())\n",
    "                    return metric(example, prediction), avatar.metrics\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return 0, AvatarMetrics()\n",
    "\n",
    "            futures.append(executor.submit(process_with_metrics))\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            score, metrics = future.result()\n",
    "            total_score += score\n",
    "            # Only combine token counts and call counts, not execution times\n",
    "            combined_metrics.total_calls += metrics.total_calls\n",
    "            combined_metrics.total_tokens_in += metrics.total_tokens_in\n",
    "            combined_metrics.total_tokens_out += metrics.total_tokens_out\n",
    "            for tool, count in metrics.calls_by_tool.items():\n",
    "                combined_metrics.calls_by_tool[tool] = combined_metrics.calls_by_tool.get(tool, 0) + count\n",
    "            combined_metrics.api_call_history.extend(metrics.api_call_history)\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    combined_metrics.total_execution_time = total_execution_time\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        try:\n",
    "            avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "            prediction = avatar(**example.inputs().toDict())\n",
    "            score = metric(example, prediction)\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in avatar.metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def format_metrics_report(metrics: AvatarMetrics, model_name: str = \"gpt-4\") -> str:\n",
    "    cost = metrics.estimate_cost(model_name)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Avatar Execution Metrics Report\n",
    "==============================\n",
    "Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "Total API Calls: {metrics.total_calls}\n",
    "Total Tokens: {metrics.total_tokens_in + metrics.total_tokens_out:,} ({metrics.total_tokens_in:,} in, {metrics.total_tokens_out:,} out)\n",
    "Estimated Cost: ${cost:.4f}\n",
    "\n",
    "Average Time per Call: {metrics.total_execution_time / metrics.total_calls:.2f} seconds\n",
    "\n",
    "Tool Usage Breakdown:\n",
    "-------------------\n",
    "\"\"\"\n",
    "    for tool, count in sorted(metrics.calls_by_tool.items()):\n",
    "        report += f\"{tool}: {count} calls\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:29<29:28, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "DeepId2 method achieved an accuracy score of 99.15% on the Labeled Faces in the Wild dataset for the Face Verification task. | 99.15% => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:36<15:44, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Train Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is 94.8%. | 94.8 => 1.0\n",
      "The corresponding Inception score of the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The UAS score of the Arc-hybrid method on the Penn_Treebank dataset for the Dependency_Parsing task is 94.51%. | 93.56 => 0.0\n",
      "The corresponding Percentage_error score of the Bi-LSTM___skip_connections_w__CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n",
      "The BLEU-2 score of the LeakGAN method on the Chinese Poems dataset for the Text Generation task is 0.456. | 0.881 => 0.0\n",
      "The corresponding MAP score of the subCNN method on the PASCAL_VOC_2007 dataset for the Object Detection task is not explicitly mentioned in the retrieved results. However, the QSQS algorithm, which is an improvement over traditional methods, achieved a MAP score of 75.11% on the PASCAL VOC 2007 dataset. | 68.5% => 0.0\n",
      "The corresponding accuracy score of the MAML method on the OMNIGLOT 1-Shot Learning dataset for Few-Shot Image Classification is approximately 98.83% for 5-way classification. | 98.7% => 0.0\n",
      "The accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n",
      "The corresponding AP score of the FDNet method on the WIDER Face Easy dataset for the Face Detection task is not available in the provided search results. The search results primarily discuss FDNet in the context of time series forecasting, not face detection. | 0.950 => 0.0\n",
      "The F-Measure score of the PSENet-1s method on the IC17-MLT dataset for the Scene_Text_Detection task is 72.45%. | 72.45% => 1.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task is not readily available in the searched resources. Further specific resources or publications may be needed to find this information. | 26.1 => 0.0\n",
      "The corresponding Accuracy score of the C-LSTM method on the SST-2 Binary classification dataset for the Sentiment Analysis task is 94.96%. | 87.8 => 0.0\n",
      "The corresponding F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.91. | 93.91 => 1.0\n",
      "The average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 78.14. | 79.15 => 0.0\n",
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The F1 score for the Neural-CRF_AE method on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is not directly available from the search results. Further specific research or access to the original paper or dataset results may be required to obtain this information. | 92.29 => 0.0\n",
      "The corresponding Accuracy score of the Sample_Clustering method on the CUB-200 0-Shot Learning dataset for the Few-Shot Image Classification task is 44.3%. |  44.3% => 1.0\n",
      "The corresponding Inception score of the BigGAN-deep method on the ImageNet_128x128 dataset for the Conditional Image Generation task is 124.5. | 166.5 => 0.0\n",
      "The specific Viewpoint_I_AEPE score for the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is not directly available from the search results. Further detailed research or access to specific datasets or publications may be required to obtain this information. | 5.99 => 0.0\n",
      "The corresponding Mean_IoU score of the ResNet-38_MS_COCO method on the PASCAL_VOC_2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "The corresponding Aspect score of the LSTM-LOC method on the Sentihood dataset for the Aspect-Based Sentiment Analysis task is 69.3 and 81.9. | 69.3 => 0.5\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is approximately 0.88 for the whole tumor, 0.83 for the tumor core, and 0.77 for the enhancing tumor. | 0.88 => 1.0\n",
      "The F1 score for the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task is not explicitly found in the available resources. Further specific research or access to the original paper detailing CHFusion's performance would be required to obtain this information. | 55.3% => 0.0\n",
      "The Validation_perplexity score of the AWD-LSTM-MoS + Partial Shuffle method on the Penn Treebank Word Level dataset for Language Modelling is 53.92. | 55.89 => 0.0\n",
      "The corresponding Percentage_correct score of the ACN method on the CIFAR-100 dataset for the Image_Classification task is 66.3%. | 66.3 => 1.0\n",
      "The corresponding MAP score of the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly mentioned in the retrieved documents. Further specific details might be found in the original research paper or related publications. | 0.750 => 0.0\n",
      "The F-Measure score of the DeepFlux method on the SK-LARGE dataset for the Object Skeleton Detection task is 0.738. | 0.732 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [01:34<19:58, 21.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mIoU score of the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task is not explicitly mentioned in the retrieved information. However, the ICNet method, which is another approach, yields an mIoU of 69.5% and can be boosted to 70.6% with additional data. | 66.63 => 0.0\n",
      "The AUC score for the R2U-Net method on the LUNA dataset for the Lung Nodule Segmentation task is not explicitly found in the available resources. The search results did not provide a direct answer to the AUC score for this specific dataset and task. | 0.9889 => 0.0\n",
      "The BLEU score for the Enc-Dec_Att__char_ method on the WMT2015 English-German dataset for the Machine Translation task is not readily available from the sources accessed. Further specific research or access to the original paper or dataset might be required to obtain this information. | 23.45 => 0.0\n",
      "The MAE score for the Regularized Deep Regressor method on the UNBC-McMaster Shoulder Pain dataset for the Pain Intensity Regression task is 0.389. | 0.389 => 1.0\n",
      "The Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is not explicitly found in the retrieved results. Further specific searches or access to the original paper detailing the PDF method might be necessary to obtain this information. | 84.14 => 0.0\n",
      "The Parameters score for the 200D_decomposable_attention_model_with_intra-sentence_attention method on the SNLI dataset is not explicitly mentioned in the retrieved results. However, the model is noted for achieving state-of-the-art results with significantly fewer parameters compared to previous models. | 580k => 0.0\n",
      "The Percentage_correct score of the ResNet_ELU method on the CIFAR-100 dataset for the Image Classification task is not available in the retrieved or searched results. | 73.5 => 0.0\n",
      "The corresponding Viewpoint_II_AEPE score of the DeepMatching method on the HPatches dataset for Dense Pixel Correspondence Estimation task is not directly available from the retrieved sources. Further specific research or access to detailed experimental results from relevant publications may be required to obtain this information. | 4.63 => 0.0\n",
      "The Log_Loss score of the FNN method on the Criteo dataset for the Click-Through Rate Prediction task is not directly available from the retrieved sources. | 0.45738 => 0.0\n",
      "The corresponding Percentage_error score of the Microsoft_2016b method on the Switchboard___Hub500 dataset for the Speech_Recognition task is 6.2%. | 5.8 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   8%|▊         | 5/60 [01:37<13:30, 14.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object Detection task is not explicitly found in the available resources. Further specific resources or publications may be needed to obtain this information. | 76.2% => 0.0\n",
      "The Mean_IoU score for the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task is not readily available from the current search results. It might require further specific research or access to detailed benchmark reports or publications. | 82.0% => 0.0\n",
      "The corresponding Accuracy score of the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task is not explicitly available in the retrieved data. Further specific research or access to the original paper or dataset results may be required to obtain this information. | 57.14 => 0.0\n",
      "The PSNR score of the JMPF_ method on the BSD100_-_4x_upscaling dataset for the Image Super-Resolution task is not available in the retrieved or searched documents. | 26.87 => 0.0\n",
      "The Class_IOU score for the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is not explicitly available in the retrieved or searched results. The available information does not provide a direct Class_IOU score for CoGAN on this specific task. |  0.08 => 0.0\n",
      "The TAR___FAR_0_01 score of the Triplet_probabilistic_embedding method on the IJB-A dataset for the Face_Verification task is 90%. | 90% => 1.0\n",
      "The CBS-1___ESIM method's Test_Accuracy score on the SNLI dataset for the Natural_Language_Inference task is not explicitly mentioned in the retrieved information. However, the ESIM model, which is a baseline, has a test-set accuracy of 88.0% on the SNLI dataset. | 86.73 => 0.0\n",
      "The Number_of_params score for the mLSTM___dynamic_eval method on the Text8 dataset for the Language_Modelling task is not explicitly available in the retrieved documents. However, the mLSTM + dynamic eval model has 46M parameters as per the web search results. | 45M => 0.0\n",
      "The corresponding Accuracy score of the MEAN method on the MR dataset for the Sentiment_Analysis task is not directly available from the retrieved sources. Further specific research or access to the original study or dataset results may be required to obtain this information. | 84.5 => 0.0\n",
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task is not available in the retrieved or searched results. | 29.56 => 0.0\n",
      "The Percentage_correct score of the MRN___global_features method on the COCO_Visual_Question_Answering__VQA__real_images_1_0_open_ended dataset for the Visual_Question_Answering task could not be found using the available tools. | 61.84 => 0.0\n",
      "The Checkerboards method achieves a log-average miss rate of 18.3% on the Reasonable subset of the Caltech dataset for the Pedestrian Detection task. | 17.1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  10%|█         | 6/60 [01:46<11:30, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Frame__fps_ score of the CRF-RNN method on the Cityscapes dataset for Real-Time Semantic Segmentation task is not directly available from the search results. Further specific research or direct access to the original paper or dataset results may be required to obtain this information. | 1.4 => 0.0\n",
      "The corresponding NLL_Test score of the Conv_DRAW method on the CIFAR-10 dataset for the Image_Generation task is -1791.15. | 3.58 => 0.0\n",
      "The Unigram_Acc score of the ASR method on the SearchQA dataset for Open-Domain Question Answering task is not readily available from the current sources. Further specific research or access to detailed experimental results from relevant papers may be required to obtain this information. | 41.3 => 0.0\n",
      "The corresponding AP score of the ACF-WIDER method on the WIDER_Face_Hard dataset for the Face Detection task is not directly available from the search results. Further specific research or access to the original paper or dataset results might be required to obtain this information. | 0.290 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  35%|███▌      | 21/60 [01:49<00:50,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Percentage_correct score for the Deep Networks with Internal Selective Attention through Feedback Connections method on the CIFAR-100 dataset for the Image Classification task is not explicitly mentioned in the available resources. The paper mentions that dasNet outperforms the previous state-of-the-art model on unaugmented datasets, but specific percentage scores are not provided. | 66.2 => 0.0\n",
      "The corresponding F1 score of the Yang et al. method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task is not explicitly found in the retrieved documents. Further specific details about the Yang et al. method's performance on this dataset may require direct access to the original paper or additional specific resources. | 91.26 => 0.0\n",
      "The Params score of the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for the Language_Modelling task could not be found using the available tools. | 22M => 0.0\n",
      "The Test_perplexity score of the AWD-LSTM-DOC method on the Penn Treebank Word Level dataset for the Language Modelling task is not explicitly found in the retrieved documents. Further specific searches or access to the original research paper might be necessary to obtain this information. | 52.38 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:54<00:00,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hits@1 score of the ComplEx method on the WN18 dataset for the Link Prediction task is approximately 0.089. | 0.936 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score, metrics = multi_thread_executor(toolqa_test, ToolQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.24\n",
      "\n",
      "Avatar Execution Metrics Report\n",
      "==============================\n",
      "Execution Time: 115.02 seconds\n",
      "Total API Calls: 60\n",
      "Total Tokens: 110,098 (2,194 in, 107,904 out)\n",
      "Estimated Cost: $1.0845\n",
      "\n",
      "Average Time per Call: 1.92 seconds\n",
      "\n",
      "Tool Usage Breakdown:\n",
      "-------------------\n",
      "main_llm: 60 calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Score: {score:.2f}\")\n",
    "print(format_metrics_report(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "iterative_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=3,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HD-CNN method achieves a reduction in top-1 error by 2.65%, 3.1%, and 1.1% on the CIFAR-100 dataset, but the exact percentage error score is not explicitly mentioned in the retrieved document. | 32.62 => 0.0\n",
      "The corresponding Bounding_Box_AP score of the CornerNet method on the COCO dataset for the Object Detection task is 42.2% AP. | 42.1 => 0.0\n",
      "The corresponding accuracy score of the FaceNet method on the YouTube_Faces_DB dataset for the Face_Verification task is 95.12%. | 95.12% => 1.0\n",
      "The corresponding F1_Full score of the Transition-based improved aligner ensemble method on the LDC2014T12 dataset for the AMR Parsing task is 68.4 Smatch F1 score. | 0.68 => 0.0\n",
      "The Percentage_error score of the Stochastic_Pooling method on the SVHN dataset for the Image_Classification task is approximately 1% relative test error decrease. | 2.8 => 0.0\n",
      "The corresponding Model_Entropy score of the PixelRNN method on the CIFAR-10 dataset for the Image_Generation task is not readily available in the searched resources. It might not be explicitly reported in the literature or online sources. | 3 => 0.0\n",
      "The Bounding Box AP score of the Cascade R-CNN method on the COCO dataset for the Object Detection task is 43.7. | 42.8 => 0.0\n",
      "The corresponding accuracy score of the PixelGAN_Autoencoders method on the MNIST dataset for the unsupervised image classification task is improved by 0.3% over the previous state-of-the-art. | 94.73 => 0.0\n",
      "The Parameters score for the KIM method on the SNLI dataset for the Natural Language Inference task is not explicitly available in the search results. However, the KIM method achieves an accuracy of 88.6% on the SNLI dataset. | 4.3m => 0.0\n",
      "The corresponding MAP score of the WSDDN-Ens method on the PASCAL_VOC_2007 dataset for the Weakly Supervised Object Detection task is 39.3%. | 39.3 => 1.0\n",
      "The Percentage_correct score of the Tree_Max-Avg_pooling method on the CIFAR-10 dataset for the Image_Classification task is not readily available from the sources searched. Further specific research or access to the original paper or dataset results may be required to obtain this information. | 94.0 => 0.0\n",
      "The corresponding Laptop__Acc_ score of the TD-LSTM method on the SemEval_2014_Task_4_Sub_Task_2 dataset for Aspect-Based Sentiment Analysis is 71.83%. | 68.13 => 0.0\n",
      "The corresponding Mean_NME_ score of the VRN-Guided method on the Florence dataset for the 3D Face Reconstruction task is 5.2667%. | 5.2667% => 1.0\n",
      "The F1 score of the DialogueRNN method on the IEMOCAP dataset for the Emotion Recognition in Conversation task is 63.5%. | 64.5% => 0.0\n",
      "The Parameters score of the 100D_LSTM_encoders method on the SNLI dataset for the Natural Language Inference task is 220k. | 220k => 1.0\n",
      "The BLEU score of the BPE word segmentation method on the WMT2015 English-German dataset for the Machine Translation task is 22.8. | 22.8 => 1.0\n",
      "The corresponding MRR score of the Attentive_LSTM method on the WikiQA dataset for the Question Answering task is 0.7069. | 0.7069 => 1.0\n",
      "The PSNR score of the LapSRN method on the Urban100 4x upscaling dataset for Image Super-Resolution is 25.21. | 25.21 => 1.0\n",
      "The Log_Loss score of the DeepFM method on the Company_ dataset for the Click-Through Rate Prediction task is not explicitly available in the retrieved documents or web search results. However, it is mentioned that DeepFM outperforms other models in terms of Logloss by 1.1% and 4.0% on Company∗ and Criteo datasets, respectively. | 0.02618 => 0.0\n",
      "The Bounding_Box_AP score of the D-RFCN ResNet-101 6 scales method on the COCO dataset for the Object Detection task is 43.4. | 40.9 => 0.0\n",
      "The Classification Accuracy score of the ADDA method on the SVHN-to-MNIST dataset for the Unsupervised Image-To-Image Translation task is 76.0%. | 76.0% => 1.0\n",
      "The corresponding Accuracy-CN score of the GA___feature___fix_L_w_ method on the Children_s_Book_Test dataset for the Question_Answering task is 0.475. | 70.7% => 0.0\n",
      "The Parameters score for the 50D_stacked_TC-LSTMs method on the SNLI dataset for the Natural Language Inference task is 190k. | 190k => 1.0\n",
      "The Classification_Error score of the IcGAN method on the RaFD dataset for the Image-to-Image Translation task is not readily available from the searched sources. It may require access to specific research papers or datasets that report this metric. | 8.07% => 0.0\n",
      "The average score of the Asymmetric_tri-training method on the Multi-Domain_Sentiment_Dataset for the Sentiment_Analysis task is 76.17. | 78.39 => 0.0\n",
      "The Top-5 Accuracy score of the ResNeXt-101 method on the ImageNet dataset for the Image Classification task is 85.4%. | 95.6% => 0.0\n",
      "The TAR___FAR_0_01 score of the NAN method on the IJB-A dataset for the Face_Verification task is not explicitly mentioned in the retrieved text. Further specific details might be needed from the original source or dataset documentation. | 94.10% => 0.0\n",
      "The corresponding LAS score of the CVT Multi-Task method on the Penn Treebank dataset for the Dependency Parsing task is 95.02%.The corresponding MAP score of the Online_Instance_Classifier_Refinement method on the ImageNet dataset for the Weakly_Supervised_Object_Detection task was not found in the available resources. | 6 => 0.0\n",
      " | 95.02 => 1.0\n",
      "The corresponding Train_Accuracy score of the 300D_LSTM_encoders method on the SNLI dataset for the Natural Language Inference task is not explicitly available in the retrieved documents. The available information primarily discusses the ESIM model and its performance on the SNLI dataset. | 83.9 => 0.0\n",
      "The corresponding MAP score of the SVDNet method on the Market-1501 dataset for the Person_Re-Identification task is not explicitly mentioned in the retrieved documents. However, the SVDNet method achieves a rank-1 accuracy of 84.41% on the Market-1501 dataset with ResNet-50. | 62.1 => 0.0\n",
      "The Recall_50 score of the Mult-DAE method on the Netflix dataset for the Collaborative_Filtering task is not explicitly found in the retrieved documents. Further specific searches or access to the original research paper might be required to obtain this information. | 0.438 => 0.0\n",
      "The F1 score of the CollaboNet method on the JNLPBA dataset for the Named Entity Recognition (NER) task is not explicitly mentioned in the retrieved documents. However, it is noted that CollaboNet achieves state-of-the-art performance and improves F1 scores compared to baselines. For precise F1 score details, consulting the original paper or additional specific resources may be necessary. | 78.58 => 0.0\n",
      "The BLEU score of the Word-level LSTM with attention method on the IWSLT2015 German-English dataset for the Machine Translation task is 20.2. | 20.2 => 1.0\n",
      "The Time__ms_ score for the DeepLab method on the Cityscapes dataset for Real-Time Semantic Segmentation is not explicitly found in the available resources. Further specific research or direct access to the relevant paper or dataset results may be required. | 4000 => 0.0\n",
      "The corresponding MOS score of the ESPCN method on the Set5 - 4x upscaling dataset for the Image Super-Resolution task is not readily available in the retrieved or searched documents. Further specific research or direct access to the dataset results may be required to obtain this information. | 2.89 => 0.0\n",
      "The BLEU score for the ConvS2S__MLE_SLE_ method on the IWSLT2015 German-English dataset for the Machine Translation task is not directly available from the retrieved sources. Further specific research or access to the original paper or dataset results may be required to obtain this information. | 32.84 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [01:04<42:05, 64.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Medium_Human-Normalized_Score score of the Ape-X method on the Atari-57 dataset for the Atari_Games task is not directly available from the retrieved sources. Further specific research or access to detailed experimental results from the original Ape-X method publication may be required to obtain this information. | 434.1% => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  20%|██        | 8/40 [01:06<03:13,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PSNR score of the FAFR_ method on the BSD100_-_4x_upscaling dataset for the Image Super-Resolution task is not available in the retrieved or searched documents. Further specific resources or publications may be needed to find this information. | 26.91 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:07<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding ROUGE-2 score of the FTSum_g method on the GigaWord dataset for the Text Summarization task is 17.65. | 17.65 => 1.0\n",
      "Average Score: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new instruction: New Instruction: You will be given `Tools`, which is a list of resources available to accomplish the `Goal`. Your task is to decide which tool to use and what input values to provide based on the user query. To enhance the effectiveness of your actions, ensure that you select the most appropriate tool for the type of information being sought. For instance, use `ARXIV_SEARCH` for specific academic paper-related queries and `WEB_SEARCH` for broader information gathering. This strategic selection will help in obtaining more relevant and precise results.\n",
      "\n",
      "When structuring your queries, aim for specificity and detail. Well-structured queries lead to more precise and relevant outcomes. Avoid vague or general queries, as they can result in incomplete or irrelevant information. Additionally, consider using multiple tools to cross-verify the information you gather. This practice will enhance the accuracy and completeness of your results, ensuring that the information is both comprehensive and reliable.\n",
      "\n",
      "Incorporate a feedback loop into your process, where the results from one tool can inform and refine the queries for another tool. This iterative approach will help you gather more comprehensive information and improve the overall quality of your output. Remember, you can use a tool multiple times with different input queries if applicable, and you also have the option to provide the final answer directly without using any tools if the situation allows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding LAS score of the CVT Multi-Task method on the Penn Treebank dataset for the Dependency Parsing task is 95.02%. | 95.02 => 1.0\n",
      "The Bounding Box AP score of the CornerNet method on the COCO dataset for the Object Detection task is 42.2%. | 42.1 => 0.0\n",
      "The F1 score of the DialogueRNN method on the IEMOCAP dataset for Emotion Recognition in Conversation is 71.08%.The corresponding MRR score of the Attentive_LSTM method on the WikiQA dataset for the Question Answering task is 0.7069. | 0.7069 => 1.0\n",
      " | 64.5% => 0.0\n",
      "The PSNR score of the LapSRN method on the Urban100 dataset for 4x upscaling in the Image Super-Resolution task is 25.21. | 25.21 => 1.0\n",
      "The corresponding average score of the Asymmetric tri-training method on the Multi-Domain Sentiment Dataset for the sentiment analysis task is 78.39. | 78.39 => 1.0\n",
      "The Top-5 Accuracy score of the ResNeXt-101 method on the ImageNet dataset for the Image Classification task is 98.0%. | 95.6% => 0.0\n",
      "The Percentage error score of the Stochastic Pooling method on the SVHN dataset for the Image Classification task is 2.8%. | 2.8 => 1.0\n",
      "The corresponding Laptop__Acc_ score of the TD-LSTM method on the SemEval_2014_Task_4_Sub_Task_2 dataset for Aspect-Based Sentiment Analysis is 71.83%. | 68.13 => 0.0\n",
      "The F1_Full score of the Transition-based improved aligner ensemble method on the LDC2014T12 dataset for the AMR Parsing task is 0.73. | 0.68 => 0.0\n",
      "The corresponding accuracy score of the FaceNet method on the YouTube Faces DB dataset for the Face Verification task is 95.12%. | 95.12% => 1.0\n",
      "The Parameters score of the 100D_LSTM_encoders method on the SNLI dataset for the Natural Language Inference task is 220k. | 220k => 1.0\n",
      "The corresponding MAP score of the WSDDN-Ens method on the PASCAL_VOC_2007 dataset for the Weakly Supervised Object Detection task is 39.3%. | 39.3 => 1.0\n",
      "The Classification_Error score of the IcGAN method on the RaFD dataset for the Image-to-Image Translation task is 8.07%. | 8.07% => 1.0\n",
      "The corresponding Mean NME score of the VRN-Guided method on the Florence dataset for the 3D Face Reconstruction task is 5.2667%. | 5.2667% => 1.0\n",
      "The BLEU score for the Word-level LSTM with attention method on the IWSLT2015 German-English dataset for the Machine Translation task is 20.2. | 20.2 => 1.0\n",
      "The corresponding accuracy score of the PixelGAN_Autoencoders method on the MNIST dataset for the unsupervised image classification task is 94.73%. | 94.73 => 1.0\n",
      "The BLEU score for the BPE word segmentation method on the WMT2015 English-German dataset for the Machine Translation task is 22.8. | 22.8 => 1.0\n",
      "The HD-CNN method achieves a percentage error score of approximately 26.55% on the CIFAR-100 dataset for the image classification task. | 32.62 => 0.0\n",
      "The Bounding Box AP score of the D-RFCN + SNIP method with ResNet-101 and multi-scale on the COCO dataset is 43.4. | 40.9 => 0.0\n",
      "The Bounding Box AP score of the Cascade R-CNN method on the COCO dataset for the Object Detection task is approximately 42.8. | 42.8 => 1.0\n",
      "The F1 score of the CollaboNet method on the JNLPBA dataset for the Named Entity Recognition (NER) task is 78.58. | 78.58 => 1.0\n",
      "The corresponding ROUGE-2 score of the FTSum_g method on the GigaWord dataset for the Text Summarization task is 17.65. | 17.65 => 1.0\n",
      "The corresponding Parameters score of the 50D_stacked_TC-LSTMs method on the SNLI dataset for the Natural Language Inference task is 190k. | 190k => 1.0\n",
      "The corresponding Accuracy-CN score of the GA_feature_fix_L_w_ method on the Children_s_Book_Test dataset for the Question_Answering task is 0.475. | 70.7% => 0.0\n",
      "The corresponding Classification_Accuracy score of the ADDA method on the SVHN-to-MNIST dataset for the Unsupervised Image-To-Image Translation task is 76.0%. | 76.0% => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:29<19:02, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Medium Human-Normalized Score for the Ape-X method on the Atari-57 dataset is 21322.5. | 434.1% => 0.0\n",
      "The Model_Entropy score for the PixelRNN method on the CIFAR-10 dataset for the Image Generation task is not readily available from the current search results. It appears that specific information regarding the Model_Entropy score for PixelRNN is missing from the accessible literature and web resources. | 3 => 0.0\n",
      "The Log_Loss score for the DeepFM method on the Company_ dataset for the Click-Through Rate Prediction task is not explicitly available in the search results. The available information indicates that DeepFM outperforms other models in terms of AUC and Logloss on similar datasets, but the specific Log_Loss score for the Company_ dataset is not provided. | 0.02618 => 0.0\n",
      "The KIM method on the SNLI dataset for the Natural Language Inference task achieves an accuracy of 88.6%. However, specific details about the Parameters score were not found in the available resources. | 4.3m => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   8%|▊         | 3/40 [00:38<06:50, 11.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Train_Accuracy score of the 300D_LSTM_encoders method on the SNLI dataset for the Natural Language Inference task is not explicitly found in the available resources. However, related models and methods have achieved accuracy scores around 86% to 89% on the SNLI dataset. | 83.9 => 0.0\n",
      "The corresponding Recall_50 score of the Mult-DAE method on the Netflix dataset for the Collaborative Filtering task is 0.380. | 0.438 => 0.0\n",
      "The corresponding MAP score of the SVDNet method on the Market-1501 dataset for the Person Re-Identification task is not explicitly mentioned in the available resources. However, the rank-1 accuracy is reported to be improved significantly, indicating enhanced performance. | 62.1 => 0.0\n",
      "The MOS score for the ESPCN method on the Set5 - 4x upscaling dataset for the Image Super-Resolution task is not readily available in the current search results. It appears that the specific MOS score for ESPCN is not commonly reported or may not have been published in the sources searched. | 2.89 => 0.0\n",
      "The BLEU score for the ConvS2S MLE SLE method on the IWSLT2015 German-English dataset for the Machine Translation task could not be found in the available resources. | 32.84 => 0.0\n",
      "The specific inference time in milliseconds (ms) for the DeepLab method on the Cityscapes dataset for real-time semantic segmentation is not readily available from the search results. The DeepLab series, including DeepLabv3+, is known for achieving real-time semantic segmentation without compromising accuracy, but the exact time in milliseconds was not found in the provided resources. | 4000 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  20%|██        | 8/40 [00:59<03:13,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was unable to find the specific PSNR score for the FAFR_ method on the BSD100 4x upscaling dataset for the Image Super-Resolution task after multiple searches. | 26.91 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  40%|████      | 16/40 [01:05<01:05,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Percentage_correct score of the Tree_Max-Avg_pooling method on the CIFAR-10 dataset for the Image Classification task is 94.0%. | 94.0 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  62%|██████▎   | 25/40 [01:16<00:29,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAP score for the Online Instance Classifier Refinement method on the ImageNet dataset for the Weakly Supervised Object Detection task is 6. | 6 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [02:54<00:00,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TAR_FAR_0_01 score of the NAN method on the IJB-A dataset for the Face Verification task is not readily available from the current search results. It may require accessing specific academic papers or datasets that detail the performance metrics of the NAN method on the IJB-A dataset. | 94.10% => 0.0\n",
      "Average Score: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new instruction: New Instruction: You will be given `Tools`, which is a list of resources available to accomplish the `Goal`. Your task is to decide which tool to use and what input values to provide based on the user query. To enhance the effectiveness of your actions, ensure that you select the most appropriate tool for the type of information being sought. For instance, use `ARXIV_SEARCH` for specific academic paper-related queries and `WEB_SEARCH` for broader information gathering. This strategic selection will help in obtaining more relevant and precise results. When structuring your queries, aim for specificity and detail. Well-structured queries lead to more precise and relevant outcomes. Avoid vague or general queries, as they can result in incomplete or irrelevant information. For example, include specific dataset names, methods, and metrics in your queries to improve the relevance of the results.\n",
      "\n",
      "Incorporate a feedback loop into your process, where the results from one tool can inform and refine the queries for another tool. This iterative approach will help you gather more comprehensive information and improve the overall quality of your output. Ensure that the logic for selecting tools is aligned with the task requirements. If the task involves finding specific scores or metrics, ensure that the query is detailed enough to extract this information from the appropriate source. Remember, you can use a tool multiple times with different input queries if applicable, and you also have the option to provide the final answer directly without using any tools if the situation allows.\n",
      "\n",
      "Additionally, consider using multiple tools to cross-verify the information you gather. This practice will enhance the accuracy and completeness of your results, ensuring that the information is both comprehensive and reliable. Re-evaluate the choice of tools based on the task, ensuring that the tool selected is the most appropriate for the type of information being sought. By addressing these areas, the performance on negative inputs can be significantly improved, leading to more accurate and relevant outcomes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding accuracy score of the FaceNet method on the YouTube Faces DB dataset for the Face Verification task is 95.12%. | 95.12% => 1.0\n",
      "The PSNR score of the LapSRN method on the Urban100 dataset for 4x upscaling in the Image Super-Resolution task is 25.21. | 25.21 => 1.0\n",
      "The corresponding LAS score of the CVT___Multi-Task method on the Penn Treebank dataset for the Dependency Parsing task is 95.02%. | 95.02 => 1.0\n",
      "CornerNet achieves a Bounding Box AP score of 42.2% on the MS COCO dataset for the Object Detection task. | 42.1 => 0.0\n",
      "The Percentage error score of the Stochastic Pooling method on the SVHN dataset for the Image Classification task is 2.8%.The corresponding Parameters score of the 50D_stacked_TC-LSTMs method on the SNLI dataset for the Natural Language Inference task is 190k. | 190k => 1.0\n",
      "The corresponding MAP score of the WSDDN-Ens method on the PASCAL_VOC_2007 dataset for the Weakly Supervised Object Detection task is 39.3%. | 39.3 => 1.0\n",
      "The corresponding accuracy score of the PixelGAN_Autoencoders method on the MNIST dataset for the unsupervised image classification task is 94.73%. | 94.73 => 1.0\n",
      "The Top-5 Accuracy score of the ResNeXt-101 method on the ImageNet dataset for the Image Classification task is 94.1%. | 95.6% => 0.0\n",
      " | 2.8 => 1.0\n",
      "The F1 score of the DialogueRNN method on the IEMOCAP dataset for the Emotion Recognition in Conversation task is 63.5%. | 64.5% => 0.0\n",
      "The BLEU score for the BPE word segmentation method on the WMT2015 English-German dataset for the Machine Translation task is 22.8. | 22.8 => 1.0\n",
      "The Bounding Box AP score for the D-RFCN + SNIP (ResNet-101, multi-scale) method on the COCO dataset for the Object Detection task is 43.4. | 40.9 => 0.0\n",
      "The BLEU score for the Word-level LSTM with attention method on the IWSLT2015 German-English dataset for the Machine Translation task is 20.2. | 20.2 => 1.0\n",
      "The 100D LSTM encoders method on the SNLI dataset for the Natural Language Inference task has a Parameters score of 220k. | 220k => 1.0\n",
      "The corresponding Average score of the Asymmetric_tri-training method on the Multi-Domain_Sentiment_Dataset for the Sentiment_Analysis task is 78.39. | 78.39 => 1.0\n",
      "The HD-CNN method achieves a percentage error of approximately 32.62% on the CIFAR-100 dataset for the image classification task, as it improves the standard CNNs' top-1 error by 2.65%. | 32.62 => 1.0\n",
      "The Classification Accuracy score of the ADDA method on the SVHN-to-MNIST dataset for the Unsupervised Image-To-Image Translation task is 76.0%. | 76.0% => 1.0\n",
      "The TAR@FAR=0.01 score of the NAN method on the IJB-A dataset for the Face Verification task is 0.933. | 94.10% => 0.5\n",
      "The Percentage_correct score of the Tree_Max-Avg_pooling method on the CIFAR-10 dataset for the Image Classification task is 94.0%. | 94.0 => 1.0\n",
      "The corresponding Mean_NME_ score of the VRN-Guided method on the Florence dataset for the 3D Face Reconstruction task is 5.2667%. | 5.2667% => 1.0\n",
      "The corresponding Accuracy-CN score of the GA_feature_fix_L_w_ method on the Children_s_Book_Test dataset for the Question_Answering task could not be found using the available tools. | 70.7% => 0.0\n",
      "The ROUGE-2 score of the FTSum_g method on the GigaWord dataset for the Text Summarization task is 17.65. | 17.65 => 1.0\n",
      "The corresponding MRR score of the Attentive_LSTM method on the WikiQA dataset for the Question Answering task is 0.7069. | 0.7069 => 1.0\n",
      "The Bounding Box AP score of the Cascade R-CNN method on the COCO dataset for the Object Detection task is approximately 43.7%. | 42.8 => 0.0\n",
      "The Log_Loss score for the DeepFM method on the Company_ dataset for the Click-Through Rate Prediction task is 0.8715. | 0.02618 => 0.0\n",
      "The Parameters score for the KIM method on the SNLI dataset for the Natural Language Inference task is not explicitly available in the search results. However, the KIM method achieves an accuracy of 88.6% on the SNLI dataset. | 4.3m => 0.0\n",
      "The Model_Entropy score for the PixelRNN method on the CIFAR-10 dataset for the Image Generation task is not available in the current search results. | 3 => 0.0\n",
      "The corresponding Train_Accuracy score of the 300D_LSTM_encoders method on the SNLI dataset for the Natural Language Inference task is not explicitly mentioned in the available resources. However, related models and configurations achieve accuracy scores around 86% to 88% on the SNLI dataset. | 83.9 => 0.0\n",
      "The F1_Full score for the Transition-based improved aligner ensemble method on the LDC2014T12 dataset for the AMR Parsing task is 0.73. | 0.68 => 0.0\n",
      "The Classification_Error score for the IcGAN method on the RaFD dataset for the Image-to-Image Translation task is not readily available from the current search results. Further detailed research or access to specific academic papers or datasets might be required to obtain this information. | 8.07% => 0.0\n",
      "The Laptop__Acc_ score of the TD-LSTM method on the SemEval 2014 Task 4 Sub Task 2 dataset for Aspect-Based Sentiment Analysis is 71.88. | 68.13 => 0.0\n",
      "The BLEU score for the ConvS2S MLE SLE method on the IWSLT2015 German-English dataset for the Machine Translation task could not be found in the available resources. | 32.84 => 0.0\n",
      "The corresponding F1 score of the CollaboNet method on the JNLPBA dataset for the Named Entity Recognition (NER) task is 78.58. | 78.58 => 1.0\n",
      "The MAP score of the Online Instance Classifier Refinement method on the ImageNet dataset for the Weakly Supervised Object Detection task is 6. | 6 => 1.0\n",
      "The MAP score for the SVDNet method on the Market-1501 dataset for the Person Re-Identification task is not explicitly mentioned in the available resources. However, the rank-1 accuracy is reported to be improved significantly with SVDNet. | 62.1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:37<24:21, 37.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Medium Human-Normalized Score for the Ape-X method on the Atari-57 dataset is not readily available from the current search results. Further detailed research or access to specific academic papers or datasets may be required to obtain this information. | 434.1% => 0.0\n",
      "The MOS score for the ESPCN method on the Set5 4x upscaling dataset for the Image Super-Resolution task is not readily available from the current search results. It appears that specific MOS scores for ESPCN are not commonly reported or may not have been published in the sources searched. | 2.89 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  20%|██        | 8/40 [00:45<02:20,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find the specific PSNR score for the FAFR_ method on the BSD100 4x upscaling Image Super-Resolution task. Further detailed search or a specific paper reference may be needed. | 26.91 => 0.0\n",
      "The Recall_50 score for the Mult-DAE method on the Netflix dataset for the Collaborative Filtering task is not readily available from the current search results. Further detailed research or access to specific academic papers or datasets might be required to obtain this specific metric. | 0.438 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:31<00:00,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific Time__ms_ score for the DeepLab method on the Cityscapes dataset for Real-Time Semantic Segmentation is not readily available from the current search results. It is recommended to refer to the original research papers or official documentation of the DeepLab method for precise performance metrics. | 4000 => 0.0\n",
      "Average Score: 0.4875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new instruction: New Instruction: You will be given `Tools`, which is a list of resources available to accomplish the `Goal`. Your task is to decide which tool to use and what input values to provide based on the user query. To enhance the effectiveness of your actions, ensure that you select the most appropriate tool for the type of information being sought. For instance, use `ARXIV_SEARCH` for specific academic paper-related queries and `WEB_SEARCH` for broader information gathering. This strategic selection will help in obtaining more relevant and precise results. When structuring your queries, aim for specificity and detail. Well-structured queries lead to more precise and relevant outcomes. Avoid vague or general queries, as they can result in incomplete or irrelevant information. For example, include specific dataset names, methods, and metrics in your queries to improve the relevance of the results.\n",
      "\n",
      "Incorporate a feedback loop into your process, where the results from one tool can inform and refine the queries for another tool. This iterative approach will help you gather more comprehensive information and improve the overall quality of your output. Ensure that the logic for selecting tools is aligned with the task requirements. If the task involves finding specific scores or metrics, ensure that the query is detailed enough to extract this information from the appropriate source. Remember, you can use a tool multiple times with different input queries if applicable, and you also have the option to provide the final answer directly without using any tools if the situation allows. Additionally, consider using multiple tools to cross-verify the information you gather. This practice will enhance the accuracy and completeness of your results, ensuring that the information is both comprehensive and reliable.\n",
      "\n",
      "To address the areas identified in the feedback, prioritize the use of `ARXIV_SEARCH` for academic-related queries to access detailed and specific information. Use `WEB_SEARCH` for broader information gathering or when the required information is not found in academic databases. Implement an iterative approach by refining queries based on feedback from the tools, ensuring that queries are specific and detailed, including relevant keywords such as dataset names, methods, and metrics. This will improve the relevance of the results obtained. If the initial query does not yield satisfactory results, refine the query based on the feedback from the tool and try again. By addressing these areas, the performance on negative inputs can be significantly improved, leading to more accurate and relevant outcomes.\n",
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 60.49 seconds\n",
      "                Total API Calls: 6\n",
      "                - Comparator calls: 3\n",
      "                - Feedback instruction calls: 3\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 142,360\n",
      "                - Input tokens: 139,890\n",
      "                - Output tokens: 2,470\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $4.3449\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "result = iterative_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization cost: $4.3449\n",
      "Final score achieved: 0.500\n",
      "\n",
      "Iteration 0:\n",
      "Score: 0.300\n",
      "Comparator tokens in: 34327\n",
      "Comparator tokens out: 448\n",
      "Feedback tokens in: 593\n",
      "Feedback tokens out: 266\n",
      "Execution time: 85.27s\n",
      "\n",
      "Iteration 1:\n",
      "Score: 0.500\n",
      "Comparator tokens in: 55641\n",
      "Comparator tokens out: 434\n",
      "Feedback tokens in: 729\n",
      "Feedback tokens out: 381\n",
      "Execution time: 196.46s\n",
      "\n",
      "Iteration 2:\n",
      "Score: 0.487\n",
      "Comparator tokens in: 47721\n",
      "Comparator tokens out: 469\n",
      "Feedback tokens in: 879\n",
      "Feedback tokens out: 472\n",
      "Execution time: 112.59s\n"
     ]
    }
   ],
   "source": [
    "optimized_actor_agent = result[\"agent\"]\n",
    "optimization_metrics = result[\"metrics\"]\n",
    "\n",
    "# Now you can process the metrics\n",
    "print(f\"Total optimization cost: ${optimization_metrics['total_cost']:.4f}\")\n",
    "print(f\"Final score achieved: {optimization_metrics['final_score']:.3f}\")\n",
    "\n",
    "# Analyze per-iteration performance\n",
    "for iteration in optimization_metrics['iteration_details']:\n",
    "    print(f\"\\nIteration {iteration['iteration']}:\")\n",
    "    print(f\"Score: {iteration['score']:.3f}\")\n",
    "    print(f\"Comparator tokens in: {iteration['comparator_metrics']['tokens_in']}\")\n",
    "    print(f\"Comparator tokens out: {iteration['comparator_metrics']['tokens_out']}\")\n",
    "    print(f\"Feedback tokens in: {iteration['feedback_metrics']['tokens_in']}\")\n",
    "    print(f\"Feedback tokens out: {iteration['feedback_metrics']['tokens_out']}\")\n",
    "    print(f\"Execution time: {iteration['total_iteration_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:09<09:21,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "The accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n",
      "The corresponding MAP score of the subCNN method on the PASCAL VOC 2007 dataset for the Object Detection task is 68.5%. | 68.5% => 1.0\n",
      "The Validation perplexity score of the AWD-LSTM-MoS + Partial Shuffle method on the Penn Treebank Word Level dataset for Language Modelling is 53.92. | 55.89 => 0.0\n",
      "The corresponding Average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 79.15. | 79.15 => 1.0\n",
      "The Mean IoU score of the ResNet-38_MS_COCO method on the PASCAL VOC 2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "The corresponding Percentage error score of the Microsoft 2016b method on the Switchboard + Hub500 dataset for the Speech Recognition task is 5.8%. | 5.8 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:14<06:50,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The Train_Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is 94.8%. | 94.8 => 1.0\n",
      "The BLEU score for the Enc-Dec_Att__char_ method on the WMT2015 English-German dataset for the Machine Translation task is 23.5. | 23.45 => 0.5\n",
      "The corresponding Frame fps score of the CRF-RNN method on the Cityscapes dataset for Real-Time Semantic Segmentation is 1.4 fps. | 1.4 => 1.0\n",
      "The corresponding Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is 83.58%. | 84.14 => 0.0\n",
      "The corresponding Accuracy score of the MAML method on the OMNIGLOT 1-Shot Learning dataset for the Few-Shot Image Classification task is 98.7%. | 98.7% => 1.0\n",
      "The corresponding Accuracy score of the DeepId2 method on the Labeled Faces in the Wild dataset for the Face Verification task is 99.15%. | 99.15% => 1.0\n",
      "The Inception score for the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The Inception score for the BigGAN-deep method on the ImageNet_128x128 dataset for the Conditional Image Generation task is 124.5.The Params score for the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for Language_Modelling is not explicitly mentioned in the retrieved documents. However, the Past Decode Regularization (PDR) method achieves a word level perplexity of 53.8 on the Penn Treebank dataset when used with a mixture-of-softmaxes. | 22M => 0.0\n",
      " | 166.5 => 0.0\n",
      "The MAE score of the Regularized Deep Regressor method on the UNBC-McMaster Shoulder Pain dataset for the Pain Intensity Regression task is 0.389. | 0.389 => 1.0\n",
      "The Percentage error score of the Bi-LSTM with skip connections and CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n",
      "The Percentage_correct score of the ACN method on the CIFAR-100 dataset for the Image Classification task is 66.3%. | 66.3 => 1.0\n",
      "The 200D decomposable attention model with intra-sentence attention has approximately 580k parameters on the SNLI dataset for the Natural Language Inference task. | 580k => 1.0\n",
      "The corresponding Test Accuracy score of the CBS-1 ESIM method on the SNLI dataset for the Natural Language Inference task is 86.73%. | 86.73 => 1.0\n",
      "The accuracy score of the Sample Clustering method on the CUB-200 0-Shot Learning dataset for the Few-Shot Image Classification task is 44.3%. |  44.3% => 1.0\n",
      "The corresponding Viewpoint_I_AEPE score of the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is 5.99. | 5.99 => 1.0\n",
      "The Aspect score of the LSTM-LOC method on the Sentihood dataset for Aspect-Based Sentiment Analysis is 69.3. | 69.3 => 1.0\n",
      "The corresponding BLEU-2 score of the LeakGAN method on the Chinese_Poems dataset for the Text_Generation task is 0.456. | 0.881 => 0.0\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is approximately 0.84. | 0.88 => 0.0\n",
      "The Arc-hybrid method achieves a UAS score of 95.33 on the Penn Treebank for the Dependency Parsing task. | 93.56 => 0.0\n",
      "The C-LSTM method achieved an accuracy score of 87.8% on the SST-2 Binary classification dataset for the Sentiment Analysis task. | 87.8 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [00:36<13:08, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Percentage_correct score of the MRN global features method on the COCO Visual Question Answering (VQA) real images 1.0 open-ended dataset is 66.3. | 61.84 => 0.0\n",
      "The corresponding mIoU score of the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task is not explicitly found in the available resources. The search did not yield a specific mIoU score for this method and dataset combination. | 66.63 => 0.0\n",
      "The Viewpoint_II_AEPE score for the DeepMatching method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is not explicitly available in the search results. However, the DeepMatching method has a score of 5.84 for Dense Pixel Correspondence Estimation on the HPatches dataset. | 4.63 => 0.0\n",
      "The TAR at FAR=0.01 score for the Triplet_probabilistic_embedding method on the IJB-A dataset for the Face_Verification task is 90%. | 90% => 1.0\n",
      "The MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object Detection task is 76.2%. | 76.2% => 1.0\n",
      "The Class_IOU score for the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is not explicitly available in the search results. The available data mentions CoGAN with scores like 0.08, 11%, and 45%, but it does not specify which of these, if any, corresponds to the Class_IOU score.The mLSTM with dynamic evaluation method on the Text8 dataset for the Language Modelling task has approximately 45 million parameters. | 45M => 1.0\n",
      " |  0.08 => 0.5\n",
      "The corresponding AP score of the FDNet method on the WIDER Face Easy dataset for the Face Detection task could not be found in the available resources. | 0.950 => 0.0\n",
      "The AP score for the ACF-WIDER method on the WIDER Face Hard dataset for the Face Detection task could not be found in the available resources. | 0.290 => 0.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task could not be found in the available resources. | 26.1 => 0.0\n",
      "The PSNR score for the JMPF method on the BSD100 dataset for 4x upscaling in the Image Super-Resolution task is not explicitly found in the available resources. Further specific searches or access to the original research paper might be necessary to obtain this precise information. | 26.87 => 0.0\n",
      "The test perplexity score for the AWD-LSTM-DOC method on the Penn Treebank Word Level dataset for the language modeling task is not explicitly found in the search results. However, the AWD-LSTM-DOC model is known to have achieved significant improvements over previous models, but the exact test perplexity score remains unspecified in the available data. | 52.38 => 0.0\n",
      "The F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.91. | 93.91 => 1.0\n",
      "The Percentage_correct score for the ResNet_ELU method on the CIFAR-100 dataset for the Image Classification task could not be found in the available resources. | 73.5 => 0.0\n",
      "The Checkerboards method achieves a log-average miss rate of 18.5% on the Reasonable subset of the Caltech dataset for the Pedestrian Detection task. | 17.1 => 0.0\n",
      "The F1 score for the Neural-CRF_AE method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task is not available in the current search results. Further specific sources or papers may need to be consulted to find this information. | 92.29 => 0.0\n",
      "The Log_Loss score for the FNN method on the Criteo dataset for the Click-Through Rate Prediction task could not be found in the available resources. | 0.45738 => 0.0\n",
      "The F1 score for the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task is not explicitly found in the available resources. Further specific searches or access to the original research paper detailing CHFusion's performance might be necessary to obtain this information. | 55.3% => 0.0\n",
      "The F-Measure score of the DeepFlux method on the SK-LARGE dataset for Object Skeleton Detection is 0.732. | 0.732 => 1.0\n",
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task could not be found in the available resources. | 29.56 => 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Percentage_correct score for the \"Deep Networks with Internal Selective Attention through Feedback Connections\" method on the CIFAR-100 dataset for the Image Classification task is not explicitly found in the available resources. Further detailed search or access to specific datasets or publications might be required to obtain this information. | 66.2 => 0.0\n",
      "The specific accuracy score for the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task could not be found in the available resources. | 57.14 => 0.0\n",
      "The MAP score for the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly found in the available resources. Further detailed search in specific academic papers or datasets might be required to obtain this information. | 0.750 => 0.0\n",
      "Unable to find the specific Unigram_Acc score for the ASR method on the SearchQA dataset for the Open-Domain Question Answering task. | 41.3 => 0.0\n",
      "The specific accuracy score for the MEAN method on the MR dataset for the Sentiment Analysis task could not be found in the available resources. | 84.5 => 0.0\n",
      "The Hits@1 score for the ComplEx method on the WN18 dataset for the Link Prediction task is approximately 0.089. | 0.936 => 0.0\n",
      "The NLL_Test score for the Conv_DRAW method on the CIFAR-10 dataset for the Image Generation task could not be found in the available resources. | 3.58 => 0.0\n",
      "The F-Measure score for PSENet-1s on the IC17-MLT dataset for the Scene Text Detection task is not readily available from the current search results. Further detailed research or access to specific publications or datasets may be required to obtain this information. | 72.45% => 0.0\n",
      "The F1 score for the Yang et al. method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task could not be found in the available resources. | 91.26 => 0.0\n",
      "The specific Mean IoU score for the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task could not be found in the search results. It is recommended to check the official Cityscapes or Mapillary Vistas leaderboards or related academic publications for the most accurate and up-to-date information. | 82.0% => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:40<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the R2U-Net method on the LUNA16 dataset for the Lung Nodule Segmentation task is not readily available from the current search results. It appears that the specific AUC value is not explicitly mentioned in the accessible literature or web sources. Further detailed research or access to specific publications or datasets might be required to obtain this information. | 0.9889 => 0.0\n",
      "Processing batch 2 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding MAP score of the subCNN method on the PASCAL VOC 2007 dataset for the Object Detection task is 68.5%. | 68.5% => 1.0\n",
      "The corresponding Percentage error score of the Microsoft 2016b method on the Switchboard + Hub500 dataset for the Speech Recognition task is 5.8%. | 5.8 => 1.0\n",
      "The corresponding Average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 79.15. | 79.15 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:01<01:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "The corresponding Frame fps score of the CRF-RNN method on the Cityscapes dataset for Real-Time Semantic Segmentation is 1.4 fps. | 1.4 => 1.0\n",
      "The Validation perplexity score of the AWD-LSTM-MoS + Partial Shuffle method on the Penn Treebank Word Level dataset for Language Modelling is 53.92. | 55.89 => 0.0\n",
      "The accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n",
      "The corresponding Accuracy score of the MAML method on the OMNIGLOT 1-Shot Learning dataset for the Few-Shot Image Classification task is 98.7%. | 98.7% => 1.0\n",
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The Mean IoU score of the ResNet-38_MS_COCO method on the PASCAL VOC 2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "The Inception score for the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The Params score for the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for Language_Modelling is not explicitly mentioned in the retrieved documents. However, the Past Decode Regularization (PDR) method achieves a word level perplexity of 53.8 on the Penn Treebank dataset when used with a mixture-of-softmaxes. | 22M => 0.0\n",
      "The corresponding Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is 83.58%. | 84.14 => 0.0\n",
      "The Inception score for the BigGAN-deep method on the ImageNet_128x128 dataset for the Conditional Image Generation task is 124.5. | 166.5 => 0.0\n",
      "The BLEU score for the Enc-Dec Att (char) method on the WMT2015 English-German dataset for the Machine Translation task is 23.5. | 23.45 => 0.5\n",
      "The Percentage error score of the Bi-LSTM with skip connections and CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:11<06:22,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Viewpoint_I_AEPE score of the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is 5.99. | 5.99 => 1.0\n",
      "The Train_Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is 94.8%. | 94.8 => 1.0\n",
      "The corresponding Accuracy score of the DeepId2 method on the Labeled Faces in the Wild dataset for the Face Verification task is 99.15%. | 99.15% => 1.0\n",
      "The C-LSTM method achieved an accuracy score of 87.8% on the SST-2 Binary classification dataset for the Sentiment Analysis task. | 87.8 => 1.0\n",
      "The accuracy score of the Sample Clustering method on the CUB-200 0-Shot Learning dataset for the Few-Shot Image Classification task is 44.3%. |  44.3% => 1.0\n",
      "The F-Measure score of the DeepFlux method on the SK-LARGE dataset for the Object Skeleton Detection task is 0.738. | 0.732 => 0.0\n",
      "The corresponding AP score of the FDNet method on the WIDER Face Easy dataset for the Face Detection task could not be found in the available resources. | 0.950 => 0.0\n",
      "The BLEU-2 score of the LeakGAN method on the Chinese_Poems dataset for the Text_Generation task is 0.456. | 0.881 => 0.0\n",
      "The Percentage_correct score of the ACN method on the CIFAR-100 dataset for the Image Classification task is 66.3%. | 66.3 => 1.0\n",
      "The 200D decomposable attention model with intra-sentence attention on the SNLI dataset for the Natural Language Inference task has a Parameters score of 580k. | 580k => 1.0\n",
      "The Percentage_correct score of the MRN global features method on the COCO Visual Question Answering (VQA) real images 1.0 open ended dataset is 66.3%. | 61.84 => 0.0\n",
      "The Arc-hybrid method achieves a UAS score of 91.42 on the Penn Treebank dataset for the Dependency Parsing task. | 93.56 => 0.0\n",
      "The F-Measure score of the PSENet-1s method on the IC17-MLT dataset for the Scene Text Detection task is 77.01. | 72.45% => 0.0\n",
      "The corresponding Test Accuracy score of the CBS-1 ESIM method on the SNLI dataset for the Natural Language Inference task is 86.73%. | 86.73 => 1.0\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is 0.84. | 0.88 => 0.0\n",
      "The MAE score of the Regularized Deep Regressor method on the UNBC-McMaster Shoulder Pain dataset for the Pain Intensity Regression task is 0.389. | 0.389 => 1.0\n",
      "The F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.9. | 93.91 => 0.5\n",
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task could not be found in the available resources. | 29.56 => 0.0\n",
      "The test perplexity score for the AWD-LSTM-DOC method on the Penn Treebank Word Level dataset for the language modeling task is not explicitly found in the search results. However, the AWD-LSTM-DOC method is known to have achieved significant improvements in perplexity scores over previous models, but the exact test perplexity score remains unspecified in the available data. | 52.38 => 0.0\n",
      "The Class_IOU score for the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is 45%. |  0.08 => 0.0\n",
      "The mLSTM with dynamic evaluation on the Text8 dataset for language modeling has approximately 45 million parameters. | 45M => 1.0\n",
      "The corresponding Aspect score of the LSTM-LOC method on the Sentihood dataset for the Aspect-Based Sentiment Analysis task is 69.3. | 69.3 => 1.0\n",
      "The Percentage_correct score for the method \"Deep Networks with Internal Selective Attention through Feedback Connections\" on the CIFAR-100 dataset for Image Classification is not explicitly available in the searched resources. | 66.2 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [00:28<10:35, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding mIoU score of the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task is 66.63. | 66.63 => 1.0\n",
      "The Percentage_correct score for the ResNet_ELU method on the CIFAR-100 dataset for the Image Classification task could not be found using the available tools. | 73.5 => 0.0\n",
      "The specific accuracy score for the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task could not be found in the available resources. | 57.14 => 0.0\n",
      "The corresponding MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object Detection task is 76.2%. | 76.2% => 1.0\n",
      "The F1 score for the Neural-CRF_AE method on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is not available in the current search results. | 92.29 => 0.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task could not be found in the available resources. | 26.1 => 0.0\n",
      "The Viewpoint_II_AEPE score for the DeepMatching method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is not readily available from the current search results. Further detailed investigation in specific academic papers or datasets might be required to obtain this specific metric. | 4.63 => 0.0\n",
      "The specific accuracy score for the MEAN method on the MR dataset for the Sentiment Analysis task could not be found in the available resources. | 84.5 => 0.0\n",
      "The F1 score for the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task could not be found in the available resources. | 55.3% => 0.0\n",
      "The Log_Loss score for the FNN method on the Criteo dataset for the Click-Through Rate Prediction task could not be found through the available searches. It may require access to specific research papers or datasets that are not publicly indexed or available through the tools used. | 0.45738 => 0.0\n",
      "The specific TAR___FAR_0_01 score for the Triplet_probabilistic_embedding method on the IJB-A dataset for the Face_Verification task could not be found in the available resources. | 90% => 0.0\n",
      "The PSNR score for the JMPF method on the BSD100 dataset for 4x upscaling in the Image Super-Resolution task is not explicitly found in the available resources. Further specific searches or access to the original research paper might be necessary to obtain this information. | 26.87 => 0.0\n",
      "The MAP score for the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly found in the search results. However, the aNMM model is noted to significantly outperform other neural network models on the TrecQA dataset. | 0.750 => 0.0\n",
      "I'm unable to find the specific Unigram_Acc score for the ASR method on the SearchQA dataset for the Open-Domain Question Answering task. Further detailed searches did not yield the required information. | 41.3 => 0.0\n",
      "The F1 score for the Yang et al. method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task could not be found in the available resources. | 91.26 => 0.0\n",
      "The corresponding AP score of the ACF-WIDER method on the WIDER Face Hard dataset for the Face Detection task could not be found in the available resources. | 0.290 => 0.0\n",
      "The NLL_Test score for the Conv_DRAW method on the CIFAR-10 dataset for the Image Generation task could not be found in the available resources. It is recommended to check specific academic papers or datasets related to Conv_DRAW for detailed results. | 3.58 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [01:08<21:19, 22.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the R2U-Net method on the LUNA16 dataset for the Lung Nodule Segmentation task is not explicitly found in the available resources. However, the R2U-Net model is noted for its superior performance in segmentation tasks, often showing better results in terms of AUC and accuracy compared to other models. For precise AUC values, further specific research papers or datasets might need to be consulted. | 0.9889 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  23%|██▎       | 14/60 [01:14<02:58,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean IoU score for the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task is 61.1% on the validation set with a single model. | 82.0% => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  62%|██████▏   | 37/60 [01:15<00:24,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hits@1 score for the ComplEx method on the WN18 dataset for the Link Prediction task is not readily available from the current search results. It may require consulting specific academic papers or datasets that report detailed performance metrics for the ComplEx model on WN18. | 0.936 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:26<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Checkerboards method achieves a Reasonable Miss Rate of 18.3% on the Caltech dataset for the Pedestrian Detection task. | 17.1 => 0.0\n",
      "Processing batch 3 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Validation perplexity score of the AWD-LSTM-MoS + Partial Shuffle method on the Penn Treebank Word Level dataset for Language Modelling is 53.92. | 55.89 => 0.0\n",
      "The corresponding MAP score of the subCNN method on the PASCAL VOC 2007 dataset for the Object Detection task is 68.5%. | 68.5% => 1.0\n",
      "The Mean IoU score of the ResNet-38_MS_COCO method on the PASCAL VOC 2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "The corresponding Percentage error score of the Microsoft 2016b method on the Switchboard + Hub500 dataset for the Speech Recognition task is 5.8%. | 5.8 => 1.0\n",
      "The corresponding Average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 79.15. | 79.15 => 1.0\n",
      "The accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:02<02:31,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "The BLEU score for the Enc-Dec Att (char) method on the WMT2015 English-German dataset for the Machine Translation task is 23.5. | 23.45 => 0.5\n",
      "The corresponding Frame fps score of the CRF-RNN method on the Cityscapes dataset for Real-Time Semantic Segmentation is 1.4 fps. | 1.4 => 1.0\n",
      "The Inception score for the BigGAN-deep method on the ImageNet_128x128 dataset for the Conditional Image Generation task is 124.5. | 166.5 => 0.0\n",
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The corresponding Accuracy score of the MAML method on the OMNIGLOT 1-Shot Learning dataset for the Few-Shot Image Classification task is 98.7%. | 98.7% => 1.0\n",
      "The Percentage error score of the Bi-LSTM with skip connections and CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n",
      "The Percentage_correct score of the MRN + global features method on the COCO Visual Question Answering (VQA) real images 1.0 open ended dataset is 61.8%. | 61.84 => 0.0\n",
      "The Inception score for the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The Train_Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is 94.8%. | 94.8 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:10<05:44,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Params score for the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for Language_Modelling is not explicitly mentioned in the retrieved documents. However, the Past Decode Regularization (PDR) method achieves a word level perplexity of 53.8 on the Penn Treebank dataset when used with a mixture-of-softmaxes. | 22M => 0.0\n",
      "The corresponding Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is 83.58%. | 84.14 => 0.0\n",
      "The MAE score of the Regularized Deep Regressor method on the UNBC-McMaster Shoulder Pain dataset for the Pain Intensity Regression task is 0.389. | 0.389 => 1.0\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is 0.84. | 0.88 => 0.0\n",
      "The Arc-hybrid method achieves a UAS score of 91.42 on the Penn Treebank dataset for the Dependency Parsing task. | 93.56 => 0.0\n",
      "The 200D decomposable attention model with intra-sentence attention on the SNLI dataset for the Natural Language Inference task has a Parameters score of 580k. | 580k => 1.0\n",
      "The corresponding Accuracy score of the DeepId2 method on the Labeled Faces in the Wild dataset for the Face Verification task is 99.15%. | 99.15% => 1.0\n",
      "The F-Measure score of the DeepFlux method on the SK-LARGE dataset for the Object Skeleton Detection task is 0.738. | 0.732 => 0.0\n",
      "The corresponding Test Accuracy score of the CBS-1 ESIM method on the SNLI dataset for the Natural Language Inference task is 86.73%. | 86.73 => 1.0\n",
      "The corresponding BLEU-2 score of the LeakGAN method on the Chinese_Poems dataset for the Text_Generation task is 0.456. | 0.881 => 0.0\n",
      "The corresponding MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object Detection task is 76.2%. | 76.2% => 1.0\n",
      "The corresponding Viewpoint_I_AEPE score of the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is 5.99. | 5.99 => 1.0\n",
      "The Aspect score of the LSTM-LOC method on the Sentihood dataset for the Aspect-Based Sentiment Analysis task is 69.3. | 69.3 => 1.0\n",
      "The accuracy score of the Sample_Clustering method on the CUB-200 0-Shot Learning dataset for the Few-Shot Image Classification task is 44.3%. |  44.3% => 1.0\n",
      "The corresponding accuracy score of the C-LSTM method on the SST-2 Binary classification dataset for the Sentiment Analysis task is 87.8%. | 87.8 => 1.0\n",
      "The Viewpoint_II_AEPE score of the DeepMatching method on the HPatches dataset for Dense Pixel Correspondence Estimation is 5.84. | 4.63 => 0.0\n",
      "The mLSTM with dynamic evaluation on the Text8 dataset for language modeling has 45 million parameters. | 45M => 1.0\n",
      "The specific Mean IoU score for the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task was not found in the search results. However, it was mentioned that Mapillary achieves 61.1 IoU on the validation set with a single model. For precise and updated information, checking the official Cityscapes leaderboard or Mapillary's publications might be necessary. | 82.0% => 0.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task could not be found in the available resources. | 26.1 => 0.0\n",
      "The specific accuracy score for the MEAN method on the MR dataset for the Sentiment Analysis task could not be found in the available resources. | 84.5 => 0.0\n",
      "The accuracy score for the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task could not be found in the available resources. | 57.14 => 0.0\n",
      "The F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.91. | 93.91 => 1.0\n",
      "The Checkerboards method achieves a Reasonable Miss Rate score of 18.5% on the Caltech dataset for the Pedestrian Detection task. | 17.1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [00:31<11:53, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task could not be found in the available resources. | 29.56 => 0.0\n",
      "The corresponding mIoU score of the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task was not found in the available resources. The searches did not yield specific results for this method and dataset combination. | 66.63 => 0.0\n",
      "The Class_IOU score for the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is not explicitly available in the search results. The available information includes other metrics such as 0.08, 11%, and 45%, but it is unclear which of these, if any, corresponds to the Class_IOU score. |  0.08 => 0.0\n",
      "The Percentage_correct score for the \"Deep Networks with Internal Selective Attention through Feedback Connections\" method on the CIFAR-100 dataset for the Image Classification task is not explicitly available in the current search results. The available information indicates that the method outperforms previous state-of-the-art models on the CIFAR-100 dataset, but specific percentage scores are not provided. | 66.2 => 0.0\n",
      "The TAR @ FAR=0.01 score for the Triplet_probabilistic_embedding method on the IJB-A dataset for the Face_Verification task could not be found in the available resources. | 90% => 0.0\n",
      "The F1 score for the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task is not available in the current search results. | 55.3% => 0.0\n",
      "The test perplexity score for the AWD-LSTM-DOC method on the Penn Treebank Word Level dataset for the language modeling task is not explicitly found in the search results. However, the AWD-LSTM-DOC model is known to have improved perplexity scores by almost 2.0 on the Penn Treebank dataset from previous state-of-the-art scores. The exact test perplexity score might be found in specific research papers or repositories tracking NLP progress. | 52.38 => 0.0\n",
      "The AP score for the FDNet method on the WIDER Face Easy dataset for the Face Detection task could not be found in the available resources. | 0.950 => 0.0\n",
      "The F1 score of the Neural-CRF_AE method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task is 92.29. | 92.29 => 1.0\n",
      "The Percentage_correct score for the ResNet_ELU method on the CIFAR-100 dataset for the Image Classification task could not be found in the available resources. | 73.5 => 0.0\n",
      "The Log_Loss score for the FNN method on the Criteo dataset for the Click-Through Rate Prediction task could not be found in the available resources. | 0.45738 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [00:44<11:56, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the R2U-Net method on the LUNA16 dataset for the Lung Nodule Segmentation task is not explicitly available in the search results. The available information primarily focuses on other metrics such as the Dice coefficient and F1 score. Further specific research or access to the original study might be required to obtain the exact AUC score. | 0.9889 => 0.0\n",
      "The AP score for the ACF-WIDER method on the WIDER Face Hard dataset for the Face Detection task is not readily available from the current search results. Further specific searches or access to detailed research papers or datasets might be required to obtain this information. | 0.290 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  18%|█▊        | 11/60 [00:48<02:27,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PSNR score for the JMPF method on the BSD100 dataset for 4x upscaling in the Image Super-Resolution task is not explicitly found in the available resources. The searches did not yield a specific PSNR score for this method and dataset combination. | 26.87 => 0.0\n",
      "The Hits@1 score for the ComplEx method on the WN18 dataset for the Link Prediction task is 0.089. | 0.936 => 0.0\n",
      "The NLL_Test score for the Conv_DRAW method on the CIFAR-10 dataset for the Image Generation task is not readily available from the current search results. It may require accessing specific academic papers or datasets that detail this information. | 3.58 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  35%|███▌      | 21/60 [00:54<00:59,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for the Yang et al. method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task is reported as 91.71. | 91.26 => 0.0\n",
      "The MAP score for the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly found in the search results. Further specific research or access to the original paper might be required to obtain this information. | 0.750 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  40%|████      | 24/60 [01:05<01:11,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F-Measure score for PSENet-1s on the IC17-MLT dataset for Scene Text Detection is not readily available from the current search results. Further specific research or access to detailed benchmark results may be required to obtain this information. | 72.45% => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  78%|███████▊  | 47/60 [01:54<00:26,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Percentage_correct score of the ACN method on the CIFAR-100 dataset for the Image Classification task is 66.3%. | 66.3 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:12<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Unigram_Acc score for the ASR method on the SearchQA dataset for the Open-Domain Question Answering task could not be found in the available resources. | 41.3 => 0.0\n",
      "Processing batch 4 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding MAP score of the subCNN method on the PASCAL VOC 2007 dataset for the Object Detection task is 68.5%. | 68.5% => 1.0\n",
      "The Validation perplexity score of the AWD-LSTM-MoS + Partial Shuffle method on the Penn Treebank Word Level dataset for Language Modelling is 53.92. | 55.89 => 0.0\n",
      "The corresponding Percentage error score of the Microsoft 2016b method on the Switchboard + Hub500 dataset for the Speech Recognition task is 5.8%. | 5.8 => 1.0\n",
      "The Mean IoU score of the ResNet-38_MS_COCO method on the PASCAL VOC 2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "The corresponding Average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 79.15. | 79.15 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:01<01:23,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "The accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n",
      "The corresponding Frame fps score of the CRF-RNN method on the Cityscapes dataset for Real-Time Semantic Segmentation is 1.4 fps. | 1.4 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:05<02:43,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Train_Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is 94.8%. | 94.8 => 1.0\n",
      "The Inception score for the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The Percentage error score of the Bi-LSTM with skip connections and CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n",
      "The corresponding Accuracy score of the MAML method on the OMNIGLOT 1-Shot Learning dataset for the Few-Shot Image Classification task is 98.7%. | 98.7% => 1.0\n",
      "The Inception score for the BigGAN-deep method on the ImageNet_128x128 dataset for the Conditional Image Generation task is 124.5. | 166.5 => 0.0\n",
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The Params score for the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for Language_Modelling is not explicitly mentioned in the retrieved documents. However, the Past Decode Regularization (PDR) method achieves a word level perplexity of 53.8 on the Penn Treebank dataset when used with a mixture-of-softmaxes. | 22M => 0.0\n",
      "The corresponding Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is 83.58%. | 84.14 => 0.0\n",
      "The MAE score of the Regularized Deep Regressor method on the UNBC-McMaster Shoulder Pain dataset for the Pain Intensity Regression task is 0.389. | 0.389 => 1.0\n",
      "The corresponding Accuracy score of the DeepId2 method on the Labeled Faces in the Wild dataset for the Face Verification task is 99.15%. | 99.15% => 1.0\n",
      "The accuracy score of the Sample Clustering method on the CUB-200 0-Shot Learning dataset for the Few-Shot Image Classification task is 44.3%. |  44.3% => 1.0\n",
      "The corresponding accuracy score of the C-LSTM method on the SST-2 Binary classification dataset for the Sentiment Analysis task is 87.8%. | 87.8 => 1.0\n",
      "The 200D decomposable attention model with intra-sentence attention has a Parameters score of 580k on the SNLI dataset for the Natural Language Inference task. | 580k => 1.0\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is 0.84. | 0.88 => 0.0\n",
      "The Percentage_correct score of the ACN method on the CIFAR-100 dataset for the Image Classification task is 66.3%. | 66.3 => 1.0\n",
      "The AWD-LSTM-DOC method achieved a test perplexity score of 47.17 on the Penn Treebank Word Level dataset for the Language Modelling task. | 52.38 => 0.0\n",
      "The Aspect score of the LSTM-LOC method on the Sentihood dataset for the Aspect-Based Sentiment Analysis task is 69.3. | 69.3 => 1.0\n",
      "The corresponding BLEU-2 score of the LeakGAN method on the Chinese_Poems dataset for the Text_Generation task is 0.456. | 0.881 => 0.0\n",
      "The F-Measure score of the PSENet-1s method on the IC17-MLT dataset for the Scene Text Detection task is 77.01. | 72.45% => 0.0\n",
      "The corresponding Viewpoint_I_AEPE score of the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is 5.99. | 5.99 => 1.0\n",
      "The BLEU score for the Enc-Dec_Att__char_ method on the WMT2015 English-German dataset for the Machine Translation task is 23.5. | 23.45 => 0.5\n",
      "The Viewpoint_II_AEPE score for the DeepMatching method on the HPatches dataset for the Dense Pixel Correspondence Estimation task could not be found in the available resources. | 4.63 => 0.0\n",
      "The corresponding MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object Detection task is 76.2%. | 76.2% => 1.0\n",
      "The Percentage_correct score of the MRN global features method on the COCO Visual Question Answering (VQA) real images 1.0 open-ended dataset is 66.3%. | 61.84 => 0.0\n",
      "The mLSTM dynamic eval method on the Text8 dataset for the Language Modelling task has approximately 45 million parameters. | 45M => 1.0\n",
      "The Percentage_correct score for the \"Deep Networks with Internal Selective Attention through Feedback Connections\" method on the CIFAR-100 dataset for Image Classification is not explicitly available from the current search results. | 66.2 => 0.0\n",
      "The Class_IOU score for the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is 45%. |  0.08 => 0.0\n",
      "The corresponding Test Accuracy score of the CBS-1 ESIM method on the SNLI dataset for the Natural Language Inference task is 86.73%. | 86.73 => 1.0\n",
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task is 29.56. | 29.56 => 1.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task could not be found in the available resources. | 26.1 => 0.0\n",
      "The AUC score for the R2U-Net method on the LUNA16 dataset for the Lung Nodule Segmentation task is 0.982. | 0.9889 => 0.0\n",
      "The corresponding AP score of the ACF-WIDER method on the WIDER Face Hard dataset for the Face Detection task is 0.290. | 0.290 => 1.0\n",
      "The F1 score for the Neural-CRF_AE method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task is not explicitly found in the available resources. The search results did not provide a specific F1 score for this method. | 92.29 => 0.0\n",
      "The corresponding AP score of the FDNet method on the WIDER Face Easy dataset for the Face Detection task is 0.950. | 0.950 => 1.0\n",
      "The Log_Loss score for the FNN method on the Criteo dataset for the Click-Through Rate Prediction task could not be found through the available searches. It may require access to specific research papers or datasets that are not publicly indexed or available online. | 0.45738 => 0.0\n",
      "The Percentage_correct score for the ResNet_ELU method on the CIFAR-100 dataset for the Image Classification task could not be found in the available resources. | 73.5 => 0.0\n",
      "The specific Mean IoU score for the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task was not found in the search results. However, it was mentioned that Mapillary achieves 61.1 IoU on the validation set with a single model. | 82.0% => 0.0\n",
      "The specific accuracy score for the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task could not be found in the available resources. It is recommended to check specific research papers or publications related to this method for detailed performance metrics. | 57.14 => 0.0\n",
      "The F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.91. | 93.91 => 1.0\n",
      "The F1 score for the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task could not be found in the available resources. | 55.3% => 0.0\n",
      "The TAR___FAR_0_01 score for the Triplet_probabilistic_embedding method on the IJB-A dataset for the Face_Verification task could not be found in the available resources. It is recommended to check specific academic papers or datasets for this detailed metric. | 90% => 0.0\n",
      "The specific accuracy score for the MEAN method on the MR dataset for the Sentiment Analysis task could not be found in the available resources. | 84.5 => 0.0\n",
      "The NLL_Test score for the Conv_DRAW method on the CIFAR-10 dataset for the Image Generation task is not explicitly available in the search results. The closest related information found is a score of 3.58 from a different context, but it is not confirmed as the NLL_Test score for Conv_DRAW. | 3.58 => 0.5\n",
      "The Unigram_Acc score for the ASR method on the SearchQA dataset for the Open-Domain Question Answering task is not readily available from the current search results. Further specific research or access to detailed datasets or papers may be required to obtain this information. | 41.3 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [00:56<23:43, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding mIoU score of the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task is not explicitly found in the available resources. The searches did not yield a specific mIoU score for this method and dataset combination. | 66.63 => 0.0\n",
      "The PSNR score for the JMPF method on the BSD100 dataset for 4x upscaling in the Image Super-Resolution task is not explicitly found in the available resources. Further specific searches or access to the original research paper might be required to obtain this information. | 26.87 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  18%|█▊        | 11/60 [00:56<03:17,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F-Measure score for the DeepFlux method on the SK-LARGE dataset for Object Skeleton Detection is not explicitly found in the available resources. Further detailed search or access to specific research papers or datasets might be required to obtain this information. | 0.732 => 0.0\n",
      "The MAP score for the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly found in the search results. The available information suggests that aNMM significantly outperforms other neural network models on the TrecQA dataset, but specific MAP scores are not provided in the retrieved documents. | 0.750 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  35%|███▌      | 21/60 [01:11<01:36,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score for the Yang et al. method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task could not be found in the available resources. | 91.26 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  62%|██████▏   | 37/60 [01:16<00:28,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hits@1 score for the ComplEx method on the WN18 dataset for the Link Prediction task is not readily available from the sources searched. It is recommended to refer to the original paper \"Complex Embeddings for Simple Link Prediction\" by Théo Trouillon et al. for the specific value, as the search did not yield the exact score. | 0.936 => 0.0\n",
      "The Arc-hybrid method achieves a UAS score of 90.22 on the Penn Treebank for the Dependency Parsing task. | 93.56 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:21<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Checkerboards method achieves a log-average miss rate of 18.5% on the Reasonable subset of the Caltech dataset for the Pedestrian Detection task. | 17.1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)\n",
    "batch_num = 4\n",
    "iterative_monkey.thread_safe_evaluator_batch(toolqa_test, optimized_actor_agent,batch_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
