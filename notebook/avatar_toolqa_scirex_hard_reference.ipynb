{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USR_NAME = 'shirwu'\n",
    "TOOL_QA_ROOT = '/dfs/project/kgrlm/shirwu/msr_intern/home/t-yingxinwu/msr_intern/ToolQA-rebuttal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'hard'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "# dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    "    max_iters=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Disable all INFO logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Silence all loggers that might be chatty\n",
    "loggers_to_silence = [\n",
    "    \"httpx\",\n",
    "    \"httpcore\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"dspy\",\n",
    "    \"langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"tiktoken\",\n",
    "    \"asyncio\",\n",
    "    \"faiss\",\n",
    "    \"anthropic\"\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_silence:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable tokenizer parallelism warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Which method achieves the highest PCK score on Leeds_Sports_Poses dataset for Pose_Estimation task?', 'answer': 'Pyramid_Residual_Modules__PRMs_'}) (input_keys={'paper_id', 'question'})\n",
      "physics | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APICallMetrics:\n",
    "    timestamp: datetime\n",
    "    tool_name: str\n",
    "    tokens_in: int = 0\n",
    "    tokens_out: int = 0\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AvatarMetrics:\n",
    "    total_calls: int = 0\n",
    "    total_tokens_in: int = 0\n",
    "    total_tokens_out: int = 0\n",
    "    total_execution_time: float = 0.0\n",
    "    calls_by_tool: Dict[str, int] = field(default_factory=dict)\n",
    "    api_call_history: List[APICallMetrics] = field(default_factory=list)\n",
    "    \n",
    "    def add_call(self, metrics: APICallMetrics):\n",
    "        self.total_calls += 1\n",
    "        self.total_tokens_in += metrics.tokens_in\n",
    "        self.total_tokens_out += metrics.tokens_out\n",
    "        self.total_execution_time += metrics.execution_time\n",
    "        self.calls_by_tool[metrics.tool_name] = self.calls_by_tool.get(metrics.tool_name, 0) + 1\n",
    "        self.api_call_history.append(metrics)\n",
    "    \n",
    "    def merge(self, other: 'AvatarMetrics'):\n",
    "        \"\"\"Merge another AvatarMetrics instance into this one\"\"\"\n",
    "        self.total_calls += other.total_calls\n",
    "        self.total_tokens_in += other.total_tokens_in\n",
    "        self.total_tokens_out += other.total_tokens_out\n",
    "        self.total_execution_time += other.total_execution_time\n",
    "        for tool, count in other.calls_by_tool.items():\n",
    "            self.calls_by_tool[tool] = self.calls_by_tool.get(tool, 0) + count\n",
    "        self.api_call_history.extend(other.api_call_history)\n",
    "\n",
    "    def estimate_cost(self, model_name: str = \"gpt-4\") -> float:\n",
    "        pricing = {\n",
    "            \"gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
    "            \"gpt-3.5-turbo\": {\"input\": 0.0005, \"output\": 0.0015}\n",
    "        }\n",
    "        if model_name not in pricing:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        rates = pricing[model_name]\n",
    "        input_cost = (self.total_tokens_in / 1000) * rates[\"input\"]\n",
    "        output_cost = (self.total_tokens_out / 1000) * rates[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "class AvatarWithMetrics(Avatar):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metrics = AvatarMetrics()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error counting tokens: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _wrapped_tool_call(self, tool, input_text: str) -> str:\n",
    "        start_time = time.time()\n",
    "        tokens_in = self._count_tokens(input_text)\n",
    "        \n",
    "        try:\n",
    "            result = tool.run(input_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tool execution error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            execution_time = time.time() - start_time\n",
    "            tokens_out = self._count_tokens(str(result))\n",
    "            \n",
    "            metrics = APICallMetrics(\n",
    "                timestamp=datetime.now(),\n",
    "                tool_name=tool.name,\n",
    "                tokens_in=tokens_in,\n",
    "                tokens_out=tokens_out,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self.metrics.add_call(metrics)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        metrics = APICallMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            tool_name=\"main_llm\",\n",
    "            tokens_in=self._count_tokens(str(args) + str(kwargs)),\n",
    "            tokens_out=self._count_tokens(str(result)),\n",
    "            execution_time=total_time\n",
    "        )\n",
    "        self.metrics.add_call(metrics)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for example in test_set:\n",
    "            def process_with_metrics(example=example):\n",
    "                try:\n",
    "                    avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "                    prediction = avatar(**example.inputs().toDict())\n",
    "                    return metric(example, prediction), avatar.metrics\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return 0, AvatarMetrics()\n",
    "\n",
    "            futures.append(executor.submit(process_with_metrics))\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            score, metrics = future.result()\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        try:\n",
    "            avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "            prediction = avatar(**example.inputs().toDict())\n",
    "            score = metric(example, prediction)\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in avatar.metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def format_metrics_report(metrics: AvatarMetrics, model_name: str = \"gpt-4\") -> str:\n",
    "    cost = metrics.estimate_cost(model_name)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Avatar Execution Metrics Report\n",
    "==============================\n",
    "Total Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "Total API Calls: {metrics.total_calls}\n",
    "Total Tokens: {metrics.total_tokens_in + metrics.total_tokens_out:,} ({metrics.total_tokens_in:,} in, {metrics.total_tokens_out:,} out)\n",
    "Estimated Cost: ${cost:.4f}\n",
    "\n",
    "Total Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "\n",
    "Tool Usage Breakdown:\n",
    "-------------------\n",
    "\"\"\"\n",
    "    for tool, count in sorted(metrics.calls_by_tool.items()):\n",
    "        report += f\"{tool}: {count} calls\\n\"\n",
    "\n",
    "    report += \"\\nTotal calling time per API:\\n\"\n",
    "    api_call_total_time = {}\n",
    "    for call in metrics.api_call_history:\n",
    "        api_call_total_time[call.tool_name] = api_call_total_time.get(call.tool_name, 0) + call.execution_time\n",
    "    for tool, total_time in api_call_total_time.items():\n",
    "        report += f\"{tool}: {total_time:.2f} seconds\\n\"\n",
    "\n",
    "        \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The method achieving the highest Mean_IoU score on the CamVid dataset for Semantic Segmentation is TMANet-50 with a score of 76.5. | PSPNet => 0.0\n",
      "The Paragraph_vector method for the Question Answering task has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, as mentioned in the context of various research papers discussing reinforcement learning methods applied to Atari games. | Atari_2600_Chopper_Command => 0.0\n",
      "LiteFlowNet achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [01:02<1:01:21, 62.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The search did not yield specific datasets for the Deep_Speech method evaluation in Speech Recognition. Further detailed search or specific papers might be needed to find this information. | Switchboard___Hub500 => 0.0\n",
      "The current state-of-the-art method for the Atari 2600 Name This Game dataset is MuZero. | IQN => 0.0\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the adversarial training mechanism for regularizing neural language models, which achieved a test perplexity score of 38.07. | AWD-LSTM-DOC => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieves a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The DRCN method for Image Super-Resolution on the Set5 dataset with 4x upscaling is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | MOS, PSNR, SSIM => 0.67\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH is evaluated on the Switchboard and Hub500 datasets for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The 3DDFA method is evaluated on the Florence dataset for the 3D Face Reconstruction task using the standard benchmark metric, which is the geometric error between reconstructed meshes and the ground truth. | Mean_NME_ => 0.5\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes a variety of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The method that achieves the highest Score score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The available tools did not provide specific information about the dataset on which the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further research or access to specific academic papers or datasets might be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for specific evaluation metrics used. | Score => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and IFC (Information Fidelity Criterion). | PSNR, SSIM => 0.5\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mIoU (mean Intersection over Union), fwIoU (frequency weighted Intersection over Union), and Pixel Accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The CRN method for Image-to-Image Translation task is not explicitly mentioned in the retrieved documents. Further specific information about the datasets used for evaluating the CRN method in this context is not available from the current search results. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The NICE method is evaluated on the CIFAR-10 dataset for the Image Generation task using the Inception score, which measures the visual quality and diversity of generated images. | NLL_Test => 0.0\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN, Daily Mail, and CBT datasets for the Question Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as BLEU-4, BERTScore F1, BLEURT, and Exact Match. | MAP, MRR => 0.0\n",
      "The Transformer method is evaluated on the WMT 2014 English-to-German and English-to-French translation tasks for the Machine Translation task. | IWSLT2015_English-German => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with almost an order of magnitude fewer parameters than previous work. | 300D_Residual_stacked_encoders => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both human and noop start settings. | Atari_2600_River_Raid => 0.0\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as detection accuracy, which is represented by regression loss. | Mean_PCK => 0.0\n",
      "The Ann_PAT_MT method is evaluated on the CoNLL-2014_A2 dataset for Grammatical Error Detection using metrics such as precision, recall, and F0.5 score, which weights precision twice as much as recall. | F0_5 => 1.0\n",
      "The Snips method is evaluated on the TIMIT dataset for the Speech Recognition task.The SRCNN method for Video Super-Resolution is evaluated on video benchmarks such as Set5 and SuperTexture. | Vid4_-_4x_upscaling => 0.0\n",
      " | LibriSpeech_test-clean => 0.0\n",
      "The DPN-131 method is evaluated on several datasets for the Image Classification task, including the RVL-CDIP dataset, Tobacco-3482 dataset, and Places365-Standard dataset. | ImageNet => 0.0\n",
      "The FDNet method is evaluated on the WIDER_FACE_Easy dataset for the Face Detection task using metrics such as accuracy on the validation set, where it achieved 95.9% on the easy set. | AP => 0.0\n",
      "The DR-BiLSTM (Ensemble) model achieves the highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task, outperforming other models significantly. | __Unigram_and_bigram_features => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for the Scene Segmentation task using metrics such as accuracy and boundary F1-measure (BF). These metrics are used to assess the performance of segmentation architectures, focusing on both region accuracies and boundary precision. | Mean_IoU => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question_Answering task using accuracy as the primary metric. The performance is measured by the proportion of test cases where the ground truth is among the top answers proposed by the model. | CNN, Daily_Mail => 0.5\n",
      "The SVDCNN method for text classification is evaluated on several large-scale datasets, including AG's News Corpus, Sogou News Corpus, DBPedia Ontology Dataset, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The dual attention mechanism in the model improves performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII Human Pose dataset. | FLIC_Elbows => 0.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for the Dense Pixel Correspondence Estimation task using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is defined as 'accuracy@', which measures the proportion of 'correct' pixels from the first image with respect to the total number of pixels. A pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The highest Recall_50 score on the Million Song Dataset for the Collaborative Filtering task is achieved by the EASE model with a score of 0.428. | Mult-VAE_PR => 0.0\n",
      "The current state-of-the-art method for the Yelp Binary classification dataset in the Sentiment Analysis task is XLNet. However, specific information about the method achieving the highest error score is not readily available from the search results. | Char-level_CNN => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [02:15<1:06:07, 68.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated on the Bing_News dataset for Click-Through Rate Prediction using metrics such as Area Under the Curve (AUC). | AUC, Log_Loss => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:17<00:00,  2.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available resources did not provide specific information on the dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further investigation or access to specific research papers or datasets might be required to obtain this information. | Atari_2600_Video_Pinball => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score, metrics = multi_thread_executor(toolqa_test, ToolQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.24\n",
      "\n",
      "Avatar Execution Metrics Report\n",
      "==============================\n",
      "Total Execution Time: 5639.32 seconds\n",
      "Total API Calls: 60\n",
      "Total Tokens: 91,865 (1,702 in, 90,163 out)\n",
      "Estimated Cost: $5.4608\n",
      "\n",
      "Total Execution Time: 5639.32 seconds\n",
      "\n",
      "Tool Usage Breakdown:\n",
      "-------------------\n",
      "main_llm: 60 calls\n",
      "\n",
      "Total calling time per API:\n",
      "main_llm: 5639.32 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average Score on ArxivQA before opitmization: {aqa_score:.2f}\")\n",
    "print(f\"Test Score: {score:.2f}\")\n",
    "print(format_metrics_report(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dspy.teleprompt import AvatarOptimizer\n",
    "\n",
    "# teleprompter = AvatarOptimizer(\n",
    "#     metric=metric,\n",
    "#     max_iters=3,\n",
    "#     max_negative_inputs=10,\n",
    "#     max_positive_inputs=10,\n",
    "#     lower_bound=0.5,\n",
    "#     upper_bound=0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_actor_agent = teleprompter.compile(\n",
    "#     student=actor_agent,\n",
    "#     trainset=toolqa_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "iterative_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=2,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:12<08:22, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest BLEU score on the WMT2014 English-German dataset for the Machine Translation task is Bi-SimCut with a BLEU score of 35.15. | Weighted_Transformer__large_ => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The IQN method is evaluated on 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the retrieved information. The available papers discuss DCCL in the context of Generalized Category Discovery and Unsupervised Domain Adaptation, but not specifically for Machine Translation. | IWSLT2015_German-English => 0.0\n",
      "The method that achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for Atari Games is LBC with a score of 10077.52%. | Ape-X => 0.0\n",
      "The method PTSR (Patch Translator for Image Super-Resolution) achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models. | PFF => 0.0\n",
      "The A3C-CTS method is evaluated on the whole Atari 2600 suite, including Montezuma's Revenge and Bellemare et al.'s set of hard exploration games with sparse rewards. | Atari_2600_Venture => 0.0\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 87.0 F1, achieved by the span-based model presented in the paper \"A Span Selection Model for Semantic Role Labeling\" by Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. | Li_et_al_ => 0.0\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the average F1 score as the evaluation metric. | F1 => 1.0\n",
      "The Bi-LSTM trained on FCE method achieves the highest F0.5 score on the FCE dataset for the Grammatical Error Detection task, as indicated by the result from the paper by Masahiro Kaneko and Mamoru Komachi. | CoNLL-2014_A2 => 0.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on the COLX 563 dataset, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "Frustum_PointNets method is evaluated on the KITTI dataset for the Object Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The method achieving the highest MRR score on the FB15k dataset for the Link Prediction task is AutoKGE with an MRR of 0.861. | TuckER => 0.0\n",
      "The highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is not explicitly mentioned in the available resources. Further specific research or access to detailed datasets and publications may be required to obtain this information. | NAN => 1.0\n",
      "The highest Validation_perplexity score on the Penn Treebank Word Level dataset for Language Modelling is achieved by OpenAI's GPT-3 with a word-level perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The method that achieves the highest Score score on the Atari_2600_Road_Runner dataset for the Atari_Games task is GDI-H3 with a score of 999999. | Duel_noop => 0.0\n",
      "The available resources did not provide specific datasets on which the Sample_Clustering method is evaluated for the Few-Shot Image Classification task. Further detailed research or access to specific publications or datasets might be required to obtain this information. | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The IDE____CamStyle method is evaluated on the PRID2011, iLIDS-VID, and VIPeR datasets for the Person Re-Identification task. | DukeMTMC-reID => 0.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using the NDCG@100 metric for the Collaborative Filtering task. | Recall_20, Recall_50 => 0.0\n",
      "The TARNet method is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both a randomized and a non-randomized component, for the Causal Inference task. | IDHP => 0.5\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric. | Percentage_error => 1.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric. | Matched, Mismatched => 0.0\n",
      "CornerNet-Squeeze is evaluated on the COCO dataset for the Real-Time_Object_Detection task. | COCO => 1.0\n",
      "The DDQN__tuned__noop method is evaluated on 57 Atari games. | Atari_2600_Berzerk => 0.0\n",
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Ms__Pacman => 0.0\n",
      "The DDQN__tuned__hs method evaluated datasets for the Atari_Games task are not explicitly mentioned in the retrieved documents. However, it is likely that the method was evaluated on the standard set of 57 Atari games, as is common in research involving Atari game tasks. | Atari_2600_Assault => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:54<18:56, 29.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Time_Pilot => 0.0\n",
      "The BiDAF___Self_Attention__single_model_ method is evaluated on the SQuAD and CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. | F-Measure => 0.5\n",
      "The DeepFM method achieves the highest Log_Loss score for the Click-Through Rate Prediction task on the Criteo dataset. The Criteo dataset is a well-known ad tech industry benchmarking dataset used for evaluating CTR prediction models. | Criteo => 1.0\n",
      "The Duel_hs method is evaluated on 57 Atari games, as it is compared against other state-of-the-art algorithms across all these games. | Atari_2600_Video_Pinball => 0.0\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link_Prediction task using metrics such as precision, recall, and AUC (Area Under the Curve). | Accuracy => 0.0\n",
      "The Transformer method is evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using the BLEU metric. The evaluation reports tokenized BLEU using the \"multi-bleu.perl\" script. | BLEU_score => 1.0\n",
      "The LapSRN method is evaluated on the Urban100 - 4x upscaling dataset using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | PSNR => 0.5\n",
      "The ResNet_ELU method is evaluated on the CIFAR-100 dataset using the test error percentage as a metric. The ELU networks achieved a test error of 24.28%, which is the best published result on CIFAR-100 without using multi-view evaluation or model averaging. | Percentage_correct => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  15%|█▌        | 6/40 [01:04<04:58,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PFF method for Image Super-Resolution is evaluated on the Set5 and Set14 datasets. | Set14_-_4x_upscaling => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  52%|█████▎    | 21/40 [01:06<00:34,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains (books, DVDs, electronics, and kitchen appliances), and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with results showing that DANN has significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:07<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset for the Weakly Supervised Object Detection task using two main metrics: Average Precision (AP) at 50% Intersection-over-Union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth on the test set, while CorLoc measures the percentage of images with at least one correctly localized object in the training and validation sets. | MAP => 0.5\n",
      "Average Score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new instruction: I'm here to help you craft the new instruction based on the feedback provided. Here's a detailed instruction for the group to follow:\n",
      "\n",
      "---\n",
      "\n",
      "New Instruction: You will be given `Tools`, which is a list of tools to use to accomplish the `Goal`. Your task is to decide which tool to use and what input values to provide based on the user query. To improve performance, follow these enhanced guidelines:\n",
      "\n",
      "1. **Tool Selection and Query Refinement:** Begin by evaluating the specificity and clarity of the input query. If the query pertains to a specific dataset or metric, prioritize using the `ARXIV_SEARCH` or `RETRIEVE` tools. For broader or ambiguous queries, consider using the `WEB_SEARCH` tool, which has been enhanced to handle such queries more effectively. Implement a preprocessing step to refine the input query, breaking down complex queries into simpler components to align better with the tool's capabilities.\n",
      "\n",
      "2. **Fallback Mechanism and Feedback Loop:** If the initial tool choice does not yield satisfactory results, employ a fallback mechanism. This involves trying another tool with a refined query. Additionally, establish a feedback loop where the system learns from unsuccessful attempts. This loop should adjust tool selection and query refinement strategies based on past performance, ensuring continuous improvement in handling negative inputs.\n",
      "\n",
      "3. **Action Execution:** Once the appropriate tool and refined query are determined, output the `Action` needed to accomplish the `Goal`. The `Action` should include the selected tool and the input query to pass to the tool. Remember, you can opt to use no tools and provide the final answer directly if applicable. You can also use one tool multiple times with different input queries if necessary. By following these steps, you will enhance the effectiveness of tool usage and improve overall performance on the task.\n",
      "\n",
      "--- \n",
      "\n",
      "This new instruction incorporates the feedback to improve handling of negative inputs while retaining the general guidelines from the previous instruction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The IQN method is evaluated on the 57 Atari 2600 games in the Arcade Learning Environment (ALE). | Atari_2600_Kung-Fu_Master => 0.0\n",
      "The method that achieves the highest score on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 88.59, achieved by the HeSyFu model. | Li_et_al_ => 0.0\n",
      "The Transformer method is typically evaluated on the IWSLT2015 German-English dataset using metrics such as BLEU, METEOR, and NIST. These metrics are commonly used to assess the quality of machine translation systems. | BLEU_score => 0.5\n",
      "The BiDAF Self Attention single model method is evaluated on the Stanford Question Answering Dataset (SQuAD). | SQuAD1_1 => 0.5\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 score as the evaluation metric. | F1 => 1.0\n",
      "The method that achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for the Atari_Games task is LBC with a score of 10077.52%. | Ape-X => 0.0\n",
      "The DCCL method evaluation datasets for the Machine Translation task were not found in the available resources. The ARXIV_SEARCH and WEB_SEARCH did not yield specific datasets for this task. | IWSLT2015_German-English => 0.0\n",
      "The IDE____CamStyle method for Person Re-Identification is evaluated on the Market-1501 and DukeMTMC-reID datasets. | DukeMTMC-reID => 0.5\n",
      "The ResNet_ELU method evaluation metrics on the CIFAR-100 dataset for Image Classification are not readily available in the current search results. It may require accessing specific research papers or documentation related to ResNet_ELU for detailed information. | Percentage_correct => 0.0\n",
      "Based on the available information, the specific method achieving the highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is not clearly identified in the retrieved documents. Further detailed research or access to specific datasets and publications may be required to obtain this information. | NAN => 1.0\n",
      "The method achieving the highest BLEU score on the WMT2014 English-German dataset for Machine Translation is Bi-SimCut with a BLEU score of 35.15. | Weighted_Transformer__large_ => 0.0\n",
      "The DeepFM method achieves the highest Log_Loss score for Click-Through Rate Prediction on the Criteo dataset. | Criteo => 1.0\n",
      "The TARNet method for causal inference is evaluated on datasets such as the Infant Health and Development Program dataset and the Lalonde dataset. These datasets are commonly used for benchmarking treatment effect estimation algorithms. | IDHP => 0.5\n",
      "OpenAI's GPT-3 reportedly scored a word-level perplexity score of 20.5 on the Penn Treebank Word Level dataset for the Language Modelling task. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The highest MRR score on the FB15k dataset for the Link Prediction task is achieved by NBFNet, with an MRR of 0.9237. | TuckER => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:28<18:35, 28.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric for the Speech Recognition task. | Percentage_error => 1.0\n",
      "The Bi-LSTM trained on the FCE dataset achieves the highest F0.5 score of 34.3 for the Grammatical Error Detection task. | CoNLL-2014_A2 => 0.0\n",
      "The CornerNet-Squeeze method is evaluated on the PASCAL VOC and MS COCO datasets for the Real-Time Object Detection task. | COCO => 0.5\n",
      "The method that achieves the highest PCK score on the Leeds Sports Poses dataset for the Pose Estimation task is OmniPose with a PCK score of 99.5%. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The Duel_noop method is evaluated on the Atari-5 dataset, which includes a subset of five ALE games. | Atari_2600_Ms__Pacman => 0.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on the COLX 563 dataset, with scores above 97 F1 on in-domain datasets. | CoNLL_2005 => 0.0\n",
      "Frustum_PointNets is evaluated on the KITTI and SUN RGB-D datasets for the Object Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The DANN method evaluation metrics for the Multi-Domain Sentiment Dataset in the Sentiment Analysis task are not explicitly found in the available resources. It is recommended to refer to specific research papers or documentation related to the DANN method for detailed evaluation metrics. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The PFF method evaluation datasets for the Image Super-Resolution task were not found in the available resources. It is recommended to check specific research papers or publications related to the PFF method for detailed information on the datasets used. | Set14_-_4x_upscaling => 0.0\n",
      "The DDQN__tuned__noop method is evaluated on various Atari Games datasets, but specific datasets are not clearly identified in the available search results. | Atari_2600_Berzerk => 0.0\n",
      "The datasets on which the Sample_Clustering method is evaluated for the Few-Shot Image Classification task could not be found using the available tools. It is recommended to consult specific research papers or documentation related to the Sample_Clustering method for detailed information. | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using various evaluation metrics, but specific metrics were not found in the search results. Typically, collaborative filtering methods like Mult-DAE are evaluated using metrics such as RMSE (Root Mean Square Error), MAE (Mean Absolute Error), precision, recall, and F1-score. However, for precise metrics used in the evaluation of Mult-DAE on the Netflix dataset, consulting the original research paper or related documentation would be necessary. | Recall_20, Recall_50 => 0.0\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link Prediction task using metrics such as Mean Reciprocal Rank (MRR) and Hits@100. | Accuracy => 0.0\n",
      "The Duel_hs method evaluation datasets for the Atari_Games task are not explicitly mentioned in the available resources. It might be beneficial to consult specific research papers or documentation related to the Duel_hs method for detailed information. | Atari_2600_Video_Pinball => 0.0\n",
      "The DDQN__tuned__hs method evaluated datasets for the Atari_Games task were not found in the available search results. Further specific information might be required to locate the exact datasets used. | Atari_2600_Assault => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric. This involves determining the relationship between sentence pairs as entailment, contradiction, or neutral. | Matched, Mismatched => 0.0\n",
      "The PSENet-1s method is typically evaluated on the SCUT-CTW1500 dataset using metrics such as Precision, Recall, and F1-score for the Curved_Text_Detection task. These metrics help in assessing the accuracy and effectiveness of the text detection model in identifying and localizing curved text in images. | F-Measure => 1.0\n",
      "The LapSRN method is evaluated on the Urban100 - 4x upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). | PSNR => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:40<11:48, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games, using both human and noop start settings. The evaluation includes mean and median human normalized scores, as well as mean rank and Elo scores across all games. | Atari_2600_Time_Pilot => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  70%|███████   | 28/40 [00:41<00:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method achieving the highest PSNR score on the Set14 4x upscaling dataset for Image Super-Resolution is DRCT-L with a PSNR of 29.54. | PFF => 0.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL VOC 2012 dataset for Weakly Supervised Object Detection using metrics such as Average Precision (AP) at 50% Intersection-over-Union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth, while CorLoc evaluates the percentage of images with at least one correctly localized object instance. | MAP => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [00:42<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The A3C-CTS method is evaluated on the Atari 2600 suite of games, which includes a variety of games used for benchmarking reinforcement learning algorithms. However, specific datasets or games for the A3C-CTS method were not explicitly mentioned in the retrieved documents. | Atari_2600_Venture => 0.0\n",
      "Average Score: 0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated new instruction: To effectively accomplish the task using the provided tools, follow these enhanced guidelines while retaining the core principles from the previous instruction. Begin by evaluating the specificity and clarity of the input query. For queries that are specific and directly related to a dataset or metric, prioritize using tools like `ARXIV_SEARCH` or `RETRIEVE`. For broader or ambiguous queries, consider using the `WEB_SEARCH` tool, which is better suited for handling such queries. Implement a preprocessing step to break down complex queries into simpler components, ensuring they align more closely with the tool's capabilities. This approach will help in identifying patterns that lead to successful outcomes.\n",
      "\n",
      "Incorporate a more adaptive computational logic for tool selection and query refinement. If the initial tool choice does not yield satisfactory results, employ a fallback mechanism by trying another tool with a refined query. Establish a feedback loop where the system learns from past failures, adjusting tool selection and query refinement strategies based on previous performance. This continuous learning process will enhance the system's ability to handle negative inputs effectively, ensuring that multiple approaches are considered before concluding the task.\n",
      "\n",
      "Finally, once the appropriate tool and refined query are determined, execute the `Action` needed to accomplish the `Goal`. The `Action` should include the selected tool and the input query to pass to the tool. Remember, you can opt to use no tools and provide the final answer directly if applicable. You can also use one tool multiple times with different input queries if necessary. By following these steps, you will enhance the effectiveness of tool usage and improve overall performance on the task, leading to better handling of both positive and negative inputs.\n",
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 43.35 seconds\n",
      "                Total API Calls: 4\n",
      "                - Comparator calls: 2\n",
      "                - Feedback instruction calls: 2\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 63,500\n",
      "                - Input tokens: 61,926\n",
      "                - Output tokens: 1,574\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $1.9522\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "result = iterative_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization cost: $1.9522\n",
      "Final score achieved: 0.250\n",
      "\n",
      "Iteration 0:\n",
      "Score: 0.250\n",
      "Comparator tokens in: 27790\n",
      "Comparator tokens out: 477\n",
      "Feedback tokens in: 623\n",
      "Feedback tokens out: 384\n",
      "Execution time: 94.29s\n",
      "\n",
      "Iteration 1:\n",
      "Score: 0.225\n",
      "Comparator tokens in: 32703\n",
      "Comparator tokens out: 387\n",
      "Feedback tokens in: 810\n",
      "Feedback tokens out: 326\n",
      "Execution time: 59.78s\n"
     ]
    }
   ],
   "source": [
    "optimized_actor_agent = result[\"agent\"]\n",
    "optimization_metrics = result[\"metrics\"]\n",
    "\n",
    "# Now you can process the metrics\n",
    "print(f\"Total optimization cost: ${optimization_metrics['total_cost']:.4f}\")\n",
    "print(f\"Final score achieved: {optimization_metrics['final_score']:.3f}\")\n",
    "\n",
    "# Analyze per-iteration performance\n",
    "for iteration in optimization_metrics['iteration_details']:\n",
    "    print(f\"\\nIteration {iteration['iteration']}:\")\n",
    "    print(f\"Score: {iteration['score']:.3f}\")\n",
    "    print(f\"Comparator tokens in: {iteration['comparator_metrics']['tokens_in']}\")\n",
    "    print(f\"Comparator tokens out: {iteration['comparator_metrics']['tokens_out']}\")\n",
    "    print(f\"Feedback tokens in: {iteration['feedback_metrics']['tokens_in']}\")\n",
    "    print(f\"Feedback tokens out: {iteration['feedback_metrics']['tokens_out']}\")\n",
    "    print(f\"Execution time: {iteration['execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U-Net method for Skin Cancer Segmentation is evaluated on benchmark datasets from the International Skin Imaging Collaboration 2018 (ISIC-2018) and ISIC-2017. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH is evaluated on the Switchboard (SWB) and CallHome (CH) test sets for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The highest F1 score achieved on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is 85.11%. | CVT___Multi-Task => 0.0\n",
      "The Snips method is evaluated on the Snips SmartLights dataset for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The TuckER method is evaluated on standard link prediction datasets, including FB15K-237 and WN18RR. | FB15k-237 => 0.5\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method for Natural Language Inference is evaluated on the Stanford Natural Language Inference (SNLI) dataset, MultiGenre Natural Language Inference (MultiNLI) dataset, and Quora Question Pairs dataset. | SNLI => 0.5\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using metrics such as Average Endpoint Error (AEE) and Percentage of Correct Keypoints (PCK). Lower AEE and higher PCK values indicate better performance. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The FRCN method is evaluated on datasets such as the PASCAL Visual Object Classes (VOC) challenges and the ILSVRC ImageNet dataset. | PASCAL_VOC_2007 => 0.5\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for Head Pose Estimation, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes various Atari games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The novel directed hypergraph neural network method achieves the highest accuracy for node classification on the Cora dataset. | GCN => 0.0\n",
      "The method TANDA achieves the highest MAP score on the WikiQA dataset for the Question Answering task, with a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using metrics such as perplexity, cross entropy, and bits-per-character (BPC). | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The IDE CamStyle Random Erasing method for Person Re-Identification is evaluated on the Market-1501 dataset. | Market-1501 => 1.0\n",
      "The specific evaluation metrics for DeepLab-LargeFOV on the SUN-RGBD dataset for scene segmentation were not found in the search results. Typically, semantic segmentation methods like DeepLab-LargeFOV are evaluated using metrics such as mean Intersection over Union (mIoU), pixel accuracy, and boundary F1 score. However, for precise metrics used in a specific study, consulting the original research paper or dataset documentation would be necessary. | Mean_IoU => 0.5\n",
      "The highest Recall_50 score on the Million Song Dataset for the Collaborative Filtering task is achieved by the EASE method with a score of 0.428. | Mult-VAE_PR => 0.0\n",
      "The method achieving the highest Mean_IoU score on the CamVid dataset for semantic segmentation is Border-SegGCN, with a score of 81.96%. | PSPNet => 0.0\n",
      "The Paragraph Vector method for Question Answering tasks is evaluated on datasets such as SQuAD-Open and HotpotQA. | WikiQA => 0.0\n",
      "The CRN method for Image-to-Image Translation does not have specific datasets mentioned in the available search results. The searches did not yield any specific datasets evaluated with the CRN method for this task. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for Image Classification. | Res2NeXt-29 => 0.0\n",
      "The DRCN method for image super-resolution on the Set5 dataset with 4x upscaling is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | MOS, PSNR, SSIM => 0.5\n",
      "The current state-of-the-art method achieving the highest score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero. | Bootstrapped_DQN => 0.0\n",
      "Unable to find specific dataset where SVDCNN achieves the highest error score for Sentiment Analysis task. Further detailed search may be required. | Yelp_Fine-grained_classification => 0.0\n",
      "Unable to find specific information on the highest Error score for the Yelp_Binary_classification dataset in Sentiment_Analysis. The current state-of-the-art model is XLNet, but specific error scores are not available in the retrieved data. | Char-level_CNN => 0.0\n",
      "The DPN-131 method for image classification has been evaluated on datasets such as ImageNet, STL-10, and Places365-Standard. These datasets are commonly used for evaluating image classification algorithms. | ImageNet => 0.0\n",
      "The evaluation metrics for the Paragraph_vector__lexical_overlap___dist_output_ method on the QASent dataset for the Question Answering task could not be found using the available tools. It is possible that this specific information is not publicly available or documented in the sources accessible through the tools provided. | MAP, MRR => 0.0\n",
      "The NICE method for the Image Generation task on the CIFAR-10 dataset is typically evaluated using metrics such as the Inception Score, which measures the image quality and diversity of generated images. However, specific metrics for the NICE method were not found in the search results. | NLL_Test => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [01:14<1:12:50, 74.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find specific information on the highest Score score for the Atari_2600_Name_This_Game dataset in the Atari_Games task. Consider checking recent publications or repositories related to Atari game benchmarks for the latest methods and scores. | IQN => 0.0\n",
      "The highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task is not explicitly available from the current search results. However, models like ESIM have demonstrated state-of-the-art performance with accuracy scores around 88% on the SNLI dataset. For the most accurate and up-to-date information, consulting specific research papers or benchmark leaderboards is recommended. | __Unigram_and_bigram_features => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task using metrics such as Exact Match (EM) and F1 score. However, specific evaluation metrics for the Impatient_Reader method were not found in the search results. | CNN, Daily_Mail => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as the Xiph and the Ultra Video Group database. | Vid4_-_4x_upscaling => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, which includes a wide range of games such as Montezuma's Revenge and others in the Atari suite. The evaluations are typically done using different starting conditions like null op starts and human starts to test generalization and performance. | Atari_2600_Chopper_Command => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both noop and human start settings. The evaluation includes mean and median human normalized scores across all games, comparing DQN with other state-of-the-art algorithms. | Atari_2600_River_Raid => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The Decomposable Attention Model achieves state-of-the-art results on the SNLI dataset with fewer parameters than previous models, as per the ARXIV_SEARCH results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the retrieved documents. However, the CoNLL-2014 shared task typically uses metrics like precision, recall, and F0.5 score, which weights precision twice as much as recall, for evaluating grammatical error detection systems. | F0_5 => 0.5\n",
      "The Deep Speech method for speech recognition has been evaluated on several datasets, including the TIMIT Acoustic-Phonetic Continuous Speech Corpus, which is a standard dataset used for evaluating automatic speech recognition systems. | Switchboard___Hub500 => 0.0\n",
      "The Transformer method for machine translation has been evaluated on datasets such as the WMT 2014 English-to-German and English-to-French translation tasks. These datasets are commonly used benchmarks in the field of machine translation. | IWSLT2015_English-German => 0.0\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, which measures the quality of the generated text by comparing it to reference texts. The method's performance is assessed by its ability to generate coherent and informative descriptions based on the comprehensive understanding of both the content and the structure of a table. | BLEU, ROUGE => 0.5\n",
      "The VAT_EntMin method for semi-supervised image classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset for the Face Detection task. | WIDER_Face__Easy_ => 0.0\n",
      "The IQN method achieves the highest Score score on the Atari 2600 games dataset, specifically on the game Pong, where it reaches a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The FDNet method is evaluated on the WIDER Face Easy dataset using metrics such as precision and recall, achieving an accuracy of 95.9% on the easy set. | AP => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the following datasets for the Image Classification task: STL-10, CIFAR-10, and Caltech-101. | CIFAR-10 => 0.5\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using metrics such as Average Viewpoint Precision (AVP) and potentially other standard metrics like mAP (mean Average Precision) and CorLoc (Correct Localization). | Mean_PCK => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset for Image Super-Resolution using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). | PSNR, SSIM => 1.0\n",
      "The SVDCNN method for text classification is evaluated on the following datasets: ICDAR 2015 Incidental Text (Challenge 4), MSRA-TD500, and ICDAR 2013. | AG_News => 0.0\n",
      "The Spynet method for Optical Flow Estimation is evaluated on standard optical flow benchmarks such as MPI-Sintel and KITTI datasets. | Sintel-final => 0.5\n",
      "The highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is not explicitly available from the retrieved sources. However, the SparseGPT model with 175 billion parameters is mentioned as a state-of-the-art model on WikiText-2, although specific scores related to 'Number_of_params' are not provided. | AWD-LSTM-DOC => 0.0\n",
      "The Stacked Hourglass Networks achieve state-of-the-art results on the MPII Human Pose dataset, but the specific dataset with the highest PCK_0_2 score is not explicitly mentioned in the retrieved documents. | FLIC_Elbows => 0.0\n",
      "The MemNNs__ensemble_ method for the Question Answering task is evaluated on datasets such as CNN, Daily Mail, and CBT. | CNN___Daily_Mail => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the effectiveness of the image space adaptation in achieving state-of-the-art performance in unsupervised domain adaptation tasks. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found using the available tools. It is recommended to consult the original research paper or supplementary materials for detailed information on the evaluation metrics used. | Score => 0.0\n",
      "The available data does not provide a specific dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further detailed research or access to specific experimental results might be required to answer this question accurately. | Atari_2600_Video_Pinball => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [01:47<48:36, 50.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated using metrics such as Area Under the ROC Curve (AUC) and Relative Information Gain (RIG) for the Click-Through Rate Prediction task on datasets similar to Bing_News. | AUC, Log_Loss => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:47<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on several popular datasets for the Image Classification task, including STL-10, CIFAR-10, and Caltech-101. | STL-10 => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21666666666666667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
