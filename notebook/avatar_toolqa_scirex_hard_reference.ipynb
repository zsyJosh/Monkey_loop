{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USR_NAME = 'shirwu'\n",
    "TOOL_QA_ROOT = '/dfs/project/kgrlm/shirwu/msr_intern/home/t-yingxinwu/msr_intern/ToolQA-rebuttal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'hard'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "# dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) \n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    "    max_iters=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Disable all INFO logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Silence all loggers that might be chatty\n",
    "loggers_to_silence = [\n",
    "    \"httpx\",\n",
    "    \"httpcore\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"dspy\",\n",
    "    \"langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"tiktoken\",\n",
    "    \"asyncio\",\n",
    "    \"faiss\",\n",
    "    \"anthropic\"\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_silence:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable tokenizer parallelism warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Which method achieves the highest PCK score on Leeds_Sports_Poses dataset for Pose_Estimation task?', 'answer': 'Pyramid_Residual_Modules__PRMs_'}) (input_keys={'paper_id', 'question'})\n",
      "physics | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APICallMetrics:\n",
    "    timestamp: datetime\n",
    "    tool_name: str\n",
    "    tokens_in: int = 0\n",
    "    tokens_out: int = 0\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AvatarMetrics:\n",
    "    total_calls: int = 0\n",
    "    total_tokens_in: int = 0\n",
    "    total_tokens_out: int = 0\n",
    "    total_execution_time: float = 0.0\n",
    "    calls_by_tool: Dict[str, int] = field(default_factory=dict)\n",
    "    api_call_history: List[APICallMetrics] = field(default_factory=list)\n",
    "    \n",
    "    def add_call(self, metrics: APICallMetrics):\n",
    "        self.total_calls += 1\n",
    "        self.total_tokens_in += metrics.tokens_in\n",
    "        self.total_tokens_out += metrics.tokens_out\n",
    "        self.total_execution_time += metrics.execution_time\n",
    "        self.calls_by_tool[metrics.tool_name] = self.calls_by_tool.get(metrics.tool_name, 0) + 1\n",
    "        self.api_call_history.append(metrics)\n",
    "    \n",
    "    def merge(self, other: 'AvatarMetrics'):\n",
    "        \"\"\"Merge another AvatarMetrics instance into this one\"\"\"\n",
    "        self.total_calls += other.total_calls\n",
    "        self.total_tokens_in += other.total_tokens_in\n",
    "        self.total_tokens_out += other.total_tokens_out\n",
    "        self.total_execution_time += other.total_execution_time\n",
    "        for tool, count in other.calls_by_tool.items():\n",
    "            self.calls_by_tool[tool] = self.calls_by_tool.get(tool, 0) + count\n",
    "        self.api_call_history.extend(other.api_call_history)\n",
    "\n",
    "    def estimate_cost(self, model_name: str = \"gpt-4\") -> float:\n",
    "        pricing = {\n",
    "            \"gpt-4\": {\"input\": 2.5, \"output\": 10.0},\n",
    "        }\n",
    "        if model_name not in pricing:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        rates = pricing[model_name]\n",
    "        input_cost = (self.total_tokens_in / 1000000) * rates[\"input\"]\n",
    "        output_cost = (self.total_tokens_out / 1000000) * rates[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "class AvatarWithMetrics(Avatar):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metrics = AvatarMetrics()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error counting tokens: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _wrapped_tool_call(self, tool, input_text: str) -> str:\n",
    "        start_time = time.time()\n",
    "        tokens_in = self._count_tokens(input_text)\n",
    "        \n",
    "        try:\n",
    "            result = tool.run(input_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tool execution error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            execution_time = time.time() - start_time\n",
    "            tokens_out = self._count_tokens(str(result))\n",
    "            \n",
    "            metrics = APICallMetrics(\n",
    "                timestamp=datetime.now(),\n",
    "                tool_name=tool.name,\n",
    "                tokens_in=tokens_in,\n",
    "                tokens_out=tokens_out,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self.metrics.add_call(metrics)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        metrics = APICallMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            tool_name=\"main_llm\",\n",
    "            tokens_in=self._count_tokens(str(args) + str(kwargs)),\n",
    "            tokens_out=self._count_tokens(str(result)),\n",
    "            execution_time=total_time\n",
    "        )\n",
    "        self.metrics.add_call(metrics)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for example in test_set:\n",
    "            def process_with_metrics(example=example):\n",
    "                try:\n",
    "                    avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "                    prediction = avatar(**example.inputs().toDict())\n",
    "                    return metric(example, prediction), avatar.metrics\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return 0, AvatarMetrics()\n",
    "\n",
    "            futures.append(executor.submit(process_with_metrics))\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            score, metrics = future.result()\n",
    "            total_score += score\n",
    "            # Only combine token counts and call counts, not execution times\n",
    "            combined_metrics.total_calls += metrics.total_calls\n",
    "            combined_metrics.total_tokens_in += metrics.total_tokens_in\n",
    "            combined_metrics.total_tokens_out += metrics.total_tokens_out\n",
    "            for tool, count in metrics.calls_by_tool.items():\n",
    "                combined_metrics.calls_by_tool[tool] = combined_metrics.calls_by_tool.get(tool, 0) + count\n",
    "            combined_metrics.api_call_history.extend(metrics.api_call_history)\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    combined_metrics.total_execution_time = total_execution_time\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        try:\n",
    "            avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "            prediction = avatar(**example.inputs().toDict())\n",
    "            score = metric(example, prediction)\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in avatar.metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def format_metrics_report(metrics: AvatarMetrics, model_name: str = \"gpt-4\") -> str:\n",
    "    cost = metrics.estimate_cost(model_name)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Avatar Execution Metrics Report\n",
    "==============================\n",
    "Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "Total API Calls: {metrics.total_calls}\n",
    "Total Tokens: {metrics.total_tokens_in + metrics.total_tokens_out:,} ({metrics.total_tokens_in:,} in, {metrics.total_tokens_out:,} out)\n",
    "Estimated Cost: ${cost:.4f}\n",
    "\n",
    "Average Time per Call: {metrics.total_execution_time / metrics.total_calls:.2f} seconds\n",
    "\n",
    "Tool Usage Breakdown:\n",
    "-------------------\n",
    "\"\"\"\n",
    "    for tool, count in sorted(metrics.calls_by_tool.items()):\n",
    "        report += f\"{tool}: {count} calls\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:12<12:01, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Score score on the Atari_2600_Name_This_Game dataset for the Atari_Games task is MuZero. | IQN => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The highest reported Mean_IoU score on the CamVid dataset for Semantic Segmentation is 66.1%. | PSPNet => 0.0\n",
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB Fisher CH is evaluated on the Switchboard and Hub500 datasets for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, as mentioned in the context of various research papers discussing reinforcement learning methods applied to Atari games. | Atari_2600_Chopper_Command => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "LiteFlowNet achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for the 3D Face Reconstruction task using metrics such as geometric error between reconstructed meshes and the ground truth. | Mean_NME_ => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The DPN-131 method is evaluated on datasets such as ImageNet and OSIE for the Image Classification task. | ImageNet => 0.5\n",
      "The method that achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the Direct Output Connection for a High-Rank Language Model, which is a state-of-the-art recurrent neural network (RNN) language model. | AWD-LSTM-DOC => 0.0\n",
      "The search did not yield specific datasets for the Deep_Speech method evaluation in Speech Recognition. Further detailed search or specific papers might be needed to find this information. | Switchboard___Hub500 => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes a variety of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The Paragraph_vector method for the Question Answering task has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "MuZero achieves the highest Score score on the Atari_2600_Robotank dataset for the Atari_Games task with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The Snips method is evaluated on the TIMIT dataset for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The Transformer method is evaluated on the WMT 2014 English-to-German and English-to-French translation tasks for the Machine Translation task. | IWSLT2015_English-German => 0.0\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The MemNNs__ensemble_ method for the Question_Answering task is evaluated on the CNN, Daily Mail, and CBT CN and NE datasets. | CNN___Daily_Mail => 0.5\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and IFC (Information Fidelity Criterion). | PSNR, SSIM => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as Set5 and SuperTexture. | Vid4_-_4x_upscaling => 0.0\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using the Percentage of Correct Keypoints (PCK) metric and Average Precision (AP) for object detection. | Mean_PCK => 0.5\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n",
      "The SVDCNN method for text classification is evaluated on several large-scale datasets, including AG's News Corpus, Sogou News Corpus, DBPedia Ontology Dataset, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The CRN method for Image-to-Image Translation task is not explicitly mentioned in the retrieved documents. Therefore, I cannot provide specific datasets it was evaluated on. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image Generation task were not found in the retrieved documents. Based on common practices in image generation tasks, metrics like Inception Score (IS) and Fréchet Inception Distance (FID) are typically used, but specific metrics for NICE on CIFAR-10 were not identified. | NLL_Test => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as Exact Match (EM) and F1 score. | MAP, MRR => 0.0\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset for Image Super-Resolution using metrics such as PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). | MOS, PSNR, SSIM => 0.67\n",
      "The FDNet method is evaluated on the WIDER_FACE Easy dataset for the Face Detection task using metrics such as precision and recall, achieving 95.9% on the easy set. | AP => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII dataset. | FLIC_Elbows => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the available resources. The CoNLL-2014 shared task generally uses metrics like precision, recall, and F0.5 score, which weights precision twice as much as recall, for evaluating grammatical error detection and correction systems. | F0_5 => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both human and noop start settings. | Atari_2600_River_Raid => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as boundary F1-measure (BF) to complement existing metrics that are more biased towards region accuracies. | Mean_IoU => 0.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is defined as 'accuracy@', where a pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. This threshold is set to allow some tolerance in blurred areas that are difficult to match exactly. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU. The method shows improved performance over baseline models, particularly in generating coherent and informative descriptions by incorporating both content and structure of the table through a dual attention mechanism. | BLEU, ROUGE => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question_Answering task using accuracy as the primary metric. The performance is measured by the proportion of test cases where the ground truth is among the top answers proposed by the model. | CNN, Daily_Mail => 0.5\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n",
      "The current state-of-the-art method for the Yelp Binary classification dataset in the Sentiment Analysis task is XLNet. However, specific information about the method achieving the highest error score is not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the performance of the model in terms of semantic segmentation and pixel-level adaptation. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The DR-BiLSTM (Single) model achieves the highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task, outperforming other models including ensemble models when preprocessing is applied. | __Unigram_and_bigram_features => 0.0\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for specific evaluation metrics used. | Score => 0.0\n",
      "The method with the highest Parameters score on the SNLI dataset for the Natural Language Inference task is not explicitly mentioned in the retrieved results. However, the Decomposable Attention Model is noted for achieving state-of-the-art results with fewer parameters compared to previous work. Further specific details on the highest Parameters score would require more targeted research or access to the latest benchmark results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The available searches did not provide specific information about the dataset on which the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further detailed research or access to specific papers or datasets might be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The available resources did not provide specific information on the dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further investigation in specific research papers or datasets related to Atari Games and DDQN methods might be required. | Atari_2600_Video_Pinball => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:05<00:00,  2.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated on the Bing_News dataset for Click-Through Rate Prediction using metrics such as Area Under the Curve (AUC). | AUC, Log_Loss => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score, metrics = multi_thread_executor(toolqa_test, ToolQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.24\n",
      "\n",
      "Avatar Execution Metrics Report\n",
      "==============================\n",
      "Execution Time: 127.10 seconds\n",
      "Total API Calls: 60\n",
      "Total Tokens: 91,949 (1,702 in, 90,247 out)\n",
      "Estimated Cost: $0.9067\n",
      "\n",
      "Average Time per Call: 2.12 seconds\n",
      "\n",
      "Tool Usage Breakdown:\n",
      "-------------------\n",
      "main_llm: 60 calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average Score on ArxivQA before opitmization: {aqa_score:.2f}\")\n",
    "print(f\"Test Score: {score:.2f}\")\n",
    "print(format_metrics_report(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 0.00 seconds\n",
      "                Evaluation Time: 0.00 seconds\n",
      "                Total API Calls: 0\n",
      "                - Comparator calls: 0\n",
      "                - Feedback instruction calls: 0\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 0\n",
      "                - Input tokens: 0\n",
      "                - Output tokens: 0\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $0.0000\n",
      "                \n",
      "Processing batch 1 of 2...\n",
      "The method that achieves the highest Score score on the Atari_2600_Name_This_Game dataset for the Atari_Games task is MuZero. | IQN => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, as mentioned in the context of various research papers discussing reinforcement learning methods applied to Atari games. | Atari_2600_Chopper_Command => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "LiteFlowNet achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is achieved by the method described in the paper \"Direct Output Connection for a High-Rank Language Model\" by Sho Takase, Jun Suzuki, and Masaaki Nagata. This method improves the current state-of-the-art language model and achieves the best score on the WikiText-2 dataset. | AWD-LSTM-DOC => 0.0\n",
      "The search did not yield specific datasets for the Deep_Speech method evaluation in Speech Recognition. Further detailed search or specific papers might be needed to find this information. | Switchboard___Hub500 => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the standard benchmark metric, which is the geometric error between reconstructed meshes and the ground truth. | Mean_NME_ => 0.5\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The Paragraph_vector method for the Question Answering task has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The DPN-131 method is evaluated on datasets such as ImageNet and OSIE for the Image Classification task. | ImageNet => 0.5\n",
      "The method that achieves the highest Score score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes a variety of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The method achieving the highest Mean_IoU score on the CamVid dataset for Semantic Segmentation is Border-SegGCN, with a score of 81.96%. | PSPNet => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using metrics related to feature matching and homography estimation. However, specific metrics such as accuracy or error rates are not explicitly mentioned in the available resources. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The Field-gating Seq2seq dual attention method for the Table-to-text Generation task on the WikiBio dataset is evaluated using metrics such as BLEU, ROUGE, and PARENT. These metrics assess the quality of the generated text by comparing it to reference texts, with PARENT being specifically designed to better correlate with human judgments by aligning n-grams from the reference and generated texts to the semi-structured data. | BLEU, ROUGE => 0.5\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The MemNNs__ensemble_ method for the Question_Answering task is evaluated on the CNN, Daily Mail, and CBT CN and NE datasets. | CNN___Daily_Mail => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The Transformer method is evaluated on the WMT 2014 English-to-German and English-to-French translation tasks for the Machine Translation task. | IWSLT2015_English-German => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The Snips method is evaluated on the TIMIT dataset for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as Mean Intersection over Union (Mean IoU) and Global Pixel Accuracy. These metrics are commonly used to assess the performance of semantic segmentation models. | Mean_IoU => 0.5\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB, Fisher, and CH is evaluated on datasets such as the Switchboard portion of the NIST 2000 evaluation set and the CallHome portion. Additionally, experiments were conducted on the TIMIT Acoustic-Phonetic Continuous Speech Corpus. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the performance of the model in terms of semantic segmentation and pixel-level adaptation. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n",
      "The CRN method datasets for the Image-to-Image Translation task were not found in the available resources. Further specific research or access to the original paper detailing the CRN method might be required to obtain this information. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image Generation task are not explicitly mentioned in the retrieved documents. However, common metrics for evaluating image generation tasks on datasets like CIFAR-10 include Inception Score and Fréchet Inception Distance (FID). | NLL_Test => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and IFC (Information Fidelity Criterion). | PSNR, SSIM => 0.5\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as Exact Match (EM) and F1 score. These metrics assess the accuracy of the predicted answer spans compared to the ground truth answers. | MAP, MRR => 0.0\n",
      "The available resources did not provide a specific dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further detailed research or access to specific experimental results might be required to find this information. | Atari_2600_Video_Pinball => 0.0\n",
      "The Ann_PAT_MT method is evaluated on the CoNLL-2014_A2 dataset for Grammatical Error Detection using metrics such as precision, recall, and F-score, which are common in evaluating error detection tasks. | F0_5 => 0.5\n",
      "The available resources did not provide specific information on the dataset where the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further detailed research or access to specific academic papers or datasets might be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The SVDCNN method for text classification is evaluated on several large-scale datasets, including AG's News Corpus, Sogou News Corpus, DBPedia Ontology Dataset, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both human and noop start settings. | Atari_2600_River_Raid => 0.0\n",
      "The current state-of-the-art method for the Yelp Binary classification dataset in the Sentiment Analysis task is XLNet. However, specific information about the method achieving the highest error score is not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The ConvNet method is evaluated on the Pascal3D dataset for the Keypoint Detection task using metrics such as detection accuracy, which is often represented by regression loss. However, specific metrics for Pascal3D were not found in the retrieved documents. | Mean_PCK => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face Easy dataset for the Face Detection task using metrics such as precision, recall, and average precision (AP). The method achieves a 95.9% precision on the Easy set of the validation dataset. | AP => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The method with the highest Parameters score on the SNLI dataset for the Natural Language Inference task is not explicitly mentioned in the retrieved results. However, the Decomposable Attention Model is noted for achieving state-of-the-art results with fewer parameters compared to previous work. Further specific details on the highest Parameters score would require more targeted research or access to the latest benchmark results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII dataset. | FLIC_Elbows => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question_Answering task using accuracy as the primary metric. The performance is measured by the proportion of test cases where the ground truth is among the top answers proposed by the model. | CNN, Daily_Mail => 0.5\n",
      "The DR-BiLSTM (Single) model achieves the highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task, outperforming both single and ensemble models with preprocessing. | __Unigram_and_bigram_features => 0.0\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset for Image Super-Resolution using metrics such as PSNR (Peak Signal-to-Noise Ratio). | MOS, PSNR, SSIM => 0.5\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as Set5, Set14, and other video benchmarks commonly used in super-resolution tasks. | Vid4_-_4x_upscaling => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [01:25<41:21, 42.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method for Click-Through Rate Prediction on the Bing_News dataset is evaluated using metrics such as the Area Under the Curve (AUC). | AUC, Log_Loss => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:31<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found using the available tools. It is possible that this specific information is not publicly available or documented in the sources accessible through the tools provided. | Score => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 of 2...\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using metrics related to feature matching and homography estimation. However, specific metrics such as accuracy or error rates are not explicitly mentioned in the available resources. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "LiteFlowNet achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The DPN-131 method is evaluated on datasets such as ImageNet and OSIE for the Image Classification task. | ImageNet => 0.5\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, as mentioned in the context of various research papers discussing reinforcement learning methods applied to Atari games. | Atari_2600_Chopper_Command => 0.0\n",
      "The search did not yield specific datasets for the Deep_Speech method evaluation in Speech Recognition. Further detailed search or specific papers might be needed to find this information. | Switchboard___Hub500 => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for the 3D Face Reconstruction task using the standard benchmark metric, which is the geometric error between reconstructed meshes and the ground truth. | Mean_NME_ => 0.5\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The Paragraph_vector method for the Question Answering task has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The method that achieves the highest Score score on the Atari_2600_Robotank dataset for the Atari_Games task is MuZero with a score of 131.13. | Bootstrapped_DQN => 0.0\n",
      "The highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is achieved by the method described in the paper \"Direct Output Connection for a High-Rank Language Model\" by Sho Takase, Jun Suzuki, and Masaaki Nagata. This method improves the current state-of-the-art language model and achieves the best score on the WikiText-2 dataset. | AWD-LSTM-DOC => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes a variety of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "SERNet-Former achieves the highest Mean_IoU score of 84.62% on the CamVid dataset for the Semantic Segmentation task. | PSPNet => 0.0\n",
      "The current state-of-the-art Train Accuracy on the SNLI dataset for the Natural Language Inference task is achieved by Neural Tree Indexers for Text Understanding, with an accuracy of 86.1%. | __Unigram_and_bigram_features => 0.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and IFC (Information Fidelity Criterion). | PSNR, SSIM => 0.5\n",
      "The Transformer method is evaluated on the WMT 2014 English-to-German and English-to-French translation tasks for the Machine Translation task. | IWSLT2015_English-German => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both human and noop start settings. | Atari_2600_River_Raid => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset for Image Super-Resolution using metrics such as PSNR (Peak Signal-to-Noise Ratio). | MOS, PSNR, SSIM => 0.5\n",
      "The IQN method achieves the highest Score score for the Atari_Games task on the Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The available resources did not provide a specific dataset where the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further detailed research or access to specific experimental results might be required to find this information. | Atari_2600_Video_Pinball => 0.0\n",
      "The SVDCNN method for text classification is evaluated on several large-scale datasets, including AG's News Corpus, Sogou News Corpus, DBPedia Ontology Dataset, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using the Percentage of Correct Keypoints (PCK) metric. | Mean_PCK => 0.5\n",
      "The available searches did not provide specific information about the dataset on which the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further specific research or access to the original paper or dataset documentation might be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as Set5 and SuperTexture. | Vid4_-_4x_upscaling => 0.0\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The CRN method datasets for the Image-to-Image Translation task were not found in the available resources. Further specific research or access to the original paper describing the CRN method might be necessary to obtain this information. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n",
      "The current state-of-the-art method for the Yelp Binary classification dataset in the Sentiment Analysis task is XLNet. However, specific information about the method achieving the highest error score is not readily available from the search results. | Char-level_CNN => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB, Fisher, and CH is evaluated on the Switchboard and CallHome portions of the NIST 2000 evaluation set for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 1.0\n",
      "The FDNet method is evaluated on the WIDER_FACE Easy dataset for the Face Detection task using metrics such as precision and recall, achieving 95.9% on the easy set, 94.5% on the medium set, and 87.9% on the hard set on the validation set. | AP => 0.0\n",
      "The highest Recall_50 score on the Million Song Dataset for the Collaborative Filtering task is not explicitly available from the current search results. Further specific research or access to detailed competition results might be required to obtain this information. | Mult-VAE_PR => 0.0\n",
      "The Snips method is evaluated on the TIMIT dataset for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The MemNNs__ensemble_ method for the Question_Answering task is evaluated on the CNN, Daily Mail, and CBT CN and NE datasets. | CNN___Daily_Mail => 0.5\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as boundary F1-measure (BF) to complement existing metrics that are more biased towards region accuracies. | Mean_IoU => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as Exact Match (EM) and F1 score. These metrics assess the accuracy of the predicted answer spans compared to the ground truth answers. | MAP, MRR => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the performance of the model in terms of semantic segmentation and pixel-level adaptation. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The Prior_Duel_hs method evaluation metrics for the Atari_2600_Alien dataset in the Atari_Games task could not be found in the available resources. It is recommended to check the original research paper or supplementary materials for specific evaluation metrics used. | Score => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the available resources. However, common evaluation metrics for grammatical error detection tasks include precision, recall, and F0.5 score, which weights precision twice as much as recall. These metrics are typically used to assess the performance of systems in detecting grammatical errors. | F0_5 => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII dataset. | FLIC_Elbows => 0.0\n",
      "The method with the highest Parameters score on the SNLI dataset for the Natural Language Inference task is not explicitly mentioned in the retrieved results. However, the Decomposable Attention Model is noted for achieving state-of-the-art results with fewer parameters compared to previous work. Further specific details on the highest Parameters score would require more targeted research or access to the latest benchmark results. | 300D_Residual_stacked_encoders => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [01:49<1:47:11, 109.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question_Answering task using accuracy as the primary metric. The performance is measured by the proportion of test cases where the ground truth is among the top answers proposed by the model. | CNN, Daily_Mail => 0.5\n",
      "MuZero achieves the highest Score score on the Atari_2600_Name_This_Game dataset for the Atari_Games task. | IQN => 0.0\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU. The method shows improved performance in table-to-text generation tasks compared to baseline models, particularly due to its dual attention mechanism. | BLEU, ROUGE => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [01:52<45:35, 47.16s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method is evaluated on the Bing_News dataset for Click-Through Rate Prediction using metrics such as the Area Under the Curve (AUC). | AUC, Log_Loss => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [01:53<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image Generation task are not explicitly mentioned in the retrieved documents. However, common metrics for evaluating image generation tasks include Inception Score and Fréchet Inception Distance (FID). | NLL_Test => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Evaluation Metrics Report\n",
      "==============================\n",
      "Total Execution Time: 247.90 seconds\n",
      "Average Time per Batch: 123.95 seconds\n",
      "Best Score: 0.258 (Batch 1)\n",
      "Total Tokens: 192,239 (3,284 in, 188,955 out)\n",
      "Total Cost: $11.4358\n",
      "\n",
      "Per-Batch Performance:\n",
      "--------------------\n",
      "\n",
      "Batch 1:\n",
      "  Score: 0.258\n",
      "  Execution Time: 124.19s\n",
      "  Tokens: 97,684 (1,642 in, 96,042 out)\n",
      "  Cost: $5.8118\n",
      "\n",
      "Batch 2:\n",
      "  Score: 0.250\n",
      "  Execution Time: 123.71s\n",
      "  Tokens: 94,555 (1,642 in, 92,913 out)\n",
      "  Cost: $5.6240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25833333333333336,\n",
       " {'batches': [{'batch_id': 1,\n",
       "    'score': 0.25833333333333336,\n",
       "    'execution_time': 124.19007873535156,\n",
       "    'tokens_in': 1642,\n",
       "    'tokens_out': 96042,\n",
       "    'cost': 5.81178},\n",
       "   {'batch_id': 2,\n",
       "    'score': 0.25,\n",
       "    'execution_time': 123.70830774307251,\n",
       "    'tokens_in': 1642,\n",
       "    'tokens_out': 92913,\n",
       "    'cost': 5.62404}],\n",
       "  'total_execution_time': 247.89838647842407,\n",
       "  'total_tokens_in': 3284,\n",
       "  'total_tokens_out': 188955,\n",
       "  'best_batch': 1,\n",
       "  'total_cost': 11.43582,\n",
       "  'final_score': 0.25833333333333336,\n",
       "  'average_batch_time': 123.94919323921204})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pure batch sampling\n",
    "\n",
    "from new_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "batch_sampling_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=0,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")\n",
    "result = batch_sampling_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")\n",
    "batch_num = 2\n",
    "batch_sampling_monkey.thread_safe_evaluator_batch(toolqa_test, result['agent'], batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "iterative_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=1,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method achieving the highest MRR score on the FB15k dataset for the Link Prediction task is AutoKGE with an MRR of 0.861. | TuckER => 0.0\n",
      "The big transformer model achieved the highest BLEU score of 28.4 on the WMT2014 English-German dataset for the Machine Translation task. | Weighted_Transformer__large_ => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The method that achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for Atari Games is LBC with a score of 10077.52%. | Ape-X => 0.0\n",
      "The Bi-LSTM trained on FCE method achieves the highest F0.5 score on the FCE dataset for the Grammatical Error Detection task, as indicated by the result from the paper by Masahiro Kaneko and Mamoru Komachi. | CoNLL-2014_A2 => 0.0\n",
      "The IQN method is evaluated on 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 87.0 F1, achieved by the span-based model presented in the paper \"A Span Selection Model for Semantic Role Labeling\" by Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. | Li_et_al_ => 0.0\n",
      "The method PTSR (Patch Translator for Image Super-Resolution) achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models. | PFF => 0.0\n",
      "The highest Validation_perplexity score on the Penn Treebank Word Level dataset for Language Modelling is achieved by OpenAI's GPT-3 with a word-level perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the retrieved information. The available papers discuss DCCL in the context of Generalized Category Discovery and Unsupervised Domain Adaptation, but not specifically for Machine Translation. | IWSLT2015_German-English => 0.0\n",
      "The A3C-CTS method is evaluated on the whole Atari 2600 suite, including Montezuma's Revenge and Bellemare et al.'s set of hard exploration games with sparse rewards. | Atari_2600_Venture => 0.0\n",
      "The Frustum_PointNets method is evaluated on the KITTI dataset for the Object_Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The method that achieves the highest Score score on the Atari_2600_Road_Runner dataset for the Atari_Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The Duel_hs method is evaluated on the Atari_2600_Video_Pinball dataset for the Atari_Games task. | Atari_2600_Video_Pinball => 1.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on the COLX 563 dataset, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using the NDCG@100 metric for the Collaborative Filtering task. | Recall_20, Recall_50 => 0.0\n",
      "The DDQN__tuned__noop method is evaluated on 57 Atari games. | Atari_2600_Berzerk => 0.0\n",
      "The IDE____CamStyle method is evaluated on the PRID2011, iLIDS-VID, and VIPeR datasets for the Person Re-Identification task. | DukeMTMC-reID => 0.0\n",
      "The BiDAF___Self_Attention__single_model_ method is evaluated on the SQuAD and CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n",
      "The datasets on which the Sample_Clustering method is evaluated for the Few-Shot Image Classification task are not explicitly mentioned in the available resources. Further specific research or access to the original paper or documentation of the Sample_Clustering method might be required to obtain this information. | CUB-200_-_0-Shot_Learning => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:56<17:58, 28.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Time_Pilot => 0.0\n",
      "The DeepFM method achieves the highest Log_Loss score for the Click-Through Rate Prediction task on the Criteo dataset. The Criteo dataset is a well-known ad tech industry benchmarking dataset used for evaluating CTR prediction models. | Criteo => 1.0\n",
      "The TARNet method is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both a randomized and a non-randomized component, for the Causal Inference task. | IDHP => 0.5\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric. | Percentage_error => 1.0\n",
      "CornerNet-Squeeze is evaluated on the COCO dataset for the Real-Time_Object_Detection task. | COCO => 1.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. | F-Measure => 0.5\n",
      "The Transformer method is evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using the BLEU metric. The evaluation reports tokenized BLEU using the \"multi-bleu.perl\" script. | BLEU_score => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  15%|█▌        | 6/40 [01:01<04:38,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Ms__Pacman => 0.0\n",
      "The PFF method for Image Super-Resolution is evaluated on the Set5 and Set14 datasets. | Set14_-_4x_upscaling => 0.5\n",
      "The method that achieves the highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is NAN with a score of 59.70%. | NAN => 1.0\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews across four domains, and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset for the Weakly Supervised Object Detection task using two main metrics: Average Precision (AP) at 50% intersection-over-union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth on the test set, while CorLoc measures the percentage of images with at least one correctly localized instance on the training and validation subsets. | MAP => 0.5\n",
      "The ResNet_ELU method is evaluated on the CIFAR-100 dataset using the test error percentage as the primary metric. The ELU networks achieved a test error of 24.28%, which is noted as one of the best published results on CIFAR-100 without using multi-view evaluation or model averaging. | Percentage_correct => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  18%|█▊        | 7/40 [01:04<03:54,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__hs method evaluated datasets for the Atari_Games task are not explicitly mentioned in the retrieved documents. It is likely evaluated on the standard set of 57 Atari games, as is common in reinforcement learning research involving Atari environments. | Atari_2600_Assault => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  28%|██▊       | 11/40 [01:04<01:35,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LapSRN method is evaluated on the Urban100 - 4x upscaling dataset using two evaluation metrics: peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). | PSNR => 0.5\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link_Prediction task using common evaluation metrics for link prediction tasks, such as AUC (Area Under the Curve) and possibly other metrics like precision, recall, or F1-score, although specific metrics for MTGAE were not found in the retrieved documents. | Accuracy => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using accuracy as the primary metric for the Natural Language Inference task. | Matched, Mismatched => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:08<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Subgraph_embeddings method evaluation metrics on the WebQuestions dataset for the Question_Answering task are not explicitly found in the available resources. It is likely that standard metrics such as accuracy, precision, recall, and F1-score are used, but specific details would require access to the original research paper or documentation on the Subgraph_embeddings method. | F1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics Report\n",
      "========================\n",
      "Execution Time: 75.46 seconds\n",
      "Total Tokens: 54,656 (1,090 in, 53,566 out)\n",
      "Total Cost: $0.5384\n",
      "Average Score: 0.263\n",
      "Average Score: 0.2625\n",
      "Evaluation Cost: $0.5384\n",
      "Generated new instruction: New Instruction: You will be given `Tools`, which is a list of resources to use to accomplish the `Goal`. When presented with a user query, your task is to decide which tool to use and what input values to provide. To enhance the effectiveness of your actions, begin by formulating specific and detailed queries. Break down complex queries into simpler components to improve the relevance and accuracy of the results. This will help in identifying the most appropriate tools to use for each part of the query.\n",
      "\n",
      "For each query, implement a strategy to combine multiple tools, especially for complex or broad questions. Start with a web search to gather initial information, then use retrieval or arXiv search for more detailed insights. This combination will allow you to gather comprehensive information and ensure a thorough understanding of the task. Remember, you can use one tool multiple times with different input queries if applicable, and you can also opt to use no tools and provide the final answer directly if the information is already clear.\n",
      "\n",
      "Introduce an iterative search process where initial results are verified and refined using additional tools or queries. This will help in cross-verifying information and ensuring the accuracy of the results. Establish a feedback loop where the results of each query are analyzed to identify areas of improvement in tool usage and query formulation. This continuous improvement process will help in refining your approach to handling negative inputs and enhance overall performance.\n",
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 90.93 seconds\n",
      "                Evaluation Time: 75.46 seconds\n",
      "                Total API Calls: 2\n",
      "                - Comparator calls: 1\n",
      "                - Feedback instruction calls: 1\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 29,034\n",
      "                - Input tokens: 28,280\n",
      "                - Output tokens: 754\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $0.8936\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "result = iterative_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization cost: $0.8936\n",
      "Final score achieved: 0.263\n",
      "\n",
      "Iteration 0:\n",
      "Score: 0.263\n",
      "Comparator tokens in: 27656\n",
      "Comparator tokens out: 479\n",
      "Feedback tokens in: 624\n",
      "Feedback tokens out: 275\n",
      "Execution time: 90.93s\n"
     ]
    }
   ],
   "source": [
    "optimized_actor_agent = result[\"agent\"]\n",
    "optimization_metrics = result[\"metrics\"]\n",
    "\n",
    "# Now you can process the metrics\n",
    "print(f\"Total optimization cost: ${optimization_metrics['total_cost']:.4f}\")\n",
    "print(f\"Final score achieved: {optimization_metrics['final_score']:.3f}\")\n",
    "\n",
    "# Analyze per-iteration performance\n",
    "for iteration in optimization_metrics['iteration_details']:\n",
    "    print(f\"\\nIteration {iteration['iteration']}:\")\n",
    "    print(f\"Score: {iteration['score']:.3f}\")\n",
    "    print(f\"Comparator tokens in: {iteration['comparator_metrics']['tokens_in']}\")\n",
    "    print(f\"Comparator tokens out: {iteration['comparator_metrics']['tokens_out']}\")\n",
    "    print(f\"Feedback tokens in: {iteration['feedback_metrics']['tokens_in']}\")\n",
    "    print(f\"Feedback tokens out: {iteration['feedback_metrics']['tokens_out']}\")\n",
    "    print(f\"Execution time: {iteration['total_iteration_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6.The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a score of 84.62. | PSPNet => 0.0\n",
      " | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The method that achieves the highest accuracy score on the Cora dataset for the node classification task is SSP with an accuracy of 90.16% ± 0.59%. | GCN => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The IDE + CamStyle + Random Erasing method for Person Re-Identification is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets. | Market-1501 => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the effectiveness of the image-to-image translation in terms of semantic segmentation performance. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The SRCNN method is evaluated on the Manga109 - 4x upscaling dataset using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics. | PSNR, SSIM => 1.0\n",
      "The Stacked Hourglass Networks method achieves the highest PCK@0.2 score on the FLIC dataset for the Pose Estimation task. | FLIC_Elbows => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [02:11<2:09:32, 131.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari 2600 Name This Game dataset for the Atari Games task. | IQN => 0.0\n",
      "The Snips method for Speech Recognition is evaluated on datasets such as the TIMIT Acoustic-Phonetic Continuous Speech Corpus and the Snips SmartLights dataset. These datasets are used to assess various aspects of speech recognition and spoken language understanding. | LibriSpeech_test-clean => 0.0\n",
      "The method achieving the highest error score on the Yelp Binary classification dataset for Sentiment Analysis is not explicitly mentioned in the available resources. However, the state-of-the-art performance on this dataset is achieved by a shallow-and-wide network with word inputs, which establishes a new state-of-the-art performance with an accuracy of 95.9%. | Char-level_CNN => 0.0\n",
      "The method achieving the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is not explicitly mentioned in the retrieved results. However, the SparseGPT model with 175 billion parameters is noted as a state-of-the-art model on WikiText-2, although specific 'Number_of_params' scores were not found. | AWD-LSTM-DOC => 0.0\n",
      "The Deep Speech method is evaluated on several datasets for the speech recognition task, including the TIMIT Acoustic-Phonetic Continuous Speech Corpus and LibriSpeech. These datasets are commonly used for evaluating automatic speech recognition systems. | Switchboard___Hub500 => 0.0\n",
      "The NICE method for image generation on the CIFAR-10 dataset is evaluated using the Inception Score (IS) metric, which measures the quality and diversity of generated images. | NLL_Test => 0.0\n",
      "The SRCNN method for Video Super-Resolution is commonly evaluated on datasets such as DIV2K and other publicly available benchmark datasets. However, specific datasets used for SRCNN in video super-resolution tasks are not consistently mentioned across sources. | Vid4_-_4x_upscaling => 0.0\n",
      "The MemNNs__ensemble_ method is evaluated on the SQuAD, CNN, Daily Mail, and CBT datasets for the Question Answering task. | CNN___Daily_Mail => 0.5\n",
      "The Paragraph Vector method has been evaluated on datasets such as SQuAD, HotpotQA, and TriviaQA for the Question Answering task. | WikiQA => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task are not explicitly mentioned in the available resources. However, common evaluation metrics for similar tasks include F1 score, Precision, and Recall. It is recommended to refer to specific papers or documentation related to Ann_PAT_MT for precise metrics. | F0_5 => 0.0\n",
      "The CRN method for Image-to-Image Translation is evaluated on datasets such as COCO-Stuff.The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB, Fisher, and CH datasets, and the N-gram + RNNLM language model trained on Switchboard, Fisher, Gigaword, and Broadcast, is evaluated on the Switchboard portion of the NIST 2000 evaluation set and the CallHome portion. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      " | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The SPyNet method for optical flow estimation is evaluated on several standard datasets, including the MPI Sintel dataset and the KITTI benchmarks. These datasets are commonly used for evaluating optical flow methods due to their challenging scenarios and ground truth data. | Sintel-final => 0.5\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) and Multi-Genre Natural Language Inference (MultiNLI) datasets for the Natural Language Inference task. | SNLI => 0.5\n",
      "The IQN method achieves high performance on the Atari 2600 games dataset, but specific highest scores for individual games are not detailed in the retrieved information. It is known for achieving high scores across a variety of games in the dataset. | Atari_2600_Atlantis => 0.0\n",
      "The specific evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task were not found in the available resources. Further detailed research or access to specific papers or datasets might be required to obtain this information. | Score => 0.0\n",
      "The Transformer method for machine translation has been evaluated on several datasets, including the WMT 2014 English-to-German and English-to-French translation tasks. These datasets are commonly used benchmarks in the field to assess the performance of machine translation models. | IWSLT2015_English-German => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task primarily using the metric of perplexity. This metric measures how well the model predicts a sample and is a common evaluation metric for language models. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task on the Atari 2600 Bowling game with a score of 68.1. | Atari_2600_Video_Pinball => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset, with AP scores of 0.965 for the easy subset, 0.955 for the medium subset, and 0.904 for the hard subset. | WIDER_Face__Easy_ => 1.0\n",
      "The FDNet method is evaluated on the WIDER Face Easy dataset using metrics such as Average Precision (AP). Specifically, FDNet achieves an AP of 95.9% on the Easy set of the WIDER Face validation dataset. | AP => 1.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D face reconstruction using metrics such as geometric error between reconstructed meshes and the ground truth, and NME (Normalized Mean Error) normalized by the face bounding box size. | Mean_NME_ => 1.0\n",
      "The method that achieves the highest score on the Atari 2600 Robotank dataset for the Atari Games task is MuZero. | Bootstrapped_DQN => 0.0\n",
      "The Impatient Reader method is evaluated on the CNN/Daily Mail dataset for the Question Answering task primarily using accuracy metrics. However, specific evaluation metrics for the Impatient Reader method were not explicitly found in the search results. The method's performance is often compared to other models like the Attentive Reader, with similar performance noted on the CNN and Daily Mail datasets. | CNN, Daily_Mail => 0.5\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score on the Sintel final pass dataset for the Optical Flow Estimation task. | Sintel-final => 1.0\n",
      "The DRCN method is evaluated on the Set5 dataset for 4x upscaling in the Image Super-Resolution task using metrics such as PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). | MOS, PSNR, SSIM => 0.67\n",
      "The DQN_noop method is evaluated on the Atari 2600 games. The evaluation typically involves testing the trained DQN agent on a variety of games within the Atari suite, which includes 60 different games. The performance is measured in terms of average scores achieved across these games, often using different evaluation protocols such as noop starts and human starts. | Atari_2600_River_Raid => 0.0\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset for the Image Classification task using top-1 and top-5 error rates. These metrics are standard for assessing the performance of image classification models on the ImageNet dataset. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The FRCN (Fast Region-based Convolutional Network) method is evaluated on several datasets for object detection tasks, including the PASCAL VOC and MS COCO datasets. | PASCAL_VOC_2007 => 0.5\n",
      "The VAT_EntMin method for semi-supervised image classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The DR-BiLSTM (Single) model achieves the highest reported train accuracy on the SNLI dataset for the Natural Language Inference task, outperforming other models including ensemble models when preprocessing is applied. | __Unigram_and_bigram_features => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using metrics such as the Percentage of Correct Keypoints (PCK) and Average Precision (AP). | Mean_PCK => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [03:02<1:21:25, 84.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method for Click-Through Rate Prediction on the Bing_News dataset is evaluated using metrics such as Area under ROC Curve (AUC) and Relative Information Gain (RIG). | AUC, Log_Loss => 0.5\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using metrics such as the proportion of correctly matched pixels, referred to as 'accuracy@'. This metric measures the proportion of correct pixel matches in the first image with respect to the total number of pixels, considering a pixel match correct if it is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The U-Net method for skin cancer segmentation has been evaluated on several datasets, including the ISIC 2016, ISIC 2017, and ISIC 2018 datasets. These datasets are commonly used in the field for benchmarking segmentation models. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The SVDCNN method for text classification is evaluated on several datasets, including AG's News, Sogou News, DBPedia, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The DeepLab-LargeFOV method is typically evaluated using metrics such as mean Intersection over Union (mIoU) and Pixel Accuracy on the SUN-RGBD dataset for the Scene Segmentation task. These metrics are commonly used in semantic segmentation to assess the performance of models. | Mean_IoU => 0.5\n",
      "The Bootstrapped DQN method is evaluated on the Atari 2600 games dataset, which is a standard benchmark for reinforcement learning algorithms. This dataset includes a variety of classic Atari games, and the Bootstrapped DQN has been shown to improve learning times and performance across most of these games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The DPN-131 method for image classification has been evaluated on datasets such as the RVL-CDIP and Places365-Standard. These datasets are used to assess the performance and generalization capabilities of the model in different image classification tasks. | ImageNet => 0.0\n",
      "The method achieving the highest MAE score on the BIWI dataset for the Head Pose Estimation task is the one proposed in the paper \"RankPose: Learning Generalised Feature with Rank Supervision for Head Pose Estimation,\" which improved the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The specific datasets on which the DQN_hs method is evaluated for the Atari Games task could not be identified from the available resources. It appears that detailed information about DQN_hs is not readily accessible through the current search methods. Further investigation or access to specific research papers or datasets might be required. | Atari_2600_Chopper_Command => 0.0\n",
      "The specific evaluation metrics for the Paragraph_vector__lexical_overlap___dist_output_ method on the QASent dataset for the Question Answering task were not found in the available resources. It is possible that this information is not publicly documented or is part of a specific research paper or dataset documentation that was not accessible through the tools used. | MAP, MRR => 0.0\n",
      "The current state-of-the-art method on the SNLI dataset for Natural Language Inference with the highest parameters is not explicitly mentioned in the available resources. However, Neural Tree Indexers for Text Understanding is noted as a state-of-the-art method. Further specific details on parameter counts for the highest scoring models are not readily available from the current search results. | 300D_Residual_stacked_encoders => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  13%|█▎        | 8/60 [03:15<13:37, 15.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information about the SVDCNN method achieving the highest error score for the Sentiment Analysis task on a specific dataset is not readily available from the current search results. Further detailed research or access to specific academic papers or datasets might be required to obtain this information. | Yelp_Fine-grained_classification => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [03:27<00:00,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, ROUGE, and PARENT. | BLEU, ROUGE => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics Report\n",
      "========================\n",
      "Execution Time: 209.63 seconds\n",
      "Total Tokens: 150,298 (1,642 in, 148,656 out)\n",
      "Total Cost: $1.4907\n",
      "Average Score: 0.286\n"
     ]
    }
   ],
   "source": [
    "score, one_shot_metrics = iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterative evolution summary:\n",
      "Score: 0.286\n",
      "Total Cost: $1.4907\n",
      "Total Time: 209.63s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nIterative evolution summary:\")\n",
    "print(f\"Score: {one_shot_metrics['average_score']:.3f}\")\n",
    "print(f\"Total Cost: ${one_shot_metrics['total_cost']:.4f}\")\n",
    "print(f\"Total Time: {one_shot_metrics['execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a score of 84.62. | PSPNet => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest accuracy score on the Cora dataset for the node classification task is SSP with an accuracy of 90.16% ± 0.59%. | GCN => 0.0\n",
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) and Multi-Genre Natural Language Inference (MultiNLI) datasets for the Natural Language Inference task. | SNLI => 0.5\n",
      "The method achieving the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is not explicitly mentioned in the retrieved results. However, the SparseGPT model with 175 billion parameters is noted as a state-of-the-art model on WikiText-2, although specific 'Number_of_params' scores were not found. | AWD-LSTM-DOC => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n",
      "The IQN method achieves high performance on the Atari 2600 games dataset, but specific highest scores for individual games are not detailed in the retrieved information. It is known for achieving high scores across a variety of games in the dataset. | Atari_2600_Atlantis => 0.0\n",
      "The method achieving the highest MAE score on the BIWI dataset for the Head Pose Estimation task is the one proposed in the paper \"RankPose: Learning Generalised Feature with Rank Supervision for Head Pose Estimation,\" which improved the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [02:12<2:10:46, 133.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari 2600 Name This Game dataset for the Atari Games task. | IQN => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Stacked Hourglass Networks method achieves the highest PCK@0.2 score on the FLIC dataset for the Pose Estimation task. | FLIC_Elbows => 0.5\n",
      "The U-Net method for skin cancer segmentation is commonly evaluated on datasets such as the ISIC 2016, ISIC 2017, and ISIC 2018 datasets. These datasets are part of the International Skin Imaging Collaboration (ISIC) challenges, which provide a large number of dermatoscopic images for training and testing segmentation models. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU and ROUGE. Additionally, a new metric called PARENT is proposed, which aligns n-grams from the reference and generated texts to the semi-structured data before computing their precision and recall, showing better correlation with human judgments. | BLEU, ROUGE => 0.5\n",
      "The specific evaluation metrics for the Ann_PAT_MT method on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task could not be found in the available resources. It is possible that the method is not widely documented or the information is not publicly accessible. | F0_5 => 0.0\n",
      "The specific datasets on which the DQN_hs method is evaluated for the Atari Games task could not be identified from the available resources. Further detailed research or access to specific publications or datasets might be required to obtain this information. | Atari_2600_Chopper_Command => 0.0\n",
      "The NICE method is evaluated on the CIFAR-10 dataset for the Image Generation task using the Inception Score (IS), which measures the quality and diversity of generated images. | NLL_Test => 0.0\n",
      "The DeepLab-LargeFOV method is typically evaluated on the SUN-RGBD dataset for scene segmentation using metrics such as Intersection over Union (IoU) and Pixel Accuracy. These metrics are standard for assessing the performance of semantic segmentation models. | Mean_IoU => 0.5\n",
      "The DRCN method is evaluated on the Set5 4x upscaling dataset for Image Super-Resolution using metrics such as PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). | MOS, PSNR, SSIM => 0.67\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset for the Image Classification task using top-1 and top-5 error rates. These metrics are standard for assessing the performance of image classification models on the ImageNet dataset. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The IDE + CamStyle + Random Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The DR-BiLSTM (Single) model achieves the highest reported Train Accuracy on the SNLI dataset for the Natural Language Inference task, outperforming both single and ensemble models with preprocessing steps. | __Unigram_and_bigram_features => 0.0\n",
      "The SRCNN method is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) for the Image Super-Resolution task on datasets like Manga109 with 4x upscaling. | PSNR, SSIM => 1.0\n",
      "The DQN_noop method is evaluated on the Atari 2600 games using two types of evaluations: null op starts and human starts. The evaluation involves 30 episodes per game with random initial conditions for null op starts and 100 random starting points from human gameplay for human starts. The method is tested across 57 Atari games. | Atari_2600_River_Raid => 0.0\n",
      "The SRCNN method for video super-resolution is evaluated on datasets such as Vid4 and REDS4. These datasets are commonly used benchmarks in the field of video super-resolution. | Vid4_-_4x_upscaling => 0.5\n",
      "The DPN-131 method for image classification has been evaluated on datasets such as the RVL-CDIP and Places365-Standard. These datasets are used to assess the performance and generalization capabilities of the model in different image classification tasks. | ImageNet => 0.0\n",
      "The SPyNet method for Optical Flow Estimation is evaluated on standard optical flow benchmarks, including the MPI Sintel dataset and the KITTI dataset. | Sintel-final => 0.5\n",
      "The Deep Speech method for speech recognition is commonly evaluated on datasets such as the TIMIT Acoustic-Phonetic Continuous Speech Corpus and the LibriSpeech dataset. | Switchboard___Hub500 => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass dataset. | Sintel-final => 1.0\n",
      "The VAT_EntMin method for semi-supervised image classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The method achieving the highest error score on the Yelp Binary classification dataset for Sentiment Analysis is not explicitly mentioned in the available resources. However, the state-of-the-art performance is often associated with deep learning models such as XLNet and BERT, which are known for their high accuracy rather than error scores. Further specific research might be needed to identify the method with the highest error score. | Char-level_CNN => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task primarily using the metric of perplexity. This metric measures how well the model predicts a sample and is a common evaluation metric for language models. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The Bootstrapped DQN method is evaluated on the Atari 2600 games dataset, which is part of the Arcade Learning Environment. This dataset includes a wide range of classic Atari games used to benchmark reinforcement learning algorithms. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The SVDCNN method for text classification is evaluated on several datasets, including those for sentiment analysis, news classification, question-answering, and ontology extraction tasks. Specific datasets mentioned include ICDAR 2015 Incidental Text, MSRA-TD500, and ICDAR 2013. | AG_News => 0.0\n",
      "The FDNet method is evaluated on the WIDER Face Easy dataset using metrics such as Average Precision (AP). Specifically, FDNet achieves an AP of 95.9% on the Easy set of the WIDER Face validation dataset. | AP => 1.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset using metrics such as accuracy and performance comparison with other models. However, specific evaluation metrics like F1 score or exact match are not explicitly mentioned in the retrieved documents. | CNN, Daily_Mail => 0.5\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using the Percentage of Correct Keypoints (PCK) metric. This metric reports the percentage of detections that fall within a normalized distance from the ground truth keypoints. | Mean_PCK => 1.0\n",
      "The CRN method for Image-to-Image Translation is evaluated on datasets such as COCO-Stuff. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The Transformer method for machine translation is commonly evaluated on datasets such as WMT 2014 English-to-German and English-to-French translation tasks. These datasets are widely used benchmarks in the field to assess the performance of translation models. | IWSLT2015_English-German => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using metrics such as geometric error between reconstructed meshes and the ground truth. This involves calculating the Normalized Mean Error (NME) normalized by the face bounding box size, and using the Iterative Closest Points (ICP) algorithm to find corresponding nearest points between the reconstructed 3D face and the ground truth point cloud. | Mean_NME_ => 1.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is often referred to as 'accuracy@', where a pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The Snips method for speech recognition is evaluated on datasets such as the TIMIT Acoustic-Phonetic Continuous Speech Corpus and the English CTS (Switchboard and Fisher) corpora. | LibriSpeech_test-clean => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset for the Face Detection task. | WIDER_Face__Easy_ => 0.0\n",
      "The Paragraph Vector method for the Question Answering task has been evaluated on datasets such as SQuAD, HotpotQA, and TriviaQA. These datasets are commonly used for benchmarking question answering systems and involve tasks that require reasoning over multiple paragraphs. | WikiQA => 0.0\n",
      "The DDQN__tuned__noop method achieves the highest Score score on the Atari 2600 Breakout dataset for the Atari Games task. | Atari_2600_Video_Pinball => 0.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB, Fisher, and CH datasets is evaluated on the Switchboard portion of the NIST 2000 evaluation set and the CallHome portion. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The FRCN (Fast Region-based Convolutional Network) method is commonly evaluated on datasets such as PASCAL VOC and MS COCO for object detection tasks. | PASCAL_VOC_2007 => 0.5\n",
      "The current state-of-the-art method achieving the highest score on the Atari 2600 Robotank dataset for the Atari Games task is MuZero. | Bootstrapped_DQN => 0.0\n",
      "The MemNNs__ensemble_ method is evaluated on several datasets for the Question Answering task, including the Stanford Question Answering Dataset (SQuAD), CNN, Daily Mail, and CBT datasets. | CNN___Daily_Mail => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [03:15<1:28:14, 91.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method for Click-Through Rate Prediction on the Bing_News dataset is evaluated using metrics such as Area under ROC Curve (AUC) and Relative Information Gain (RIG). | AUC, Log_Loss => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA Fall-to-Winter dataset for the Image-to-Image Translation task using three metrics: mean intersection-over-union (mIoU), frequency weighted intersection-over-union (fwIoU), and pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is typically evaluated on metrics like exact match (EM) and F1 score for question answering tasks, as these are common metrics used in the field. However, specific metrics for the QASent dataset were not found in the search results. | MAP, MRR => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  13%|█▎        | 8/60 [03:23<14:03, 16.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task are not explicitly found in the available resources. It is likely that the method is evaluated using common metrics for Atari games, such as mean and median human-normalized scores, mean rank, and Elo scores, but specific details for Prior_Duel_hs are not available.The information about the SVDCNN method achieving the highest error score on a specific dataset for the Sentiment Analysis task is not readily available from the current search results. It seems that the SVDCNN method's performance details, particularly regarding error scores on specific datasets, are not well-documented or accessible through the tools used. Further investigation or access to specific research papers or datasets might be required to obtain this information. | Yelp_Fine-grained_classification => 0.0\n",
      " | Score => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [03:31<00:00,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current state-of-the-art model for the SNLI dataset in terms of parameters is not clearly identified in the available resources. The search results frequently mention Neural Tree Indexers for Text Understanding as a state-of-the-art model, but specific details about the highest parameters score are not provided. Further research or access to specific academic papers or databases might be required to obtain this information. | 300D_Residual_stacked_encoders => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 of 2...\n",
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The method that achieves the highest accuracy score on the Cora dataset for the node classification task is SSP with an accuracy of 90.16% ± 0.59%. | GCN => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a score of 84.62. | PSPNet => 0.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The Stacked Hourglass Networks method achieves the highest PCK@0.2 score on the FLIC dataset for the Pose Estimation task. | FLIC_Elbows => 0.5\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass dataset. | Sintel-final => 1.0\n",
      "The U-Net method for skin cancer segmentation has been evaluated on several datasets, including the ISIC 2016, ISIC 2017, and ISIC 2018 datasets. These datasets are commonly used in the field for benchmarking segmentation models. | Kaggle_Skin_Lesion_Segmentation => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [02:13<2:11:40, 133.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spynet method for Optical Flow Estimation is evaluated on datasets such as Flying Chairs and MPI-Sintel. | Sintel-final => 0.5\n",
      "MuZero achieves the highest score on the Atari 2600 Name This Game dataset for the Atari Games task. | IQN => 0.0\n",
      "The method achieving the highest MAE score on the BIWI dataset for the Head Pose Estimation task is the one proposed in the paper \"RankPose: Learning Generalised Feature with Rank Supervision for Head Pose Estimation,\" which improved the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The method that achieves the highest score on the Atari 2600 Robotank dataset for the Atari Games task is MuZero. | Bootstrapped_DQN => 0.0\n",
      "The NICE method is evaluated on the CIFAR-10 dataset for the Image Generation task using the Inception Score (IS) metric. This metric assesses the quality and diversity of the generated images. | NLL_Test => 0.0\n",
      "The SRCNN method for video super-resolution is commonly evaluated on datasets such as Vid4 and REDS. These datasets are used to test the performance of super-resolution algorithms in enhancing the resolution of video sequences. | Vid4_-_4x_upscaling => 0.5\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset using metrics such as accuracy and performance comparison with other models. However, specific evaluation metrics like F1 score, precision, or recall are not explicitly mentioned in the retrieved documents. | CNN, Daily_Mail => 0.5\n",
      "The DeepLab-LargeFOV method is typically evaluated on the SUN-RGBD dataset for the Scene Segmentation task using metrics such as mean Intersection over Union (mIoU) and Pixel Accuracy. These metrics are commonly used in semantic segmentation tasks to assess the performance of models. | Mean_IoU => 0.5\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB+Fisher+CH and the N-gram + RNNLM language model trained on Switchboard+Fisher+Gigaword+Broadcast are evaluated on the Switchboard portion of the NIST 2000 evaluation set and the CallHome portion. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The SRCNN method is evaluated on the Manga109 - 4x upscaling dataset using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics. | PSNR, SSIM => 1.0\n",
      "The MemNNs__ensemble_ method is evaluated on the CNN and Daily Mail datasets for the Question Answering task. | CNN___Daily_Mail => 1.0\n",
      "The DR-BiLSTM (Single) model achieves the highest reported Train Accuracy on the SNLI dataset for the Natural Language Inference task, outperforming both single and ensemble models with preprocessing steps. | __Unigram_and_bigram_features => 0.0\n",
      "The VAT_EntMin method for semi-supervised image classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) and Multi-Genre Natural Language Inference (MultiNLI) datasets for the Natural Language Inference task. | SNLI => 0.5\n",
      "The Bootstrapped DQN method is evaluated on the Atari 2600 games dataset, which is part of the Arcade Learning Environment. This dataset includes a wide range of classic Atari games used to benchmark reinforcement learning algorithms. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The IQN method achieves high performance on the 57 Atari 2600 games dataset, as indicated by its improved performance and state-of-the-art training results in risk-sensitive policies for Atari games. | Atari_2600_Atlantis => 0.0\n",
      "The method achieving the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is not explicitly mentioned in the retrieved results. However, the SparseGPT model with 175 billion parameters is noted as a state-of-the-art model on WikiText-2, although specific 'Number_of_params' scores were not found. | AWD-LSTM-DOC => 0.0\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using the Average Viewpoint Precision (AVP) metric. | Mean_PCK => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using metrics such as geometric error between reconstructed meshes and the ground truth. This involves calculating the Normalized Mean Error (NME) normalized by the face bounding box size, and using the Iterative Closest Points (ICP) algorithm to find corresponding nearest points between the reconstructed 3D face and the ground truth point cloud. | Mean_NME_ => 1.0\n",
      "The IDE + CamStyle + Random Erasing method for Person Re-Identification is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets. | Market-1501 => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The method achieving the highest error score on the Yelp Binary classification dataset for Sentiment Analysis is not explicitly mentioned in the available resources. However, the state-of-the-art performance on this dataset is achieved by a shallow word model with a 95.9% accuracy, as per the arXiv search results. | Char-level_CNN => 0.0\n",
      "Inception_V2 is evaluated on the ImageNet dataset using top-1 and top-5 error rates as metrics for the Image Classification task. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The Paragraph Vector method for the Question Answering task has been evaluated on datasets such as SQuAD, HotpotQA, and TriviaQA. These datasets are commonly used for benchmarking question answering systems and involve tasks that require reasoning over multiple paragraphs. | WikiQA => 0.0\n",
      "The Transformer method for machine translation has been evaluated on datasets such as the WMT 2014 English-to-German and English-to-French translation tasks.The specific evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task were not found in the available resources. Further detailed research or access to specific papers or datasets might be required to obtain this information. | Score => 0.0\n",
      " | IWSLT2015_English-German => 0.0\n",
      "The Snips method for speech recognition is evaluated on datasets such as the Snips SmartLights dataset and the SNIPS Audio dataset. These datasets are used to assess the performance of speech recognition and spoken language understanding systems. | LibriSpeech_test-clean => 0.0\n",
      "The CRN method for Image-to-Image Translation is evaluated on datasets such as COCO-Stuff and ADE20K. | ADE20K-Outdoor_Labels-to-Photos => 0.5\n",
      "The specific evaluation metrics for the Ann_PAT_MT method on the CoNLL-2014_A2 dataset for the Grammatical Error Detection task could not be found. It is possible that the method is not widely documented or the information is not readily available in public resources. | F0_5 => 0.0\n",
      "The Deep Speech method for speech recognition is evaluated on datasets such as TIMIT, which is a standard dataset used for evaluating automatic speech recognition systems. It consists of recordings of speakers reading phonetically-rich sentences and is commonly used for rapid evaluation and comparison of different system settings. | Switchboard___Hub500 => 0.0\n",
      "The DQN_noop method is evaluated on the Atari 2600 games using two types of evaluations: null op starts and human starts. The evaluation involves 30 episodes per game with random initial conditions for null op starts and 100 random starting points from human gameplay for human starts. The method is tested across 57 Atari games. | Atari_2600_River_Raid => 0.0\n",
      "The SVDCNN method for text classification is evaluated on several datasets, including those for sentiment analysis, news classification, question-answering, and ontology extraction tasks. Specific datasets mentioned include ICDAR 2015 Incidental Text, MSRA-TD500, and ICDAR 2013. | AG_News => 0.0\n",
      "The DPN-131 method for image classification has been evaluated on datasets such as the RVL-CDIP and Places365-Standard. These datasets are used to assess the performance and generalization capabilities of the model in different image classification tasks. | ImageNet => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 3/60 [02:57<41:48, 44.01s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method for Click-Through Rate Prediction on the Bing_News dataset is evaluated using metrics such as Area under ROC Curve (AUC) and Relative Information Gain (RIG). | AUC, Log_Loss => 0.5\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task primarily using the metric of perplexity. Dynamic evaluation is used to improve the state-of-the-art perplexity on this dataset. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The method EASE achieves the highest Recall_50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the effectiveness of the image-to-image translation in terms of semantic segmentation performance. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The FDNet method is evaluated on the WIDER Face Easy dataset using metrics such as Average Precision (AP). FDNet1.0 achieves an AP of 95.9% on the Easy set of the WIDER Face validation dataset. | AP => 1.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is often referred to as 'accuracy@', where a pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The FRCN (Fast Region-based Convolutional Network) method is commonly evaluated on datasets such as PASCAL VOC and MS COCO for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  13%|█▎        | 8/60 [03:02<09:32, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information about the SVDCNN method achieving the highest error score on a specific dataset for the Sentiment Analysis task is not available in the current search results. It seems that the specific error score or dataset for SVDCNN in this context is not well-documented or publicly accessible. | Yelp_Fine-grained_classification => 0.0\n",
      "The DRCN method is evaluated on the Set5 - 4x upscaling dataset for Image Super-Resolution using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | MOS, PSNR, SSIM => 0.67\n",
      "The DDQN__tuned__noop method achieves the highest Score score on the Atari 2600 Breakout game with a score of 418.5. | Atari_2600_Video_Pinball => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  18%|█▊        | 11/60 [03:04<05:37,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific datasets on which the DQN_hs method is evaluated for the Atari Games task could not be identified from the available resources. It appears that detailed information about DQN_hs is not readily accessible or may not be publicly documented. Further investigation or access to specific research papers or datasets might be required to obtain this information. | Atari_2600_Chopper_Command => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is typically evaluated on metrics such as exact match (EM) and F1 score for question answering tasks. However, specific evaluation metrics for this method on the QASent dataset were not found in the available resources. | MAP, MRR => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  33%|███▎      | 20/60 [03:12<02:04,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset, with specific AP scores of 0.965 for the easy set, 0.955 for the medium set, and 0.904 for the hard set. | WIDER_Face__Easy_ => 1.0\n",
      "The current state-of-the-art model for the SNLI dataset with the highest parameters score is not clearly identified in the available resources. The Neural Tree Indexers for Text Understanding is mentioned as a state-of-the-art model, but specific parameter scores are not provided. Further research or direct access to specific academic papers or repositories may be required to obtain this information. | 300D_Residual_stacked_encoders => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [03:15<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU, ROUGE, and PARENT. | BLEU, ROUGE => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Evaluation Metrics Report\n",
      "==============================\n",
      "Total Execution Time: 426.21 seconds\n",
      "Average Time per Batch: 213.11 seconds\n",
      "Best Score: 0.303 (Batch 2)\n",
      "Total Tokens: 285,748 (3,284 in, 282,464 out)\n",
      "Total Cost: $17.0464\n",
      "\n",
      "Per-Batch Performance:\n",
      "--------------------\n",
      "\n",
      "Batch 1:\n",
      "  Score: 0.286\n",
      "  Execution Time: 221.52s\n",
      "  Tokens: 145,218 (1,642 in, 143,576 out)\n",
      "  Cost: $8.6638\n",
      "\n",
      "Batch 2:\n",
      "  Score: 0.303\n",
      "  Execution Time: 204.70s\n",
      "  Tokens: 140,530 (1,642 in, 138,888 out)\n",
      "  Cost: $8.3825\n"
     ]
    }
   ],
   "source": [
    "# iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)\n",
    "batch_num = 2\n",
    "score, batch_metrics = iterative_monkey.thread_safe_evaluator_batch(\n",
    "    toolqa_test, \n",
    "    optimized_actor_agent,\n",
    "    batch_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed strategy summary:\n",
      "Best Score: 0.303\n",
      "Total Cost: $17.0464\n",
      "Total Time: 426.21s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMixed strategy summary:\")\n",
    "print(f\"Best Score: {batch_metrics['final_score']:.3f}\")\n",
    "print(f\"Total Cost: ${batch_metrics['total_cost']:.4f}\")\n",
    "print(f\"Total Time: {batch_metrics['total_execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
