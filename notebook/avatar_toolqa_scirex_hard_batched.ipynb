{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SERPER_API_KEY=6ec9fbfc4303aeb8ec8cdf254dcd7d2d2fa38915\n",
      "env: OPENAI_API_KEY=sk-proj-lBVKono3F0ltDmAx6EqlT3BlbkFJqvuHPTGtDsur1A40vq9v\n"
     ]
    }
   ],
   "source": [
    "%env SERPER_API_KEY=6ec9fbfc4303aeb8ec8cdf254dcd7d2d2fa38915\n",
    "%env OPENAI_API_KEY=sk-proj-lBVKono3F0ltDmAx6EqlT3BlbkFJqvuHPTGtDsur1A40vq9v\n",
    "HF_USR_NAME = 'shirwu'\n",
    "TOOL_QA_ROOT = '/dfs/project/kgrlm/shirwu/msr_intern/home/t-yingxinwu/msr_intern/ToolQA-rebuttal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'hard'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "# dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning) \n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    # use cpu\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device='cpu')\n",
    "    # model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    "    max_iters=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Disable all INFO logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# Silence all loggers that might be chatty\n",
    "loggers_to_silence = [\n",
    "    \"httpx\",\n",
    "    \"httpcore\",\n",
    "    \"openai\",\n",
    "    \"arxiv\",\n",
    "    \"dspy\",\n",
    "    \"langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"requests\",\n",
    "    \"urllib3\",\n",
    "    \"tiktoken\",\n",
    "    \"asyncio\",\n",
    "    \"faiss\",\n",
    "    \"anthropic\"\n",
    "]\n",
    "\n",
    "for logger_name in loggers_to_silence:\n",
    "    logging.getLogger(logger_name).setLevel(logging.WARNING)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable tokenizer parallelism warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'Which method achieves the highest PCK score on Leeds_Sports_Poses dataset for Pose_Estimation task?', 'answer': 'Pyramid_Residual_Modules__PRMs_'}) (input_keys={'paper_id', 'question'})\n",
      "physics | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class APICallMetrics:\n",
    "    timestamp: datetime\n",
    "    tool_name: str\n",
    "    tokens_in: int = 0\n",
    "    tokens_out: int = 0\n",
    "    execution_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class AvatarMetrics:\n",
    "    total_calls: int = 0\n",
    "    total_tokens_in: int = 0\n",
    "    total_tokens_out: int = 0\n",
    "    total_execution_time: float = 0.0\n",
    "    calls_by_tool: Dict[str, int] = field(default_factory=dict)\n",
    "    api_call_history: List[APICallMetrics] = field(default_factory=list)\n",
    "    \n",
    "    def add_call(self, metrics: APICallMetrics):\n",
    "        self.total_calls += 1\n",
    "        self.total_tokens_in += metrics.tokens_in\n",
    "        self.total_tokens_out += metrics.tokens_out\n",
    "        self.total_execution_time += metrics.execution_time\n",
    "        self.calls_by_tool[metrics.tool_name] = self.calls_by_tool.get(metrics.tool_name, 0) + 1\n",
    "        self.api_call_history.append(metrics)\n",
    "    \n",
    "    def merge(self, other: 'AvatarMetrics'):\n",
    "        \"\"\"Merge another AvatarMetrics instance into this one\"\"\"\n",
    "        self.total_calls += other.total_calls\n",
    "        self.total_tokens_in += other.total_tokens_in\n",
    "        self.total_tokens_out += other.total_tokens_out\n",
    "        self.total_execution_time += other.total_execution_time\n",
    "        for tool, count in other.calls_by_tool.items():\n",
    "            self.calls_by_tool[tool] = self.calls_by_tool.get(tool, 0) + count\n",
    "        self.api_call_history.extend(other.api_call_history)\n",
    "\n",
    "    def estimate_cost(self, model_name: str = \"gpt-4\") -> float:\n",
    "        pricing = {\n",
    "            \"gpt-4\": {\"input\": 2.5, \"output\": 10.0},\n",
    "        }\n",
    "        if model_name not in pricing:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        rates = pricing[model_name]\n",
    "        input_cost = (self.total_tokens_in / 1000000) * rates[\"input\"]\n",
    "        output_cost = (self.total_tokens_out / 1000000) * rates[\"output\"]\n",
    "        return input_cost + output_cost\n",
    "\n",
    "class AvatarWithMetrics(Avatar):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.metrics = AvatarMetrics()\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        try:\n",
    "            return len(self.tokenizer.encode(str(text)))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error counting tokens: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def _wrapped_tool_call(self, tool, input_text: str) -> str:\n",
    "        start_time = time.time()\n",
    "        tokens_in = self._count_tokens(input_text)\n",
    "        \n",
    "        try:\n",
    "            result = tool.run(input_text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Tool execution error: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            execution_time = time.time() - start_time\n",
    "            tokens_out = self._count_tokens(str(result))\n",
    "            \n",
    "            metrics = APICallMetrics(\n",
    "                timestamp=datetime.now(),\n",
    "                tool_name=tool.name,\n",
    "                tokens_in=tokens_in,\n",
    "                tokens_out=tokens_out,\n",
    "                execution_time=execution_time\n",
    "            )\n",
    "            self.metrics.add_call(metrics)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = super().__call__(*args, **kwargs)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        metrics = APICallMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            tool_name=\"main_llm\",\n",
    "            tokens_in=self._count_tokens(str(args) + str(kwargs)),\n",
    "            tokens_out=self._count_tokens(str(result)),\n",
    "            execution_time=total_time\n",
    "        )\n",
    "        self.metrics.add_call(metrics)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for example in test_set:\n",
    "            def process_with_metrics(example=example):\n",
    "                try:\n",
    "                    avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "                    prediction = avatar(**example.inputs().toDict())\n",
    "                    return metric(example, prediction), avatar.metrics\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    return 0, AvatarMetrics()\n",
    "\n",
    "            futures.append(executor.submit(process_with_metrics))\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            score, metrics = future.result()\n",
    "            total_score += score\n",
    "            # Only combine token counts and call counts, not execution times\n",
    "            combined_metrics.total_calls += metrics.total_calls\n",
    "            combined_metrics.total_tokens_in += metrics.total_tokens_in\n",
    "            combined_metrics.total_tokens_out += metrics.total_tokens_out\n",
    "            for tool, count in metrics.calls_by_tool.items():\n",
    "                combined_metrics.calls_by_tool[tool] = combined_metrics.calls_by_tool.get(tool, 0) + count\n",
    "            combined_metrics.api_call_history.extend(metrics.api_call_history)\n",
    "    \n",
    "    total_execution_time = time.time() - start_time\n",
    "    combined_metrics.total_execution_time = total_execution_time\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "    combined_metrics = AvatarMetrics()\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        try:\n",
    "            avatar = AvatarWithMetrics(signature, tools=tools, verbose=False, max_iters=10)\n",
    "            prediction = avatar(**example.inputs().toDict())\n",
    "            score = metric(example, prediction)\n",
    "            total_score += score\n",
    "            # Combine metrics from this run\n",
    "            for call in avatar.metrics.api_call_history:\n",
    "                combined_metrics.add_call(call)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric, combined_metrics\n",
    "\n",
    "def format_metrics_report(metrics: AvatarMetrics, model_name: str = \"gpt-4\") -> str:\n",
    "    cost = metrics.estimate_cost(model_name)\n",
    "    \n",
    "    report = f\"\"\"\n",
    "Avatar Execution Metrics Report\n",
    "==============================\n",
    "Execution Time: {metrics.total_execution_time:.2f} seconds\n",
    "Total API Calls: {metrics.total_calls}\n",
    "Total Tokens: {metrics.total_tokens_in + metrics.total_tokens_out:,} ({metrics.total_tokens_in:,} in, {metrics.total_tokens_out:,} out)\n",
    "Estimated Cost: ${cost:.4f}\n",
    "\n",
    "Average Time per Call: {metrics.total_execution_time / metrics.total_calls:.2f} seconds\n",
    "\n",
    "Tool Usage Breakdown:\n",
    "-------------------\n",
    "\"\"\"\n",
    "    for tool, count in sorted(metrics.calls_by_tool.items()):\n",
    "        report += f\"{tool}: {count} calls\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-shot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Score score on the Atari_2600_Name_This_Game dataset for the Atari_Games task is MuZero. | IQN => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:10<10:38, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest Train Accuracy score on the SNLI dataset for the Natural Language Inference task is achieved by the Neural Tree Indexers for Text Understanding and EFL (Entailment as Few-shot Learner) models, both with a Test Accuracy of 93.1%. | __Unigram_and_bigram_features => 0.0\n",
      "The method that achieves the highest F1 score on the CoNLL 2003 English dataset for Named Entity Recognition (NER) is ACE + document-context with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for the Image Classification task. | Res2NeXt-29 => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score for the Face Detection task on the WiderFace dataset. | WIDER_Face__Easy_ => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on several datasets, including the ISIC-2018 dataset and the HAM10000 dataset. | Kaggle_Skin_Lesion_Segmentation => 0.0\n",
      "The method EASE achieves the highest Recall@50 score of 0.428 on the Million Song Dataset for the Collaborative Filtering task. | Mult-VAE_PR => 0.0\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | CIFAR-10 => 0.0\n",
      "The Paragraph_vector method for the Question Answering task has been evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | WikiQA => 0.5\n",
      "The Discriminative Unsupervised Feature Learning with Convolutional Neural Networks method is evaluated on the STL-10 dataset for the Image Classification task. | STL-10 => 1.0\n",
      "The method that achieves the highest MAP score on the WikiQA dataset for the Question Answering task is TANDA, which achieved a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n",
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The DQN_hs method is evaluated on the Atari 2600 games, as mentioned in the context of various research papers discussing reinforcement learning methods applied to Atari games. | Atari_2600_Chopper_Command => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracies on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "LiteFlowNet achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass and KITTI benchmarks. | Sintel-final => 0.5\n",
      "The method that achieves the highest error score on the Yelp Binary classification dataset for the Sentiment Analysis task is not explicitly mentioned in the available resources. However, the shallow-and-wide network model has established new state-of-the-art performances on the Yelp Binary dataset with an accuracy of 95.9%. | Char-level_CNN => 0.0\n",
      "The search did not yield specific datasets for the Deep_Speech method evaluation in Speech Recognition. Further detailed search or specific papers might be needed to find this information. | Switchboard___Hub500 => 0.0\n",
      "The evaluation metrics for the Prior_Duel_hs method on the Atari_2600_Alien dataset for the Atari_Games task could not be found using the available tools. It is possible that this specific information is not publicly available or documented in the sources searched. | Score => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The Bootstrapped DQN method is evaluated on the Atari benchmark, which includes a variety of Atari 2600 games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "The method achieving the highest Mean_IoU score on the CamVid dataset for Semantic Segmentation is Border-SegGCN, with a score of 81.96%. | PSPNet => 0.0\n",
      "MuZero achieves the highest Score score of 131.13 on the Atari_2600_Robotank dataset for the Atari_Games task. | Bootstrapped_DQN => 0.0\n",
      "The current state-of-the-art on the SNLI dataset for Natural Language Inference is Neural Tree Indexers for Text Understanding. However, specific details about the highest Parameters score are not readily available from the search results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "The IDE_CamStyle_Random_Erasing method is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets for the Person Re-Identification task. | Market-1501 => 0.5\n",
      "The MemNNs__ensemble_ method for the Question_Answering task is evaluated on the CNN, Daily Mail, and CBT CN and NE datasets. | CNN___Daily_Mail => 0.5\n",
      "The Snips method is evaluated on the TIMIT Acoustic-Phonetic Continuous Speech Corpus and the English CTS (Switchboard and Fisher) corpora for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The Ann_PAT_MT method evaluation metrics on the CoNLL-2014_A2 dataset for Grammatical Error Detection are not explicitly mentioned in the retrieved documents. However, the CoNLL-2014 shared task generally uses metrics like precision, recall, and F0.5 score, which weights precision twice as much as recall, for evaluating grammatical error detection systems. | F0_5 => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset using top-1 and top-5 error rates on the validation set. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as Set5 and SuperTexture, as mentioned in the retrieved results. | Vid4_-_4x_upscaling => 0.0\n",
      "The method with the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task is the byte-level mLSTM model, which contains 46 million parameters. | AWD-LSTM-DOC => 0.0\n",
      "The FRCN method is evaluated on the VOC and MS COCO datasets for the Object Detection task. | PASCAL_VOC_2007 => 0.5\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model trained on SWB+Fisher+CH is evaluated on the Switchboard and CallHome portions of the NIST 2000 evaluation set for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for the Image-to-Image Translation task using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. These metrics assess the performance of the model in terms of semantic segmentation and pixel-level adaptation. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The IQN method achieves the highest Score score on the Atari 2600 Pong dataset, reaching a perfect score of 21. | Atari_2600_Atlantis => 0.0\n",
      "The available searches did not provide specific information about the dataset on which the SVDCNN method achieves the highest Error score for the Sentiment Analysis task. Further detailed research or access to specific academic papers or datasets might be required to find this information. | Yelp_Fine-grained_classification => 0.0\n",
      "The Transformer method is evaluated on the WMT 2014 English-to-German and English-to-French translation tasks for the Machine Translation task. | IWSLT2015_English-German => 0.0\n",
      "The FDNet method is evaluated on the WIDER_Face Easy dataset for the Face Detection task using metrics such as accuracy on different difficulty levels (Easy, Medium, Hard). Specifically, FDNet1.0 achieves 95.9% on the Easy set, 94.5% on the Medium set, and 87.9% on the Hard set. | AP => 0.0\n",
      "The Spynet method is evaluated on the MPI-Sintel dataset for the Optical_Flow_Estimation task. | Sintel-final => 0.5\n",
      "The TuckER method is evaluated on four standard link prediction datasets: FB15k, FB15k-237, WN18, and WN18RR. | FB15k-237 => 0.5\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset for the Natural Language Inference task. | SNLI => 1.0\n",
      "The SRCNN method is evaluated on the Manga109_-_4x_upscaling dataset using metrics such as PSNR (Peak Signal-to-Noise Ratio) and IFC (Information Fidelity Criterion). | PSNR, SSIM => 0.5\n",
      "The CRN method for the Image-to-Image Translation task does not have specific datasets mentioned in the retrieved documents. Further specific information might be found in the original research papers or documentation related to the CRN method. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The DeepLab-LargeFOV method is evaluated on the SUN-RGBD dataset for Scene Segmentation using metrics such as accuracy and boundary F1-measure (BF). These metrics are used to assess the performance of segmentation architectures, focusing on both region accuracies and boundary precision. | Mean_IoU => 0.0\n",
      "The AWD-LSTM-DOC method is evaluated on the WikiText-2 dataset for the Language Modelling task using the metric of perplexity. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The VAT_EntMin method for Semi-Supervised Image Classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The NICE method evaluation metrics on the CIFAR-10 dataset for the Image Generation task are not explicitly mentioned in the retrieved documents. However, common metrics for evaluating image generation tasks on datasets like CIFAR-10 include Inception Score, Fréchet Inception Distance (FID), and Perplexity. These metrics assess the quality and diversity of generated images. | NLL_Test => 0.0\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using a metric based on the number of correctly matched pixels compared to the overall number of pixels. This is often referred to as 'accuracy@', where a pixel is considered correct if its match in the second image is closer than a certain threshold to the ground truth. | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The DQN_noop method is evaluated on 57 Atari games, using both human and noop start settings. | Atari_2600_River_Raid => 0.0\n",
      "The Paragraph_vector__lexical_overlap___dist_output_ method is evaluated on the QASent dataset for the Question Answering task using metrics such as Exact Match (EM) and F1 score. These metrics assess the accuracy of the predicted answer spans compared to the ground truth answers. | MAP, MRR => 0.0\n",
      "The SVDCNN method for text classification is evaluated on several large-scale datasets, including AG's News Corpus, Sogou News Corpus, DBPedia Ontology Dataset, Yelp Reviews, Yahoo! Answers, and Amazon Reviews. | AG_News => 0.5\n",
      "The available resources did not provide the specific dataset on which the DDQN__tuned__noop method achieves the highest Score score for the Atari_Games task. Further detailed research or access to specific experimental results may be required to obtain this information. | Atari_2600_Video_Pinball => 0.0\n",
      "The Impatient_Reader method is evaluated on the CNN/Daily Mail dataset for the Question_Answering task using accuracy as the primary metric. The performance is measured by the proportion of test cases where the ground truth is among the top answers proposed by the model. | CNN, Daily_Mail => 0.5\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD and TriviaQA. | TriviaQA => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [02:03<1:08:08, 70.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PNN method evaluation metrics on the Bing_News dataset for the Click-Through Rate Prediction task are not explicitly mentioned in the retrieved documents. However, common evaluation metrics for CTR prediction tasks typically include accuracy, precision, recall, and F1-score. | AUC, Log_Loss => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score for the Pose Estimation task on the MPII dataset. | FLIC_Elbows => 0.0\n",
      "The DPN-131 method is evaluated on datasets such as the RVL-CDIP dataset, Tobacco-3482 dataset, and Places365-Standard dataset for the Image Classification task. | ImageNet => 0.0\n",
      "The Field-gating Seq2seq dual attention method is evaluated on the WikiBio dataset using metrics such as BLEU and human evaluation. The dual attention mechanism improves the model's performance by over 1 BLEU compared to the vanilla attention mechanism. | BLEU, ROUGE => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:07<00:00,  2.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DRCN method is evaluated on the Set5 4x upscaling dataset for Image Super-Resolution using the PSNR (Peak Signal-to-Noise Ratio) metric. The 30-layer DRCN network exceeds the performance of the second-best method, CSCN, by 0.47dB on the 4x scale. | MOS, PSNR, SSIM => 0.33\n",
      "The ConvNet method for Keypoint Detection on the Pascal3D dataset is evaluated using metrics such as Average Precision (AP) and Intersection over Union (IoU). | Mean_PCK => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score, metrics = multi_thread_executor(toolqa_test, ToolQASignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score: 0.21\n",
      "\n",
      "Avatar Execution Metrics Report\n",
      "==============================\n",
      "Execution Time: 127.71 seconds\n",
      "Total API Calls: 60\n",
      "Total Tokens: 91,737 (1,702 in, 90,035 out)\n",
      "Estimated Cost: $0.9046\n",
      "\n",
      "Average Time per Call: 2.13 seconds\n",
      "\n",
      "Tool Usage Breakdown:\n",
      "-------------------\n",
      "main_llm: 60 calls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Average Score on ArxivQA before opitmization: {aqa_score:.2f}\")\n",
    "print(f\"Test Score: {score:.2f}\")\n",
    "print(format_metrics_report(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batched_optimizer import AvatarOptimizerWithMetrics\n",
    "\n",
    "iterative_monkey = AvatarOptimizerWithMetrics(\n",
    "    metric=metric,\n",
    "    max_iters=2,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:14<09:12, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The method that achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for Atari Games is LBC with a score of 10077.52%. | Ape-X => 0.0\n",
      "The X-Transformer achieved the highest BLEU score of 46.63 on the WMT2014 English-German dataset for the Machine Translation task. | Weighted_Transformer__large_ => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the retrieved information. The available papers discuss DCCL in the context of Generalized Category Discovery and Unsupervised Domain Adaptation, but not specifically for Machine Translation. | IWSLT2015_German-English => 0.0\n",
      "The IQN method is evaluated on 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The A3C-CTS method is evaluated on the whole Atari 2600 suite, including Montezuma's Revenge and Bellemare et al.'s set of hard exploration games with sparse rewards. | Atari_2600_Venture => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The method that achieves the highest MRR score on the FB15k dataset for the Link Prediction task is AutoKGE with an MRR of 0.861. | TuckER => 0.0\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 87.0 F1, achieved by the span-based model presented in the paper \"A Span Selection Model for Semantic Role Labeling\" by Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. | Li_et_al_ => 0.0\n",
      "The Bi-LSTM trained on FCE method achieves the highest F0.5 score on the FCE dataset for the Grammatical Error Detection task, as indicated by the result from the paper by Masahiro Kaneko and Mamoru Komachi. | CoNLL-2014_A2 => 0.0\n",
      "The method PTSR (Patch Translator for Image Super-Resolution) achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models. | PFF => 0.0\n",
      "The current state-of-the-art on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3. | Duel_noop => 0.0\n",
      "LISA method achieves the highest F1 score for Predicate_Detection task on both in-domain datasets, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "The highest Validation_perplexity score on the Penn Treebank Word Level dataset for Language Modelling is achieved by OpenAI's GPT-3 with a word-level perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The Frustum_PointNets method is evaluated on the KITTI and Lyft datasets for the Object_Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is not explicitly mentioned in the available resources. However, the Renovating Parsing R-CNN (RP R-CNN) is a notable method that has been highlighted for its performance in related tasks. | NAN => 1.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using the NDCG@100 metric for the Collaborative Filtering task. | Recall_20, Recall_50 => 0.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric. | Percentage_error => 1.0\n",
      "The TARNet method is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both a randomized and a non-randomized component, for the Causal Inference task. | IDHP => 0.5\n",
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Ms__Pacman => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:50<17:05, 26.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Time_Pilot => 0.0\n",
      "The Transformer method is evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using the BLEU metric. The evaluation reports tokenized BLEU using the \"multi-bleu.perl\" script. | BLEU_score => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  10%|█         | 4/40 [00:50<06:10, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__noop method is evaluated on 57 Atari games. | Atari_2600_Berzerk => 0.0\n",
      "The DDQN__tuned__hs method evaluated datasets for the Atari_Games task are not explicitly mentioned in the retrieved documents. Further specific information might be found in the original research papers or documentation related to the method. | Atari_2600_Assault => 0.0\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link_Prediction task using metrics such as AUC (Area Under the Curve) and possibly other standard link prediction evaluation metrics, although specific metrics were not explicitly found in the search results. | Accuracy => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using metrics such as accuracy and F1 score, which are common for Natural Language Inference tasks. | Matched, Mismatched => 0.0\n",
      "The Duel_hs method is evaluated on 57 Atari games, as it is compared with other algorithms across all these games. | Atari_2600_Video_Pinball => 0.0\n",
      "The IDE____CamStyle method is evaluated on the PRID2011, iLIDS-VID, and VIPeR datasets for the Person Re-Identification task. | DukeMTMC-reID => 0.0\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on datasets such as miniImageNet and Fewshot-CIFAR100 (FC100). | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. | F-Measure => 0.5\n",
      "CornerNet-Squeeze is evaluated on the COCO dataset for the Real-Time_Object_Detection task. | COCO => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  15%|█▌        | 6/40 [00:59<04:15,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PFF method for Image Super-Resolution is evaluated on the Set5 and Set14 datasets. | Set14_-_4x_upscaling => 0.5\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains (books, DVDs, electronics, and kitchen appliances), and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The DeepFM method achieves the highest Log_Loss score for the Click-Through Rate Prediction task on the Criteo dataset. The Criteo dataset is a well-known ad tech industry benchmarking dataset used for evaluating CTR prediction models. | Criteo => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  20%|██        | 8/40 [01:01<02:33,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ResNet_ELU method is evaluated on the CIFAR-100 dataset using the test error percentage as a metric. The ELU networks achieved a test error of 24.28%, which is noted as the best published result on CIFAR-100 without using multi-view evaluation or model averaging. | Percentage_correct => 1.0\n",
      "The BiDAF___Self_Attention__single_model_ method is evaluated on the SQuAD and CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  28%|██▊       | 11/40 [01:03<01:23,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LapSRN method is typically evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) on datasets like Urban100 for the Image Super-Resolution task. | PSNR => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:05<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset using two main metrics: Average Precision (AP) at 50% Intersection-over-Union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth boxes, while CorLoc is the percentage of images with at least one correctly localized instance of the target object class.The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 score as the evaluation metric for the Question Answering task. | F1 => 1.0\n",
      " | MAP => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 of 3...\n",
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The Bi-LSTM trained on FCE method achieves the highest F0.5 score on the FCE dataset for the Grammatical Error Detection task, as indicated by the result from the paper by Masahiro Kaneko and Mamoru Komachi. | CoNLL-2014_A2 => 0.0\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 87.0 F1, achieved by the span-based model presented in the paper \"A Span Selection Model for Semantic Role Labeling\" by Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. | Li_et_al_ => 0.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the retrieved information. The available papers discuss DCCL in the context of Generalized Category Discovery and Unsupervised Domain Adaptation, but not specifically for Machine Translation. | IWSLT2015_German-English => 0.0\n",
      "The method PTSR (Patch Translator for Image Super-Resolution) achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models.The IQN method is evaluated on 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      " | PFF => 0.0\n",
      "The A3C-CTS method is evaluated on the whole Atari 2600 suite, including Montezuma's Revenge and Bellemare et al.'s set of hard exploration games with sparse rewards. | Atari_2600_Venture => 0.0\n",
      "The highest BLEU score on the WMT2014 English-German dataset for Machine Translation is 46.63, achieved by the X-Transformer model. | Weighted_Transformer__large_ => 0.0\n",
      "The current state-of-the-art on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The highest Validation_perplexity score on the Penn Treebank Word Level dataset for Language Modelling is achieved by OpenAI's GPT-3 with a word-level perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "LISA method achieves the highest F1 score for Predicate_Detection task on both in-domain datasets, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "The Frustum_PointNets method is evaluated on the KITTI dataset for the Object_Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The Duel_hs method is evaluated on the DQN-replay dataset for Atari Games. | Atari_2600_Video_Pinball => 0.0\n",
      "The highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is not explicitly available from the current search results. The Renovating Parsing R-CNN (RP R-CNN) is a notable method mentioned in the literature, but specific AP_0_5 scores for this dataset were not found in the available resources. | NAN => 1.0\n",
      "The method that achieves the highest MRR score on the FB15k dataset for the Link Prediction task is AutoKGE with an MRR of 0.861. | TuckER => 0.0\n",
      "The DDQN__tuned__noop method is evaluated on 57 Atari games. | Atari_2600_Berzerk => 0.0\n",
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Ms__Pacman => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:50<16:06, 25.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Time_Pilot => 0.0\n",
      "The DDQN__tuned__hs method evaluated datasets for the Atari_Games task are not explicitly mentioned in the retrieved documents. Further specific information might be found in the original research papers or documentation related to the method. | Atari_2600_Assault => 0.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using the NDCG@100 metric for the Collaborative Filtering task. | Recall_20, Recall_50 => 0.0\n",
      "The PFF method for Image Super-Resolution is evaluated on the Set5 and Set14 datasets. | Set14_-_4x_upscaling => 0.5\n",
      "The TARNet method is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both a randomized and a non-randomized component, for the Causal Inference task. | IDHP => 0.5\n",
      "CornerNet-Squeeze is evaluated on the COCO dataset for the Real-Time_Object_Detection task. | COCO => 1.0\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link_Prediction task using metrics such as AUC (Area Under the Curve) and possibly other standard link prediction evaluation metrics, although specific metrics were not explicitly found in the search results. | Accuracy => 0.0\n",
      "The Transformer method is evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using the BLEU metric. The evaluation reports tokenized BLEU using the \"multi-bleu.perl\" script. | BLEU_score => 1.0\n",
      "The BiDAF___Self_Attention__single_model_ method is evaluated on the SQuAD and CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  12%|█▎        | 5/40 [00:57<05:37,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LapSRN method is evaluated on the Urban100 4x upscaling dataset using the metrics Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | PSNR => 0.0\n",
      "Agent57 is the first deep reinforcement learning agent to outperform the standard human benchmark on all 57 Atari games, but specific Medium_Human-Normalized_Score data is not available. | Ape-X => 0.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric. | Percentage_error => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  20%|██        | 8/40 [01:01<02:57,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ResNet_ELU method is evaluated on the CIFAR-100 dataset using the test error percentage as a metric. The ELU networks achieved a test error of 24.28%, which is noted as the best published result on CIFAR-100 without using multi-view evaluation or model averaging. | Percentage_correct => 1.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. | F-Measure => 0.5\n",
      "The DeepFM method achieves the highest Log_Loss score for the Click-Through Rate Prediction task on the Criteo dataset. The Criteo dataset is a well-known ad tech industry benchmarking dataset used for evaluating CTR prediction models. | Criteo => 1.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset for the Weakly Supervised Object Detection task using two main metrics: Average Precision (AP) at 50% intersection-over-union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth on the test set, while CorLoc measures the percentage of images with at least one correctly localized instance on the training and validation subsets. | MAP => 0.5\n",
      "The IDE____CamStyle method is evaluated on the PRID2011, iLIDS-VID, and VIPeR datasets for the Person Re-Identification task. | DukeMTMC-reID => 0.0\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains (books, DVDs, electronics, and kitchen appliances), and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric. | Matched, Mismatched => 0.0\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on datasets such as miniImageNet and Fewshot-CIFAR100 (FC100). | CUB-200_-_0-Shot_Learning => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:06<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Subgraph_embeddings method for the Question Answering task on the WebQuestions dataset is evaluated using the F1 score as the primary metric. | F1 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3 of 3...\n",
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The method that achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for Atari Games is LBC with a score of 10077.52%. | Ape-X => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for Machine Translation. | WMT2014_English-French => 0.0\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the retrieved information. The available papers discuss DCCL in the context of Generalized Category Discovery and Unsupervised Domain Adaptation, but not specifically for Machine Translation. | IWSLT2015_German-English => 0.0\n",
      "The Bi-LSTM trained on FCE method achieves the highest F0.5 score on the FCE dataset for the Grammatical Error Detection task, as indicated by the result from the paper by Masahiro Kaneko and Mamoru Komachi. | CoNLL-2014_A2 => 0.0\n",
      "The method that achieves the highest MRR score on the FB15k dataset for the Link Prediction task is AutoKGE with an MRR of 0.861. | TuckER => 0.0\n",
      "The method PTSR (Patch Translator for Image Super-Resolution) achieves the highest PSNR score on the Set14 4x upscaling dataset for the Image Super-Resolution task, with an improvement of 21.66% in PSNR score compared to the best competitive models. | PFF => 0.0\n",
      "The X-Transformer achieved the highest BLEU score of 46.63 on the WMT2014 English-German dataset for the Machine Translation task. | Weighted_Transformer__large_ => 0.0\n",
      "The Frustum_PointNets method is evaluated on the KITTI dataset for the Object_Localization task. | KITTI_Cars_Hard => 0.5\n",
      "The current state-of-the-art on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The highest F1 score on the OntoNotes dataset for Semantic Role Labeling is 87.0 F1, achieved by the span-based model presented in the paper \"A Span Selection Model for Semantic Role Labeling\" by Hiroki Ouchi, Hiroyuki Shindo, and Yuji Matsumoto. | Li_et_al_ => 0.0\n",
      "The IQN method is evaluated on 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The A3C-CTS method is evaluated on the whole Atari 2600 suite, including Montezuma's Revenge and Bellemare et al.'s set of hard exploration games with sparse rewards. | Atari_2600_Venture => 0.0\n",
      "The highest Validation_perplexity score on the Penn Treebank Word Level dataset for Language Modelling is achieved by OpenAI's GPT-3 with a word-level perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is not explicitly available from the current search results. However, the Renovating Parsing R-CNN (RP R-CNN) is mentioned as a state-of-the-art method, but its specific AP_0_5 score on this dataset is not provided in the available resources. | NAN => 1.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric. | Percentage_error => 1.0\n",
      "The Mult-DAE method is evaluated on the Netflix dataset using the NDCG@100 metric for the Collaborative Filtering task. | Recall_20, Recall_50 => 0.0\n",
      "The DDQN__tuned__noop method is evaluated on 57 Atari games. | Atari_2600_Berzerk => 0.0\n",
      "The TARNet method is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both a randomized and a non-randomized component, for the Causal Inference task. | IDHP => 0.5\n",
      "CornerNet-Squeeze is evaluated on the COCO dataset for the Real-Time_Object_Detection task. | COCO => 1.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. | F-Measure => 0.5\n",
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Ms__Pacman => 0.0\n",
      "The BiDAF___Self_Attention__single_model_ method is evaluated on the SQuAD and CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [00:57<18:12, 28.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games for the Atari_Games task. | Atari_2600_Time_Pilot => 0.0\n",
      "The IDE____CamStyle method is evaluated on the PRID2011, iLIDS-VID, and VIPeR datasets for the Person Re-Identification task. | DukeMTMC-reID => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  15%|█▌        | 6/40 [00:59<04:27,  7.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PFF method for Image Super-Resolution is evaluated on the Set5 and Set14 datasets. | Set14_-_4x_upscaling => 0.5\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on datasets such as miniImageNet and Fewshot-CIFAR100 (FC100). | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The Transformer method is evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using the BLEU metric. The evaluation reports tokenized BLEU using the \"multi-bleu.perl\" script. | BLEU_score => 1.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset for the Weakly Supervised Object Detection task using two main metrics: Average Precision (AP) at 50% intersection-over-union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth on the test set, while CorLoc measures the percentage of images with at least one correctly localized instance on the training and validation subsets. | MAP => 0.5\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains (books, DVDs, electronics, and kitchen appliances), and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The Duel_hs method is evaluated on 57 Atari games, as it is compared with other algorithms across all these games. | Atari_2600_Video_Pinball => 0.0\n",
      "The DeepFM method achieves the highest Log_Loss score for the Click-Through Rate Prediction task on the Criteo dataset. The Criteo dataset is a well-known ad tech industry benchmarking dataset used for evaluating CTR prediction models. | Criteo => 1.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on the CoNLL-2005 and CoNLL-2012 datasets, with scores above 97 F1. | CoNLL_2005 => 0.5\n",
      "The LapSRN method is typically evaluated using image quality metrics such as PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index) on the Urban100 dataset for the 4x upscaling Image Super-Resolution task. | PSNR => 0.5\n",
      "The ResNet_ELU method is evaluated on the CIFAR-100 dataset using the test error percentage as the metric. The ELU networks achieved a test error of 24.28%, which is among the best results reported for CIFAR-100. | Percentage_correct => 1.0\n",
      "The MTGAE method evaluation metrics on the Pubmed dataset for the Link_Prediction task are not explicitly found in the available resources. Further specific literature or documentation on MTGAE might be needed to determine the exact metrics used. | Accuracy => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric. | Matched, Mismatched => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [01:10<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 score as the primary metric. | F1 => 1.0\n",
      "The DDQN__tuned__hs method evaluated datasets for the Atari_Games task are not explicitly mentioned in the retrieved documents. However, it is common for methods like DDQN to be evaluated on a set of 57 Atari games, which is a standard benchmark in reinforcement learning research. | Atari_2600_Assault => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.275\n",
      "Generated new instruction: New Instruction: You will be given `Tools`, which is a list of resources to use in order to accomplish the `Goal`. Your task is to decide which tool to use and what input values to provide based on the user query. To enhance your performance, consider the following strategies: \n",
      "\n",
      "1. **Pattern Analysis and Tool Selection:** Begin by analyzing the query to determine its specificity and complexity. For queries related to specific datasets or metrics, prioritize using `ARXIV_SEARCH` for detailed academic papers and `WEB_SEARCH` for broader context. Utilize multiple tools to gather comprehensive information, such as combining `WEB_SEARCH`, `ARXIV_SEARCH`, and `RETRIEVE` to cross-verify and gather detailed insights. This approach ensures that you are not relying on a single tool, which can lead to incomplete results.\n",
      "\n",
      "2. **Enhanced Query Formulation and Tool Combination:** Develop precise and targeted queries by breaking down the query into smaller, more specific components. Start with `WEB_SEARCH` to gather general information, then use `ARXIV_SEARCH` for in-depth analysis. Implement a feedback loop where the results from one tool inform the query for the next tool, refining the search process iteratively. This strategy allows for initial broad data gathering followed by detailed exploration, ensuring a comprehensive understanding of the topic.\n",
      "\n",
      "3. **Tool Usage Guidelines and Feedback Loop:** Follow guidelines on when to use each tool based on the type of query. Use `RETRIEVE` for specific paper details and `WEB_SEARCH` for general trends or comparisons. Cross-verify information using multiple tools to validate and fill in gaps that a single tool might miss. By implementing these strategies, you can significantly improve performance on negative inputs, leading to more accurate and comprehensive results.\n",
      "Processing batch 1 of 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ByteNet method is evaluated on the English-to-German WMT translation task for the Machine Translation task. | WMT2014_English-French => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:14<09:37, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoKGE achieves the highest MRR score of 0.861 on the FB15k dataset for the Link Prediction task. | TuckER => 0.0\n",
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The IQN method is evaluated on a baseline dataset of 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The Bi-LSTM trained on the FCE dataset achieves the highest F0.5 score of 53.49 for the Grammatical Error Detection task. | CoNLL-2014_A2 => 0.0\n",
      "The method that achieves the highest score on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The highest BLEU score achieved on the WMT2014 English-German dataset for the Machine Translation task is 46.63 by the X-Transformer model. | Weighted_Transformer__large_ => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The method achieving the highest PSNR score on the Set14 - 4x upscaling dataset for Image Super-Resolution is DRCT-L with a PSNR of 29.54. | PFF => 0.0\n",
      "GDI-H3 achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for the Atari_Games task. | Ape-X => 0.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset for the Curved_Text_Detection task using metrics such as precision, recall, and F-measure. | F-Measure => 0.5\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link Prediction task using metrics such as MRR (Mean Reciprocal Rank) and Hits@100. | Accuracy => 0.0\n",
      "CornerNet-Squeeze is evaluated on the PASCAL VOC and MS COCO datasets for the Real-Time Object Detection task. | COCO => 0.0\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews across four domains, and the evaluation involves domain adaptation tasks with labeled and unlabeled examples. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The LapSRN method is evaluated on the Urban100 4x upscaling dataset using the metrics Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | PSNR => 0.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset using metrics such as Average Precision (AP) at 50% Intersection-over-Union (IoU) and CorLoc. AP measures the precision of detected boxes against ground truth, while CorLoc evaluates the percentage of images with at least one correctly localized object instance. | MAP => 0.5\n",
      "The BiDAF Self Attention single model method is evaluated on the Stanford Question Answering Dataset (SQuAD), specifically SQuAD 2.0, for the Question Answering task. | SQuAD1_1 => 0.0\n",
      "The method that achieves the highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is NAN with a score of 59.70%. | NAN => 1.0\n",
      "The current highest F1 score on the OntoNotes dataset for the Semantic Role Labeling task is achieved by the HeSyFu model with an F1 score of 88.59. | Li_et_al_ => 0.0\n",
      "The highest validation perplexity score on the Penn Treebank Word Level dataset for language modeling is achieved by the Past Decode Regularization (PDR) method, which achieves a word level perplexity of 53.8. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric. | Matched, Mismatched => 0.0\n",
      "The TARNet method for causal inference is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both randomized and non-randomized components. | IDHP => 0.5\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task according to the available search results. | IWSLT2015_German-English => 0.0\n",
      "The ResNet_ELU method for the CIFAR-100 Image Classification task is evaluated using metrics such as test error percentage. The ELU networks achieved a test error of 24.28% on CIFAR-100, which is noted as one of the best published results without resorting to multi-view evaluation or model averaging. | Percentage_correct => 1.0\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on datasets such as miniImageNet and Fewshot-CIFAR100 (FC100). | CUB-200_-_0-Shot_Learning => 0.0\n",
      "Frustum PointNets is evaluated on KITTI and SUN RGB-D datasets for the Object Localization task.The IDE____CamStyle method for Person Re-Identification is evaluated on datasets such as DukeMTMC-reID, VIPeR, CAVIAR, and PRID. | DukeMTMC-reID => 0.5\n",
      " | KITTI_Cars_Hard => 0.5\n",
      "DeepFM achieves the highest Log_Loss score of 0.45083 on the Criteo dataset for the Click-Through Rate Prediction task. | Criteo => 1.0\n",
      "The Transformer method is typically evaluated using metrics such as BLEU, METEOR, and NIST on datasets like IWSLT2015 for the German-English Machine Translation task. | BLEU_score => 0.5\n",
      "The PFF method for Image Super-Resolution does not have specific datasets mentioned in the available search results. Further detailed information might be required from specific research papers or direct sources. | Set14_-_4x_upscaling => 0.0\n",
      "The Mult-DAE method is typically evaluated on metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) when applied to the Netflix dataset for the Collaborative Filtering task. These metrics help in assessing the accuracy of the predictions made by the model. | Recall_20, Recall_50 => 0.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated using the Word Error Rate (WER) metric on the swb_hub_500_WER_fullSWBCH dataset for the Speech Recognition task. | Percentage_error => 1.0\n",
      "LISA method achieves the highest F1 score for Predicate_Detection task on in-domain datasets, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using metrics such as F1 score, which measures the balance between precision and recall in the context of question answering tasks. | F1 => 1.0\n",
      "The A3C-CTS method is evaluated on the Atari 2600 suite of games, which includes a variety of games such as Montezuma's Revenge and other hard exploration games with sparse rewards. However, specific datasets for CTS evaluation were not found in the search results. | Atari_2600_Venture => 0.0\n",
      "The Duel_noop method is evaluated on 57 Atari games. | Atari_2600_Ms__Pacman => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [02:59<1:05:17, 103.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games, as indicated by the repeated references to evaluations across all 57 Atari games in the retrieved documents. | Atari_2600_Time_Pilot => 0.0\n",
      "The Duel_hs method evaluation datasets for the Atari Games task are not explicitly mentioned in the retrieved documents. Further specific information might be needed from the original research paper or dataset documentation. | Atari_2600_Video_Pinball => 0.0\n",
      "The DDQN__tuned__hs method is evaluated on the Atari 2600 games using the Atari Learning Environment (ALE) as the evaluation platform. However, specific datasets or games used for evaluation were not found in the search results. | Atari_2600_Assault => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [03:03<00:00,  4.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__noop method for the Atari_Games task is evaluated on various Atari 2600 games, but specific datasets or details are not clearly mentioned in the available resources. | Atari_2600_Berzerk => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2 of 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:01<00:50,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoKGE achieves the highest MRR score of 0.861 on the FB15k dataset for the Link Prediction task. | TuckER => 0.0\n",
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for the Machine Translation task. | WMT2014_English-French => 0.0\n",
      "The IQN method is evaluated on a baseline dataset of 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The method achieving the highest PSNR score on the Set14 - 4x upscaling dataset for Image Super-Resolution is DRCT-L with a PSNR of 29.54. | PFF => 0.0\n",
      "The method that achieves the highest score on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3. | Duel_noop => 0.0\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The highest BLEU score achieved on the WMT2014 English-German dataset for Machine Translation is 46.63 by the X-Transformer model. | Weighted_Transformer__large_ => 0.0\n",
      "The method achieving the highest F1 score on the OntoNotes dataset for Semantic Role Labeling is HeSyFu with an F1 score of 88.59. | Li_et_al_ => 0.0\n",
      "The Bi-LSTM trained on the FCE dataset achieves the highest F0.5 score of 53.49 for the Grammatical Error Detection task. | CoNLL-2014_A2 => 0.0\n",
      "GDI-H3 achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for the Atari_Games task. | Ape-X => 0.0\n",
      "The CornerNet-Squeeze method is evaluated on the COCO dataset for the Real-Time Object Detection task. | COCO => 1.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset for the Curved_Text_Detection task using metrics such as precision, recall, and F-measure. | F-Measure => 0.5\n",
      "LISA method achieves the highest F1 score for Predicate_Detection task on in-domain datasets, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "The BiDAF Self Attention single model method is evaluated on the Stanford Question Answering Dataset (SQuAD), specifically SQuAD 2.0, for the Question Answering task. | SQuAD1_1 => 0.0\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link Prediction task using metrics such as AUC (Area Under the Curve) and AP (Average Precision). | Accuracy => 0.0\n",
      "The LapSRN method is evaluated on the Urban100 4x upscaling dataset using the metrics Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | PSNR => 0.0\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains, and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The Duel_noop method is evaluated on 57 Atari games, including both human and noop start settings. The evaluation involves calculating mean and median human normalized scores across all games, as well as mean rank and Elo scores for inter-algorithm comparison. | Atari_2600_Ms__Pacman => 0.0\n",
      "The IDE____CamStyle method for Person Re-Identification is evaluated on datasets such as PRID2011, iLIDS-VID, Market-1501, DukeMTMC-reID, and CUHK03. | DukeMTMC-reID => 0.5\n",
      "DeepFM achieves the highest Log_Loss score for Click-Through Rate Prediction on the Company* dataset. | Criteo => 0.0\n",
      "The TARNet method for causal inference is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both randomized and non-randomized components. | IDHP => 0.5\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric for the Natural Language Inference task. | Matched, Mismatched => 0.0\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on datasets such as miniImageNet and Fewshot-CIFAR100 (FC100). | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL_VOC_2012 dataset using metrics such as mean average precision (mAP) and CorLoc for the Weakly Supervised Object Detection task. | MAP => 0.5\n",
      "The ResNet_ELU method on the CIFAR-100 dataset for Image Classification is evaluated using test error percentage as a metric. The ELU networks achieved a test error of 24.28%, which is noted as one of the best published results on CIFAR-100 without resorting to multi-view evaluation or model averaging. | Percentage_correct => 0.5\n",
      "The Transformer method is typically evaluated on the IWSLT2015 German-English dataset for the Machine Translation task using metrics such as BLEU, METEOR, and NIST. These are standard evaluation metrics for machine translation systems. | BLEU_score => 0.5\n",
      "The method achieving the highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is NAN with a score of 59.70%. | NAN => 0.5\n",
      "The DCCL method is not specifically evaluated on datasets for the Machine Translation task. The available information primarily discusses its application in generalized category discovery and visual recognition tasks. | IWSLT2015_German-English => 0.0\n",
      "The Frustum PointNets method is evaluated on the KITTI and SUN RGB-D datasets for the Object Localization task. | KITTI_Cars_Hard => 0.5\n",
      "OpenAI's GPT-3 reportedly scored a word-level perplexity score of 20.5 on the Penn Treebank Word Level dataset, which is currently the highest known validation perplexity score for language modeling on this dataset. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The Duel_hs method evaluation datasets for the Atari Games task were not specifically identified in the search results. Further detailed information might be available in specific research papers or datasets related to Atari Games evaluations. | Atari_2600_Video_Pinball => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [02:03<46:01, 72.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games. | Atari_2600_Time_Pilot => 0.0\n",
      "The A3C-CTS method is evaluated on the entire suite of Atari 2600 games, including challenging exploration games like Montezuma's Revenge, as part of the Arcade Learning Environment. | Atari_2600_Venture => 0.5\n",
      "The Mult-DAE method is typically evaluated using metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) on the Netflix dataset for the Collaborative Filtering task. However, specific details on the evaluation metrics for Mult-DAE were not found in the search results. | Recall_20, Recall_50 => 0.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using the Word Error Rate (WER) metric for the Speech Recognition task. | Percentage_error => 1.0\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 score as a metric. | F1 => 1.0\n",
      "The DDQN__tuned__hs method for the Atari_Games task is evaluated on various Atari 2600 games, but specific datasets or games used for evaluation were not found in the search results. | Atari_2600_Assault => 0.0\n",
      "The datasets on which the PFF method is evaluated for Image Super-Resolution are not explicitly mentioned in the available resources. Further specific information might be required from the original research paper or related documentation. | Set14_-_4x_upscaling => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [02:35<00:00,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__noop method for the Atari_Games task is evaluated on various Atari 2600 games, but specific datasets are not mentioned in the available search results. | Atari_2600_Berzerk => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 3 of 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▎         | 1/40 [00:00<00:24,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OmniPose achieves the highest PCK score of 99.5% on the Leeds Sports Poses dataset for the Pose Estimation task. | Pyramid_Residual_Modules__PRMs_ => 0.0\n",
      "AutoKGE achieves the highest MRR score of 0.861 on the FB15k dataset for the Link Prediction task. | TuckER => 0.0\n",
      "The ByteNet method is evaluated on the English-to-German WMT translation task for the Machine Translation task. | WMT2014_English-French => 0.0\n",
      "The Bi-LSTM trained on the FCE dataset achieves the highest F0.5 score of 53.49 for the Grammatical Error Detection task. | CoNLL-2014_A2 => 0.0\n",
      "The method achieving the highest F1 score on the OntoNotes dataset for Semantic Role Labeling is HeSyFu with an F1 score of 88.59. | Li_et_al_ => 0.0\n",
      "The highest BLEU score achieved on the WMT2014 English-German dataset for Machine Translation is 46.63 by the X-Transformer model. | Weighted_Transformer__large_ => 0.0\n",
      "The IQN method is evaluated on a baseline dataset of 57 Atari 2600 games in the ALE (Atari Learning Environment). | Atari_2600_Kung-Fu_Master => 0.5\n",
      "The method that achieves the highest SSIM score on the Vid4 - 4x upscaling dataset for Video Super-Resolution is EvTexture+ with an SSIM score of 0.8983. | VESPCN => 0.0\n",
      "The method achieving the highest PSNR score on the Set14 - 4x upscaling dataset for Image Super-Resolution is DRCT-L with a PSNR of 29.54. | PFF => 0.0\n",
      "The IDE____CamStyle method for Person Re-Identification is evaluated on the Market-1501 and DukeMTMC-reID datasets. | DukeMTMC-reID => 0.5\n",
      "The method that achieves the highest score on the Atari 2600 Road Runner dataset for the Atari Games task is GDI-H3, with a score of 999999. | Duel_noop => 0.0\n",
      "GDI-H3 achieves the highest Medium_Human-Normalized_Score on the Atari-57 dataset for the Atari_Games task. | Ape-X => 0.0\n",
      "The CornerNet-Squeeze method is evaluated on the COCO dataset for the Real-Time Object Detection task. | COCO => 1.0\n",
      "The Sample_Clustering method for Few-Shot Image Classification is evaluated on datasets such as miniImageNet and Fewshot-CIFAR100 (FC100). | CUB-200_-_0-Shot_Learning => 0.0\n",
      "The MTGAE method is evaluated on the Pubmed dataset for the Link Prediction task using metrics such as AUC (Area Under the Curve) and AP (Average Precision). | Accuracy => 0.0\n",
      "The LISA method achieves the highest F1 score for the Predicate_Detection task on both in-domain datasets, with scores above 97 F1. | CoNLL_2005 => 0.0\n",
      "The Duel_noop method is evaluated on 57 Atari games, including both human and noop start settings. The evaluation involves calculating mean and median human normalized scores across all games, as well as mean rank and Elo scores for inter-algorithm comparison. | Atari_2600_Ms__Pacman => 0.0\n",
      "The BiDAF Self Attention single model method is evaluated on the Stanford Question Answering Dataset (SQuAD) and the CNN/DailyMail datasets for the Question Answering task. | SQuAD1_1 => 0.0\n",
      "The TARNet method for causal inference is evaluated on the semi-synthetic IHDP dataset and the Jobs dataset, which includes both randomized and non-randomized components. | IDHP => 0.5\n",
      "The LapSRN method is evaluated on the Urban100 4x upscaling dataset using the metrics Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). | PSNR => 0.0\n",
      "The DCCL method is not specifically evaluated on any datasets for the Machine Translation task according to the available information. | IWSLT2015_German-English => 0.0\n",
      "The ResNet_ELU method on the CIFAR-100 dataset for Image Classification is evaluated using test error percentage as a metric. The ELU networks achieved a test error of 24.28%, which is noted as one of the best published results on CIFAR-100 without resorting to multi-view evaluation or model averaging. | Percentage_correct => 0.5\n",
      "The MT-DNN method is evaluated on the MultiNLI dataset using classification accuracy as the evaluation metric for the Natural Language Inference task. | Matched, Mismatched => 0.0\n",
      "The DANN method is evaluated on the Multi-Domain Sentiment Dataset using classification accuracy as the primary metric. The dataset includes Amazon reviews from four domains, and the evaluation involves 12 domain adaptation tasks. The DANN method is compared against a standard neural network and a Support Vector Machine, with DANN showing significantly better performance in terms of classification accuracy. | Average, Books, DVD, Electronics, Kitchen => 0.0\n",
      "The CNN___Bi-RNN___CTC__speech_to_letters___25_9__WER_if_trainedonlyon_SWB method is evaluated on the swb_hub_500_WER_fullSWBCH dataset using Word Error Rate (WER) as the primary metric. The method achieves a WER of 11.8% after training on 262 hours of SWB-1 data, which is an improvement over previous baselines. | Percentage_error => 1.0\n",
      "The PSENet-1s method is evaluated on the SCUT-CTW1500 dataset using precision, recall, and F-measure metrics. PSENet-1s achieves a precision of 82.50%, recall of 79.89%, and an F-measure of 81.17%, significantly outperforming other methods in detecting curve or arbitrarily shaped texts. | F-Measure => 0.5\n",
      "The OICR-Ens___FRCNN method is evaluated on the PASCAL VOC 2012 dataset using metrics such as Average Precision (AP) at 50% Intersection-over-Union (IoU) and CorLoc, which is the percentage of images with at least one correctly localized object. | MAP => 0.5\n",
      "The method that achieves the highest validation perplexity score on the Penn Treebank Word Level dataset for the language modeling task is GPT-3 (Zero-Shot) with a perplexity score of 20.5. | Tied_Variational_LSTM___augmented_loss => 0.0\n",
      "The Transformer method is typically evaluated on the IWSLT2015 German-English dataset using metrics such as BLEU, METEOR, and NIST. These are standard evaluation metrics for machine translation tasks. | BLEU_score => 0.5\n",
      "The Frustum PointNets method is evaluated on the KITTI and SUN RGB-D datasets for the Object Localization task. | KITTI_Cars_Hard => 0.5\n",
      "DeepFM achieves the highest Log_Loss score for Click-Through Rate Prediction on the Criteo dataset. | Criteo => 1.0\n",
      "The A3C-CTS method is evaluated on the Atari 2600 suite of games, which includes a variety of games such as Montezuma's Revenge and others within the Arcade Learning Environment. However, specific datasets or a comprehensive list of games evaluated with A3C-CTS were not found in the search results. | Atari_2600_Venture => 0.5\n",
      "The method achieving the highest AP_0_5 score on the PASCAL-Person-Part dataset for the Multi-Human Parsing task is NAN with a score of 59.70%. | NAN => 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   5%|▌         | 2/40 [02:05<46:49, 73.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Duel_noop method is evaluated on 57 Atari games. | Atari_2600_Time_Pilot => 0.0\n",
      "The Subgraph_embeddings method is evaluated on the WebQuestions dataset using the F1 score as a metric for the Question Answering task. | F1 => 1.0\n",
      "The DDQN__tuned__hs method for the Atari_Games task is evaluated on various Atari 2600 games, but specific datasets or games used for evaluation were not found in the search results. | Atari_2600_Assault => 0.0\n",
      "The Duel_hs method evaluation datasets for the Atari Games task were not specifically identified in the search results. Further detailed investigation into specific academic papers or datasets might be required to find this information. | Atari_2600_Video_Pinball => 0.0\n",
      "The datasets on which the PFF method is evaluated for Image Super-Resolution are not explicitly mentioned in the available resources. | Set14_-_4x_upscaling => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  10%|█         | 4/40 [02:29<21:04, 35.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DDQN__tuned__noop method for the Atari_Games task is evaluated on various Atari 2600 games, but specific datasets were not identified in the search results. | Atari_2600_Berzerk => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 40/40 [02:30<00:00,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mult-DAE method is evaluated on the Netflix dataset using metrics such as Mean Absolute Error (MAE) and other standard collaborative filtering evaluation metrics. However, specific details on the exact metrics used for Mult-DAE were not found in the search results. | Recall_20, Recall_50 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.225\n",
      "Generated new instruction: New Instruction: You will be given `Tools`, which is a list of resources to use in order to accomplish the `Goal`. Your task is to decide which tool to use and what input values to provide based on the user query. To enhance your performance, consider the following strategies: \n",
      "\n",
      "1. **Pattern Analysis and Tool Selection:** Begin by analyzing the query to determine its specificity and complexity. For queries related to specific datasets or metrics, prioritize using `ARXIV_SEARCH` for detailed academic papers and `WEB_SEARCH` for broader context. Ensure that the tool selected aligns with the specificity and complexity of the query. Utilize multiple tools to gather comprehensive information, such as combining `WEB_SEARCH`, `ARXIV_SEARCH`, and `RETRIEVE` to cross-verify and gather detailed insights. This approach ensures that you are not relying on a single tool, which can lead to incomplete results.\n",
      "\n",
      "2. **Enhanced Query Formulation and Tool Combination:** Develop precise and targeted queries by breaking down the query into smaller, more specific components. Start with `WEB_SEARCH` to gather general information, then use `ARXIV_SEARCH` for in-depth analysis. Implement a feedback loop where the results from one tool inform the query for the next tool, refining the search process iteratively. This strategy allows for initial broad data gathering followed by detailed exploration, ensuring a comprehensive understanding of the topic. For specific evaluations, such as ByteNet, Duel_noop, and LapSRN methods, ensure that the search queries include specific datasets or tasks to retrieve more relevant results.\n",
      "\n",
      "3. **Tool Usage Guidelines and Feedback Loop:** Follow guidelines on when to use each tool based on the type of query. Use `RETRIEVE` for specific paper details and `WEB_SEARCH` for general trends or comparisons. Cross-verify information using multiple tools to validate and fill in gaps that a single tool might miss. For example, when evaluating methods like ByteNet, Duel_noop, and LapSRN, ensure that the search queries specify the exact datasets or evaluation metrics used to improve the relevance of the results. By implementing these strategies, you can significantly improve performance on negative inputs, leading to more accurate and comprehensive results.\n",
      "\n",
      "                Optimization Process Metrics\n",
      "                ==========================\n",
      "                Total Execution Time: 42.48 seconds\n",
      "                Total API Calls: 4\n",
      "                - Comparator calls: 2\n",
      "                - Feedback instruction calls: 2\n",
      "\n",
      "                Token Usage:\n",
      "                ----------\n",
      "                Total Tokens: 120,407\n",
      "                - Input tokens: 118,719\n",
      "                - Output tokens: 1,688\n",
      "\n",
      "                Cost Analysis:\n",
      "                ------------\n",
      "                Estimated Total Cost: $3.6628\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "result = iterative_monkey.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train,\n",
    "    batch_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total optimization cost: $3.6628\n",
      "Final score achieved: 0.275\n",
      "\n",
      "Iteration 0:\n",
      "Score: 0.275\n",
      "Comparator tokens in: 30700\n",
      "Comparator tokens out: 445\n",
      "Feedback tokens in: 581\n",
      "Feedback tokens out: 355\n",
      "Execution time: 237.36s\n",
      "\n",
      "Iteration 1:\n",
      "Score: 0.225\n",
      "Comparator tokens in: 86607\n",
      "Comparator tokens out: 442\n",
      "Feedback tokens in: 831\n",
      "Feedback tokens out: 446\n",
      "Execution time: 523.10s\n"
     ]
    }
   ],
   "source": [
    "optimized_actor_agent = result[\"agent\"]\n",
    "optimization_metrics = result[\"metrics\"]\n",
    "\n",
    "# Now you can process the metrics\n",
    "print(f\"Total optimization cost: ${optimization_metrics['total_cost']:.4f}\")\n",
    "print(f\"Final score achieved: {optimization_metrics['final_score']:.3f}\")\n",
    "\n",
    "# Analyze per-iteration performance\n",
    "for iteration in optimization_metrics['iteration_details']:\n",
    "    print(f\"\\nIteration {iteration['iteration']}:\")\n",
    "    print(f\"Score: {iteration['score']:.3f}\")\n",
    "    print(f\"Comparator tokens in: {iteration['comparator_metrics']['tokens_in']}\")\n",
    "    print(f\"Comparator tokens out: {iteration['comparator_metrics']['tokens_out']}\")\n",
    "    print(f\"Feedback tokens in: {iteration['feedback_metrics']['tokens_in']}\")\n",
    "    print(f\"Feedback tokens out: {iteration['feedback_metrics']['tokens_out']}\")\n",
    "    print(f\"Execution time: {iteration['execution_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method that achieves the highest Mean IoU score on the CamVid dataset for the Semantic Segmentation task is SERNet-Former with a score of 84.62. | PSPNet => 0.0\n",
      "The method TANDA achieves the highest MAP score on the WikiQA dataset for the Question Answering task, with a MAP score of 92%. | Key-Value_Memory_Network => 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method 'RankPose' achieves the highest MAE score on the BIWI dataset for the Head Pose Estimation task, improving the MAE from 4.0 to 3.71. | 3DDFA => 0.0\n",
      "The novel directed hypergraph neural network method achieves the highest accuracy on the Cora dataset for the node classification task. | GCN => 0.0\n",
      "The Snips method is evaluated on the Snips SmartLights dataset for the Speech Recognition task. | LibriSpeech_test-clean => 0.0\n",
      "The shallow-and-wide network model achieves the highest error score on the Yelp Binary classification dataset for Sentiment Analysis, with a performance of 95.9% as per the study by Hoa T. Le, Christophe Cerisara, and Alexandre Denis. | Char-level_CNN => 0.0\n",
      "The method that achieves the highest F1 score on the CoNLL-2003 English dataset for the Named Entity Recognition (NER) task is the ACE + document-context model with an F1 score of 94.6. | CVT___Multi-Task => 0.0\n",
      "EffNet-L2 (SAM) achieves the highest Percentage_correct score of 96.08 on the CIFAR-100 dataset for Image Classification. | Res2NeXt-29 => 0.0\n",
      "The NICE method for image generation on the CIFAR-10 dataset is evaluated using metrics such as the Inception Score (IS) and the Fréchet Inception Distance (FID). | NLL_Test => 0.0\n",
      "Unable to find specific evaluation metrics for FDNet on the WIDER_Face_Easy dataset for the Face Detection task. Consider checking the original paper or related publications for detailed information. | AP => 0.0\n",
      "The FRCN method is evaluated on datasets such as PASCAL VOC and MS COCO for object detection tasks. | PASCAL_VOC_2007 => 0.5\n",
      "The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the STL-10, CIFAR-10, and Caltech-101 datasets for the Image Classification task.The method \"Discriminative Unsupervised Feature Learning with Convolutional Neural Networks\" is evaluated on the following datasets for the Image Classification task: STL-10, CIFAR-10, and Caltech-101. | STL-10 => 0.5\n",
      " | CIFAR-10 => 0.5\n",
      "The Transformer method for machine translation is evaluated on several datasets, including the WMT (Workshop on Machine Translation) datasets, such as WMT 2014 English-to-German and English-to-French translation tasks. These datasets are commonly used benchmarks in the field of machine translation. | IWSLT2015_English-German => 0.0\n",
      "The TuckER method is evaluated on standard link prediction datasets, including FB15K-237 and WN18RR. | FB15k-237 => 0.5\n",
      "The CyCADA method is evaluated on the SYNTHIA_Fall-to-Winter dataset for Image-to-Image Translation using metrics such as mean Intersection over Union (mIoU), frequency weighted Intersection over Union (fwIoU), and pixel accuracy. | Per-pixel_Accuracy, fwIOU, mIoU => 1.0\n",
      "The current state-of-the-art on the SNLI dataset for Natural Language Inference is achieved by Neural Tree Indexers for Text Understanding. However, specific parameter scores were not found in the search results. | 300D_Residual_stacked_encoders => 0.0\n",
      "The Stacked Hourglass Networks achieve the highest PCK_0_2 score on the MPII Human Pose dataset for the Pose Estimation task. | FLIC_Elbows => 0.0\n",
      "The LiteFlowNet method achieves the highest Average End-Point Error score for Optical Flow Estimation on the Sintel final pass dataset. | Sintel-final => 1.0\n",
      "The S-Norm method for the Question Answering task is evaluated on datasets such as SQuAD, SelQA, WikiQA, NewWikiQA, and InforBoxQA. | TriviaQA => 0.0\n",
      "The AWD-LSTM-DOC method is typically evaluated using perplexity as a metric on the WikiText-2 dataset for the Language Modelling task. | Number_of_params, Test_perplexity, Validation_perplexity => 0.5\n",
      "The DeepMatching method is evaluated on the HPatches dataset for Dense Pixel Correspondence Estimation using metrics such as Average Endpoint Error (AEE) and Percentage of Correct Keypoints (PCK). | Viewpoint_I_AEPE, Viewpoint_II_AEPE, Viewpoint_III_AEPE, Viewpoint_IV_AEPE, Viewpoint_V_AEPE => 0.0\n",
      "The IDE CamStyle Random Erasing method for Person Re-Identification is evaluated on the Market-1501, DukeMTMC-reID, and CUHK03 datasets. | Market-1501 => 0.5\n",
      "The DPN-131 method is evaluated on the ImageNet-1k dataset for the Image Classification task. | ImageNet => 1.0\n",
      "The VGG_Resnet_LACE_BiLSTM_acoustic_model_trained_on_SWB_Fisher_CH__N-gram___RNNLM_language_model_trained_on_Switchboard_Fisher_Gigaword_Broadcast method is evaluated on the TIMIT dataset for the Speech Recognition task. | swb_hub_500_WER_fullSWBCH => 0.0\n",
      "The 3DDFA method is evaluated on the Florence dataset for 3D Face Reconstruction using the geometric error between reconstructed meshes and the ground truth as the standard benchmark metric. | Mean_NME_ => 0.0\n",
      "The U-Net method for Skin Cancer Segmentation is evaluated on datasets such as ISIC-2016, ISIC-2017, and ISIC-2018. These datasets are part of the International Skin Imaging Collaboration and are commonly used for benchmarking segmentation tasks in skin cancer research. | Kaggle_Skin_Lesion_Segmentation => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [03:09<3:05:56, 189.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuZero achieves the highest score on the Atari_2600_Name_This_Game dataset for the Atari_Games task with a score of 157177.85. | IQN => 0.0\n",
      "The iBOWIMG_baseline method achieves the highest Percentage_correct score on the COCO VQA dataset for the Visual Question Answering task. | COCO_Visual_Question_Answering__VQA__real_images_1_0_multiple_choice => 0.5\n",
      "SparseGPT (175B, 50% Sparsity) achieves the highest Number_of_params score on the WikiText-2 dataset for the Language Modelling task. | AWD-LSTM-DOC => 0.5\n",
      "The highest Train_Accuracy score on the SNLI dataset for the Natural Language Inference task found in the search results is 86.14% achieved by a model using dropout with RNNs. | __Unigram_and_bigram_features => 0.0\n",
      "The ACF-WIDER method achieves the highest AP score on the WIDER FACE dataset for the Face Detection task. | WIDER_Face__Easy_ => 0.0\n",
      "Bootstrapped DQN is evaluated on a diverse selection of Atari games, including Amidar, Beam Rider, Battle Zone, Frostbite, and Montezuma’s Revenge, among others. It generally outperforms DQN in terms of learning speed and cumulative performance across most games. | Atari_2600_Montezuma_s_Revenge => 0.5\n",
      "MuZero achieves the highest score on the Atari_2600_Robotank dataset for the Atari_Games task. | Bootstrapped_DQN => 0.0\n",
      "The MemNNs ensemble method is evaluated on the bAbI dataset for text-based question answering tasks. | CNN___Daily_Mail => 0.0\n",
      "The Deep Speech method is evaluated on several datasets for the Speech Recognition task, including the TIMIT Acoustic-Phonetic Continuous Speech Corpus and LibriSpeech. These datasets are commonly used for evaluating automatic speech recognition systems. | Switchboard___Hub500 => 0.0\n",
      "The SRCNN method for Video Super-Resolution is evaluated on datasets such as Vid4 and the Ultra Video Group HD dataset. These datasets are commonly used for benchmarking video super-resolution techniques. | Vid4_-_4x_upscaling => 0.5\n",
      "The Spynet method for Optical Flow Estimation is evaluated on standard benchmarks such as the MPI-Sintel and KITTI datasets. | Sintel-final => 0.5\n",
      "The Inception_V2 method is evaluated on the ImageNet dataset for the Image Classification task using metrics such as top-1 and top-5 error rates. These metrics are standard for evaluating classification performance on ImageNet. | Top_1_Accuracy, Top_5_Accuracy => 1.0\n",
      "The VAT_EntMin method for semi-supervised image classification is evaluated on the MNIST, SVHN, and CIFAR-10 datasets. | CIFAR-10__4000_Labels => 0.0\n",
      "The 300D_NTI-SLSTM-LSTM_encoders method is evaluated on the Stanford Natural Language Inference (SNLI) dataset and the MultiGenre Natural Language Inference (MultiNLI) dataset for the Natural Language Inference task. | SNLI => 0.5\n",
      "The SRCNN method is evaluated on the Manga109 - 4x upscaling dataset for the Image Super-Resolution task using the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) metrics. | PSNR, SSIM => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [03:35<3:31:39, 215.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CRN method for the Image-to-Image Translation task does not have specific datasets mentioned in the available resources. Further research or specific papers might be needed to identify the datasets used for evaluation. | ADE20K-Outdoor_Labels-to-Photos => 0.0\n",
      "The DDQN__tuned__noop method achieves the highest Score score on the Atari 2600 Breakout dataset for the Atari_Games task. | Atari_2600_Video_Pinball => 0.0\n",
      "The IQN method achieves the highest Score score on the 57 Atari 2600 games dataset in the ALE, as demonstrated by improved performance in distributional reinforcement learning. | Atari_2600_Atlantis => 0.0\n"
     ]
    }
   ],
   "source": [
    "# iterative_monkey.thread_safe_evaluator(toolqa_test, optimized_actor_agent)\n",
    "batch_num = 4\n",
    "iterative_monkey.thread_safe_evaluator_batch(toolqa_test, optimized_actor_agent,batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stark11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
