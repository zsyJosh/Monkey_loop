{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following variables to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SERPER_API_KEY=''\n",
    "%env OPENAI_API_KEY=''\n",
    "HF_USR_NAME = ''\n",
    "TOOL_QA_ROOT = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "level = 'easy'\n",
    "dataset = 'scirex'\n",
    "\n",
    "dataset_dir = f'{dataset}-{level}.jsonl'\n",
    "hf_dataset_name = f'toolqa_{dataset}_{level}'\n",
    "\n",
    "df = pd.read_json(dataset_dir, lines=True)\n",
    "df.head()\n",
    "\n",
    "df['answer'] = df['answer'].apply(lambda x: str(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26de2f31cca4098a0afd9a406add96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d54c73862243eb8f3ae0e81d031fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/shirwu/toolqa_scirex_easy/commit/6856d5fcde7f21e44ca9e366cb0e172680a435f4', commit_message='Upload dataset', commit_description='', oid='6856d5fcde7f21e44ca9e366cb0e172680a435f4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict({'train': dataset})\n",
    "# push to hf for the ease for using dspy\n",
    "dataset_dict.push_to_hub(repo_id=hf_dataset_name, private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "* ToolQA\n",
    "\n",
    "Before loading our datasets and going to the execution part, we'll need to configure the `lm` in `dspy.settings`. For the purpose of this notebook we'll be using `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dspy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.OpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_tokens=4000,\n",
    "        temperature=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolQASignature(dspy.Signature):\n",
    "    \"\"\"You will be given a question. Your task is to answer the question with a short response. \n",
    "    \"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "        format=lambda x: x.strip(),\n",
    "    )\n",
    "    answer: str = dspy.OutputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"answer to the question\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from dspy.datasets import DataLoader\n",
    "\n",
    "dl = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb3e1144a7c4cd3ac71aaceba14aa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8751ed64d9a346d995c3aa11c46d1c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b13de4628d4a52a48c8be5536839e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_qa = dl.from_huggingface(\n",
    "    f'{HF_USR_NAME}/' + hf_dataset_name,\n",
    "    split=\"train\",\n",
    "    input_keys=(\"question\", \"answer\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tool_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set seed\n",
    "random.seed(42)\n",
    "\n",
    "train_idx = random.sample(range(len(tool_qa)), 40)\n",
    "remaining_idx = list(set(range(len(tool_qa))) - set(train_idx))\n",
    "test_idx = random.sample(remaining_idx, 60)\n",
    "\n",
    "toolqa_train = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in train_idx]\n",
    "]\n",
    "toolqa_test = [\n",
    "    dspy.Example(question=example.question, answer=example.answer).with_inputs(\"question\", \"paper_id\")\n",
    "    for example in [tool_qa[i] for i in test_idx]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Tools\n",
    "\n",
    "We'll setup `Avatar` modules for both signatures and all the `tools` can be used by each of the dataset. `Tool` is a pydantic model that Avatar expects the `tools` to be composed as more specifically it have 4 fields:\n",
    "\n",
    "* `name` : Name of the tool\n",
    "* `input_type` : Type of input the tool accepts\n",
    "* `output_type` : Type of output the tool returns\n",
    "* `tool` : The actual tool object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph : Sentence Level For representing a document , one can split it up into sentences , with each memory slot encoding one sentence . Both the key and the value encode the entire sentence as a bag - of - words . As the key and value are the same in this case , this is identical to a standard MemNN and this approach has been used in several papers .\n",
      "paragraph : Window Level Documents are split up into windows of words ; in our tasks we only include windows where the center word is an entity . Windows are represented using bag - of - words . Window representations for MemNNs have been shown to work well previously . However , in Key - Value MemNNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture . This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) . We will compare these approaches in our experiments .\n",
      "subsection : Transition System Given an input , most often a sentence , we define : A set of states . A special start state . A set of allowed decisions for all . A transition function returning a new state for any decision . We will use a function to compute the score of decision in state for input . The vector contains the model parameters and we assume that is differentiable with respect to . In this section , for brevity , we will drop the dependence of in the functions given above , simply writing , , , and . Throughout this work we will use transition systems in which all complete structures for the same input have the same number of decisions ( or for brevity ) . In dependency parsing for example , this is true for both the arc - standard and arc - eager transition systems , where for a sentence of length , the number of decisions for any complete parse is . A complete structure is then a sequence of decision / state pairs such that , for , and . We use the notation to refer to a decision sequence . We assume that there is a one - to - one mapping between decision sequences and states : that is , we essentially assume that a state encodes the entire history of decisions . Thus , each state can be reached by a unique decision sequence from . We will use decision sequences and states interchangeably : in a slight abuse of notation , we define to be equal to where is the state reached by the decision sequence . The scoring function can be defined in a number of ways . In this work , following chen - manning:2014:EMNLP , weiss - etAl:2015:ACL , and zhou - etAl:2015:ACL , we define it via a feed - forward neural network as Here are the parameters of the neural network , excluding the parameters at the final layer . are the final layer parameters for decision . is the representation for state computed by the neural network under parameters . Note that the score is linear in the parameters . We next describe how softmax - style normalization can be performed at the local or global level .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import sentence_transformers\n",
    "import chromadb\n",
    "from os import path as osp\n",
    "from chromadb.config import Settings\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "CHROMA_PERSIST_DIRECTORY = osp.join(TOOL_QA_ROOT, \"data/chroma_db/scirex-v2\")\n",
    "CHROMA_COLLECTION_NAME = \"all\"\n",
    "CHROMA_SERVER_HOST = \"localhost\"\n",
    "CHROMA_SERVER_HTTP_PORT = \"8000\"\n",
    "FILE_PATH = osp.join(TOOL_QA_ROOT, \"data/external_corpus/scirex/Preprocessed_Scirex.jsonl\")\n",
    "\n",
    "def sentence_embedding(model, texts):\n",
    "    embeddings = model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "def create_chroma_db(chroma_server_host, chroma_server_http_port, collection_name):\n",
    "    chroma_client = chromadb.Client(Settings(\n",
    "        chroma_api_impl=\"rest\",\n",
    "        chroma_server_host=chroma_server_host,\n",
    "        chroma_server_http_port=chroma_server_http_port,\n",
    "    ))\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def create_chroma_db_local(persist_directory, collection_name):\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    return collection\n",
    "\n",
    "def insert_to_db(texts, model_name, cuda_idx, db):\n",
    "    model = sentence_transformers.SentenceTransformer(model_name, device=f\"cuda:{cuda_idx}\")\n",
    "\n",
    "    batch_embeddings = []\n",
    "    batch_texts = []\n",
    "    start_time = time.time()\n",
    "    print(f\"Total Articles to process: {len(texts)}, Current Thread: {cuda_idx}.\")\n",
    "    for i, text in enumerate(texts):\n",
    "        # 2. generate embedding\n",
    "        embeddings = sentence_embedding(model, text).tolist()\n",
    "\n",
    "        batch_embeddings.append(embeddings)\n",
    "        batch_texts.append(text)\n",
    "        # 3. add to vectorstore per 500 articles or last article\n",
    "        if i % 100 == 0 or i == len(texts)-1:\n",
    "            batch_ids = [str(uuid.uuid1()) for _ in batch_texts]\n",
    "            db.add(\n",
    "                embeddings=batch_embeddings,\n",
    "                documents=batch_texts,\n",
    "                ids = batch_ids\n",
    "            )\n",
    "            batch_embeddings = []\n",
    "            batch_texts = []\n",
    "            print(f\"Completed Processing article count: {i}, Current Thread: {cuda_idx}, Time took: {time.time() - start_time}.\")\n",
    "    print(f\"Thread {cuda_idx} Completed. Total time took for thread: {time.time() - start_time}.\")\n",
    "\n",
    "\n",
    "# Multi-processing\n",
    "def query_llm(query, is_local=True, start=None, end=None):\n",
    "    cuda_idxes = [0]\n",
    "    number_of_processes = len(cuda_idxes)\n",
    "    input_texts = []\n",
    "    db = create_chroma_db_local(CHROMA_PERSIST_DIRECTORY, CHROMA_COLLECTION_NAME)\n",
    "    with open(FILE_PATH, 'r') as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            input_texts.append(item[\"content\"])\n",
    "    # input_texts = np.array_split(input_texts, number_of_processes)\n",
    "\n",
    "    args = ((input_texts[i], EMBED_MODEL_NAME, cuda_idxes[i], is_local) for i in range(number_of_processes))\n",
    "\n",
    "    # if there is no file under the directory \"/localscratch/yzhuang43/ra-llm/retrieval_benchmark/data/chroma_db/agenda\", insert the data into the db\n",
    "    # You should run insert_to_db the first time!\n",
    "    if len(os.listdir(CHROMA_PERSIST_DIRECTORY)) == 0:\n",
    "        insert_to_db(input_texts, model_name=EMBED_MODEL_NAME, cuda_idx=0, db=db)\n",
    "\n",
    "    input_paths = np.array_split(input_texts, number_of_processes)\n",
    "    with ProcessPoolExecutor(number_of_processes) as executor:\n",
    "        executor.map(insert_to_db, args)\n",
    "    model = sentence_transformers.SentenceTransformer(EMBED_MODEL_NAME, device=f\"cuda:0\")\n",
    "    query_embedding = sentence_embedding(model, query).tolist()\n",
    "    results = db.query(query_embeddings=query_embedding, n_results=3)\n",
    "    retrieval_content = [result for result in results['documents'][0]]\n",
    "    # print(retrieval_content)\n",
    "    retrieval_content = '\\n'.join(retrieval_content)\n",
    "    return retrieval_content\n",
    "\n",
    "query = \"What is an atom\"\n",
    "print(query_llm(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.avatar import Tool, Avatar\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper, ArxivAPIWrapper, WikipediaAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "def RETRIEVE(query: str) -> str:\n",
    "    \"\"\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE(\\'Which method achieves the highest PCK score?\\') returns relevant paper paragraph and meta data.\"\"\"\n",
    "    return query_llm(query)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        tool=StructuredTool.from_function(RETRIEVE),\n",
    "        name=\"RETRIEVE\",\n",
    "        desc=\"If you want to search for some paper information, you can use this tool and input a natural language query. For example, RETRIEVE('Which method achieves the highest PCK score?') returns relevant paper paragraph and meta data.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=GoogleSerperAPIWrapper(),\n",
    "        name=\"WEB_SEARCH\",\n",
    "        desc=\"If you have a question, you can use this tool to search the web for the answer.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        tool=ArxivAPIWrapper(),\n",
    "        name=\"ARXIV_SEARCH\",\n",
    "        desc=\"Pass the arxiv paper id to get the paper information.\",\n",
    "        input_type=\"Arxiv Paper ID\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined our `tools`, we can now create an `Avatar` object by passing the `tools` and `signature`. It takes 2 more optional parameters `verbose` and `max_iters`. `verbose` is used to display the logs and `max_iters` is used to control the number of iterations in multi step execution. \n",
    "\n",
    "An avatar agent stops the tool usage iteration once it reaches `max_iters` or when it prompts `Finish`. You can also create custom tools too, all you need to make sure is:\n",
    "\n",
    "* You pass is a class object.\n",
    "* Implements `__init__` and `run` method.\n",
    "* Must take 1 string a input and returns 1 string as output.\n",
    "\n",
    "If your tool doesn't return or takes input a string then you can make a custom wrapper to take care of that for now. In future we'll try to enable a diverse tool usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** Since DSPy 2.5.16+, TypedPredictors are now deprecated, underperform, and are about to be removed! ***\n",
      "Please use standard predictors, e.g. dspy.Predict and dspy.ChainOfThought.\n",
      "They now support type annotations and other features of TypedPredictors and tend to work much better out of the box.\n",
      "Please let us know if you face any issues: https://github.com/stanfordnlp/dspy/issues\n"
     ]
    }
   ],
   "source": [
    "actor_agent = Avatar(\n",
    "    tools=tools,\n",
    "    signature=ToolQASignature,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Open enden QA tasks are hard to evaluate on rigid metrics like exact match. So, we'll be using an improvised LLM as Judge for the evaluation of our model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client GPT3, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'question': 'What is the corresponding Medium_Human-Normalized_Score score of the Ape-X method on Atari-57 dataset for Atari_Games task?', 'answer': '434.1%'}) (input_keys={'paper_id', 'question'})\n",
      "physics | 434.1% => 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Evaluator(dspy.Signature):\n",
    "    \"\"\"Please act as an impartial judge to evaluate whether the answer is correct based on the ground truth answer\"\"\"\n",
    "    \n",
    "    question: str = dspy.InputField(\n",
    "        prefix=\"Question:\",\n",
    "        desc=\"question to ask\",\n",
    "    )\n",
    "    reference_answer: str = dspy.InputField(\n",
    "        prefix=\"Ground Truth Answer:\",\n",
    "        desc=\"Ground truth answer to the question.\",\n",
    "    )\n",
    "    answer: str = dspy.InputField(\n",
    "        prefix=\"Answer:\",\n",
    "        desc=\"Answer to the question given by the model.\",\n",
    "    )\n",
    "    rationale: str = dspy.OutputField(\n",
    "        prefix=\"Rationale:\",\n",
    "        desc=\"Explanation of why the answer is correct or incorrect.\",\n",
    "    )\n",
    "    is_correct: float = dspy.OutputField(\n",
    "        prefix=\"Correct:\",\n",
    "        desc=\"Whether the answer is correct. Give 0 if incorrect, 1 if correct, (0, 1) if partially correct.\",\n",
    "    )\n",
    "\n",
    "\n",
    "evaluator = dspy.TypedPredictor(Evaluator)\n",
    "\n",
    "\n",
    "def metric(example, prediction, trace=None):  \n",
    "    # We found sometimes the ground truth answers are incomplete or the answer\n",
    "    # is part of the ground truth answer. Therefore, for better comparison, \n",
    "    # we use a continuous value for the correct score   \n",
    "    acc = float(\n",
    "        evaluator(\n",
    "            question=example.question,\n",
    "            answer=prediction.answer,\n",
    "            reference_answer=example.answer\n",
    "        ).is_correct\n",
    "    ) \n",
    "    print(prediction.answer, '|', example.answer, '=>', acc)\n",
    "    return acc\n",
    "\n",
    "print(toolqa_train[0])\n",
    "metric(toolqa_train[0], prediction=dspy.Example(answer='physics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we can't use `dspy.Evaluate`, reason being that `Avatar` changes it's signature per iteration by adding the actions and it's results to it as fields. So we can create our own hacky thread safe evaluator for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_example(example, signature):\n",
    "    try:\n",
    "        avatar = Avatar(\n",
    "            signature,\n",
    "            tools=tools,\n",
    "            verbose=False,\n",
    "        )\n",
    "        prediction = avatar(**example.inputs().toDict())\n",
    "\n",
    "        return metric(example, prediction)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "# process_example(tool_qa[0], ToolQASignature)\n",
    "def multi_thread_executor(test_set, signature, num_threads=60):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(process_example, example, signature) for example in test_set]\n",
    "\n",
    "        for future in tqdm.tqdm(futures, total=total_examples, desc=\"Processing examples\"):\n",
    "            total_score += future.result()\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric\n",
    "\n",
    "def single_thread_executor(test_set, signature):\n",
    "    total_score = 0\n",
    "    total_examples = len(test_set)\n",
    "\n",
    "    for example in tqdm.tqdm(test_set, desc=\"Processing examples\"):\n",
    "        total_score += process_example(example, signature)\n",
    "\n",
    "    avg_metric = total_score / total_examples\n",
    "    return avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding accuracy score of the DeepId2 method on the Labeled Faces in the Wild dataset for the Face Verification task is 99.15%. | 99.15% => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:47<47:00, 47.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "The Percentage_correct score of the ResNet_ELU method on the CIFAR-100 dataset for the Image_Classification task is not readily available in the searched resources. | 73.5 => 0.0\n",
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The corresponding Accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n",
      "The Inception score of the BigGAN-deep method on the ImageNet_128x128 dataset for the Conditional Image Generation task is 124.5. | 166.5 => 0.0\n",
      "The F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.91. | 93.91 => 1.0\n",
      "The corresponding F1 score of the Neural-CRF_AE method on the CoNLL_2003 English dataset for the Named Entity Recognition (NER) task is 92.29. | 92.29 => 1.0\n",
      "The corresponding MAP score of the subCNN method on the PASCAL_VOC_2007 dataset for the Object Detection task is 68.5%. | 68.5% => 1.0\n",
      "The BLEU-2 score of the LeakGAN method on the Chinese Poems dataset for the Text Generation task is 0.456. | 0.881 => 0.0\n",
      "The corresponding Viewpoint_I_AEPE score of the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is 5.99. | 5.99 => 1.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task is not readily available from the sources searched. It may require further specific research or access to the original publication or dataset results where SliceNet was evaluated. | 26.1 => 0.0\n",
      "The Mean_IoU score of the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task is not readily available from the current search results. Further specific searches or access to detailed research papers might be required to find this information. | 82.0% => 0.0\n",
      "The corresponding average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 78.14. | 79.15 => 0.0\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is 0.84. | 0.88 => 0.0\n",
      "The corresponding Accuracy score of the MAML method on the OMNIGLOT 1-Shot Learning dataset for Few-Shot Image Classification task is 98.7%. | 98.7% => 1.0\n",
      "The corresponding Inception score of the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The corresponding Percentage error score of the Bi-LSTM with skip connections and CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n",
      "The corresponding Accuracy score of the Sample_Clustering method on the CUB-200 0-Shot Learning dataset for the Few-Shot Image Classification task is 44.3%. |  44.3% => 1.0\n",
      "The corresponding Aspect score of the LSTM-LOC method on the Sentihood dataset for the Aspect-Based Sentiment Analysis task is not explicitly found in the search results. However, the search results mention LSTM-LOC with scores 69.3 and 81.9, which might be related to its performance metrics. | 69.3 => 0.5\n",
      "The corresponding Viewpoint_II_AEPE score of the DeepMatching method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is not directly available from the retrieved sources. Further specific research or access to detailed experimental results from relevant publications or datasets may be required to obtain this information. | 4.63 => 0.0\n",
      "The Unigram_Acc score of the ASR method on the SearchQA dataset for the Open-Domain Question Answering task is not readily available in the searched resources. Further specific research or access to detailed experimental results from relevant papers may be required to obtain this information. | 41.3 => 0.0\n",
      "The F1 score for the Yang et al. method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task could not be found in the available resources. | 91.26 => 0.0\n",
      "The Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is 83.58%. | 84.14 => 0.0\n",
      "The MAE score of the Regularized_Deep_Regressor method on the UNBC-McMaster_ShoulderPain_dataset for the Pain_Intensity_Regression task is 0.389. | 0.389 => 1.0\n",
      "The corresponding AP score of the FDNet method on the WIDER_Face__Easy_ dataset for the Face Detection task is 95.9%. | 0.950 => 0.0\n",
      "The corresponding mIoU score of the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task is 66.63. | 66.63 => 1.0\n",
      "The accuracy score of the MEAN method on the MR dataset for the Sentiment Analysis task is 84.5%. | 84.5 => 1.0\n",
      "The Parameters score for the 200D decomposable attention model with intra-sentence attention on the SNLI dataset is 580k. | 580k => 1.0\n",
      "The Percentage_error score of the Microsoft_2016b method on the Switchboard___Hub500 dataset for the Speech_Recognition task is 6.2%. | 5.8 => 0.0\n",
      "The BLEU score of the Enc-Dec_Att__char_ method on the WMT2015 English-German dataset for the Machine Translation task is 23.5. | 23.45 => 0.5\n",
      "The Test_Accuracy score of the CBS-1___ESIM method on the SNLI dataset for the Natural_Language_Inference task is not directly available from the retrieved sources. Further specific research or access to the original paper or dataset might be required to obtain this information. | 86.73 => 0.0\n",
      "The Percentage_correct score of the MRN___global_features method on the COCO_Visual_Question_Answering__VQA__real_images_1_0_open_ended dataset for the Visual_Question_Answering task could not be found using the available tools. | 61.84 => 0.0\n",
      "The TAR___FAR_0_01 score of the Triplet_probabilistic_embedding method on the IJB-A dataset for the Face_Verification task is 90%. | 90% => 1.0\n",
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task is not available in the retrieved or searched resources. | 29.56 => 0.0\n",
      "The Mean_IoU score of the ResNet-38_MS_COCO method on the PASCAL_VOC_2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "The corresponding F1 score of the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task is not available in the retrieved or searched results. | 55.3% => 0.0\n",
      "The Test_perplexity score of the AWD-LSTM-DOC method on the Penn Treebank Word Level dataset for the Language Modelling task is not explicitly found in the retrieved documents or web search results. It may require access to specific research papers or datasets that report this score. | 52.38 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [01:54<57:11, 59.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Train Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is not explicitly found in the retrieved documents. However, based on the general information about ensemble methods and the SNLI dataset, it is likely to be a high accuracy score, potentially in the range of 89% to 95% as seen in similar models. | 94.8 => 0.0\n",
      "The corresponding MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object_Detection task is not directly found in the retrieved results. Further specific searches or access to the original research paper might be needed to obtain this information. | 76.2% => 0.0\n",
      "The accuracy score of the C-LSTM method on the SST-2 Binary classification dataset for the Sentiment Analysis task is not explicitly mentioned in the retrieved results. However, it is noted that the FC-GRU-CNN model is 14% higher in accuracy than the current best C-LSTM model, suggesting that the C-LSTM model's accuracy is lower than the FC-GRU-CNN model's reported accuracy. | 87.8 => 0.0\n",
      "The Checkerboards method achieves a Reasonable Miss Rate score of 11.87% on the Caltech dataset for the Pedestrian Detection task. | 17.1 => 0.0\n",
      "The Hits@1 score for the ComplEx method on the WN18 dataset for the Link Prediction task is not explicitly found in the retrieved results. However, based on the available information, the Hits@1 scores for similar models on WN18 are generally low, around 0.089, 0.035, and 0.096. It is recommended to refer to specific research papers or experimental results for the exact score. | 0.936 => 0.0\n",
      "The Params score of the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for the Language_Modelling task is not available in the retrieved or searched documents. | 22M => 0.0\n",
      "The Log_Loss score of the FNN method on the Criteo dataset for the Click-Through Rate Prediction task is not directly available from the retrieved sources. Further specific research or access to detailed experimental results from relevant studies would be required to obtain this information. | 0.45738 => 0.0\n",
      "The corresponding NLL_Test score of the Conv_DRAW method on the CIFAR-10 dataset for the Image_Generation task is -1791.15. | 3.58 => 0.0\n",
      "The F-Measure score for the DeepFlux method on the SK-LARGE dataset for Object Skeleton Detection is not explicitly mentioned in the retrieved documents. However, it is noted that DeepFlux consistently outperforms other methods on this dataset. | 0.732 => 0.0\n",
      "The corresponding AP score of the ACF-WIDER method on the WIDER_Face_Hard dataset for the Face Detection task is not explicitly available in the retrieved documents. Further specific research or access to the original paper or dataset results might be required to obtain this information. | 0.290 => 0.0\n",
      "The PSNR score of the JMPF_ method on the BSD100 - 4x upscaling dataset for the Image Super-Resolution task is not available in the retrieved documents or web search results. | 26.87 => 0.0\n",
      "The F-Measure score of the PSENet-1s method on the IC17-MLT dataset for the Scene Text Detection task is 72.45%. | 72.45% => 1.0\n",
      "The Class_IOU score for the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is not directly available from the retrieved sources. Further specific research or access to detailed datasets and papers might be required to obtain this information. |  0.08 => 0.0\n",
      "The Percentage_correct score of the ACN method on the CIFAR-100 dataset for the Image_Classification task is not directly available from the retrieved sources. Further specific searches or access to the original research paper detailing the ACN method might be necessary to obtain this information. | 66.3 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [02:09<24:47, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Number_of_params score of the mLSTM___dynamic_eval method on the Text8 dataset for the Language_Modelling task is 46 million parameters. | 45M => 0.0\n",
      "The AUC score for the R2U-Net method on the LUNA dataset for the Lung Nodule Segmentation task is 0.9419. | 0.9889 => 0.0\n",
      "The corresponding MAP score of the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly mentioned in the retrieved documents. Further specific details might be found in the original research papers or datasets. | 0.750 => 0.0\n",
      "The corresponding UAS score of the Arc-hybrid method on the Penn Treebank dataset for the Dependency Parsing task is 95.33%. | 93.56 => 0.0\n",
      "The corresponding Percentage_correct score of the Deep Networks with Internal Selective Attention through Feedback Connections method on the CIFAR-100 dataset for the Image Classification task is not explicitly mentioned in the available resources. The method is noted to outperform previous state-of-the-art models on the CIFAR-100 dataset, but the exact score is not provided. | 66.2 => 0.0\n",
      "The Validation_perplexity score of the AWD-LSTM-MoS + Partial Shuffle method on the Penn Treebank Word Level dataset for the Language Modelling task is 53.92. | 55.89 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  10%|█         | 6/60 [02:12<13:11, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Frame__fps_ score for the CRF-RNN method on the Cityscapes dataset for the Real-Time Semantic Segmentation task is not directly available from the retrieved sources. Further specific searches or access to the original CRF-RNN paper or related publications might be necessary to find this specific metric. | 1.4 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [02:12<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task was not found in the available resources. It might be necessary to consult specific research papers or datasets that directly address this method and dataset combination. | 57.14 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31666666666666665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teleprompter.thread_safe_evaluator(toolqa_test, actor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "For the optimization of the `Actor` we'll be using `AvatarOptimizer`. It's a DSPy implementation of the [Avatar](https://github.com/zou-group/avatar/) method that optimizes the `Actor` for the given `tools` using a comparator module that optimizes Actor instruction. Note, that Actor is the Module that directs tool execution and flow, it's not the signature that we are passing. It doesn't optimize the instruction of the signature we pass. It takes the following parameters:\n",
    "\n",
    "* `metric`: Metric that we'll be optimizing for\n",
    "* `max_iters`: Maximum number of iterations for the optimizer\n",
    "* `lower_bound`: Lower bound for the metric to classify example as negative\n",
    "* `upper_bound`: Upper bound for the metric to classify example as positive\n",
    "* `max_positive_inputs`: Maximum number of positive inputs sampled for comparator\n",
    "* `max_negative_inputs`: Maximum number of negative inputs sampled for comparator\n",
    "* `optimize_for`: Whether we want to maximize the metric or minimize it during optimization\n",
    "\n",
    "Once the optimizer is done we can get the optimized actor and use it for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import AvatarOptimizer\n",
    "\n",
    "teleprompter = AvatarOptimizer(\n",
    "    metric=metric,\n",
    "    max_iters=3,\n",
    "    max_negative_inputs=10,\n",
    "    max_positive_inputs=10,\n",
    "    lower_bound=0.5,\n",
    "    upper_bound=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_actor_agent = teleprompter.compile(\n",
    "    student=actor_agent,\n",
    "    trainset=toolqa_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our actor module, for this we've provided an implementation of thread safe evaluator that we above as part of class method of `AvatarOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   2%|▏         | 1/60 [00:11<11:11, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding Accuracy score of the BB8 method on the LineMOD dataset for the 6D Pose Estimation task is 89.3%. | 83.9% => 0.0\n",
      "The corresponding AP score of the ACF-WIDER method on the WIDER Face Hard dataset for the Face Detection task is 89.7. | 0.290 => 0.0\n",
      "The Inception score for the CEGAN-Ent-VI method on the CIFAR-10 dataset for the Image Generation task is 7.07. | 7.07 => 1.0\n",
      "The Reasonable Miss Rate score of the Checkerboards method on the Caltech dataset for the Pedestrian Detection task is 18.5%. | 17.1 => 0.0\n",
      "The corresponding Rank-1 score of the PDF method on the Market-1501 dataset for the Person Re-Identification task is 83.58%. | 84.14 => 0.0\n",
      "The corresponding Validation_perplexity score of the AWD-LSTM-MoS___Partial_Shuffle method on the Penn_Treebank__Word_Level_ dataset for the Language_Modelling task is 53.92. | 55.89 => 0.0\n",
      "The corresponding MAP score of the subCNN method on the PASCAL_VOC_2007 dataset for the Object Detection task is 68.5%. | 68.5% => 1.0\n",
      "The BLEU score of the ConvS2S method on the IWSLT2015 English-German dataset for the Machine Translation task is 26.73. | 26.73 => 1.0\n",
      "The BLEU-2 score of the LeakGAN method on the Chinese Poems dataset for the Text Generation task is 0.456. | 0.881 => 0.0\n",
      "The corresponding average score of the Multi-task tri-training method on the Multi-Domain Sentiment Dataset for the Sentiment Analysis task is 78.14. | 79.15 => 0.0\n",
      "The F1 score for the CHFusion method on the IEMOCAP dataset for the Multimodal Emotion Recognition task could not be found in the available resources. | 55.3% => 0.0\n",
      "The mLSTM dynamic eval method achieves a character-level cross-entropy of 1.19 bits/char on the Text8 dataset for the Language Modelling task. The Number_of_params score is not explicitly mentioned in the retrieved documents. | 45M => 0.0\n",
      "The corresponding Percentage error score of the Bi-LSTM with skip connections and CTC method on the TIMIT dataset for the Speech Recognition task is 17.7%. | 17.7 => 1.0\n",
      "The Inception Score of the BigGAN-deep method on the ImageNet 128x128 dataset for the Conditional Image Generation task is 166.5. | 166.5 => 1.0\n",
      "The Frame fps score for the CRF-RNN method on the Cityscapes dataset for the Real-Time Semantic Segmentation task could not be found in the available resources. | 1.4 => 0.0\n",
      "The corresponding Accuracy score of the DeepId2 method on the Labeled Faces in the Wild dataset for the Face Verification task is 99.15%. | 99.15% => 1.0\n",
      "The Sample_Clustering method has an accuracy score of 44.3% on the CUB-200 dataset for the Few-Shot Image Classification task. |  44.3% => 1.0\n",
      "The Hits@1 score of the ComplEx method on the WN18 dataset for the Link Prediction task is approximately 0.089. | 0.936 => 0.0\n",
      "The F1 score of the Massively Multilingual Sentence Embeddings method on the BUCC French-to-English dataset for the Cross-Lingual Bitext Mining task is 93.91. | 93.91 => 1.0\n",
      "The BLEU score for the Enc-Dec_Att__char_ method on the WMT2015 English-German dataset for the Machine Translation task is 23.5. | 23.45 => 0.5\n",
      "The AP score of the FDNet method on the WIDER Face Easy dataset for the Face Detection task is not available in the retrieved sources. | 0.950 => 0.0\n",
      "The corresponding Test_perplexity score of the AWD-LSTM-DOC method on the Penn Treebank Word Level dataset for the Language Modelling task is 52.8. | 52.38 => 0.5\n",
      "The 200D decomposable attention model with intra-sentence attention method on the SNLI dataset for the Natural Language Inference task has a Parameters score of 580k. | 580k => 1.0\n",
      "The mIoU score for the MultiObjectiveOptimization method on the Cityscapes dataset for the Multi-Task Learning task could not be found in the available resources. | 66.63 => 0.0\n",
      "The Microsoft 2016b method has a word error rate of 6.2% on the Switchboard task. | 5.8 => 0.0\n",
      "The Log_Loss score for the FNN method on the Criteo dataset for the Click-Through Rate Prediction task could not be found in the available resources. | 0.45738 => 0.0\n",
      "The BLEU score for the DCCL method on the IWSLT2015 German-English dataset for the Machine Translation task could not be found in the available resources. | 29.56 => 0.0\n",
      "The MAML method achieves an accuracy of approximately 98.7% on the OMNIGLOT dataset for the 1-Shot, 5-way Few-Shot Image Classification task. | 98.7% => 1.0\n",
      "The corresponding accuracy score of the MEAN method on the MR dataset for the Sentiment Analysis task is 84.5%. | 84.5 => 1.0\n",
      "The corresponding Accuracy score of the Planetoid method on the Cora dataset for the Document Classification task is 75.7%. | 75.7% => 1.0\n",
      "The corresponding Test Accuracy score of the CBS-1 ESIM method on the SNLI dataset for the Natural Language Inference task is 86.73%. | 86.73 => 1.0\n",
      "The Unigram_Acc score for the ASR method on the SearchQA dataset for the Open-Domain Question Answering task is not readily available from the current search results. | 41.3 => 0.0\n",
      "The Params score for the Past_Decode_Reg____AWD-LSTM-MoS___dyn__eval_ method on the Penn_Treebank__Word_Level_ dataset for Language_Modelling is not explicitly mentioned in the retrieved documents. However, related methods achieve a word level perplexity of 53.8 on the Penn Treebank dataset. | 22M => 0.0\n",
      "Unable to find the specific Mean_IoU score for the Mapillary method on the Cityscapes dataset for the Semantic Segmentation task. Further detailed search or specific paper references might be needed. | 82.0% => 0.0\n",
      "The corresponding Accuracy score of the C-LSTM method on the SST-2 Binary classification dataset for the Sentiment Analysis task is 87.8%. | 87.8 => 1.0\n",
      "The corresponding Dice Score of the InputCascadeCNN method on the BRATS-2013 dataset for the Brain Tumor Segmentation task is 0.84. | 0.88 => 0.0\n",
      "The PSNR score for the JMPF_ method on the BSD100 dataset for 4x upscaling in the Image Super-Resolution task could not be found using the available tools. | 26.87 => 0.0\n",
      "The Percentage_correct score of the MRN global features method on the COCO Visual Question Answering (VQA) real images 1.0 open-ended dataset is 66.3%. | 61.84 => 0.0\n",
      "The MAE score of the Regularized Deep Regressor method on the UNBC-McMaster Shoulder Pain dataset for the Pain Intensity Regression task is 0.389. | 0.389 => 1.0\n",
      "The Viewpoint_II_AEPE score for the DeepMatching method on the HPatches dataset for the Dense Pixel Correspondence Estimation task could not be found in the available resources. | 4.63 => 0.0\n",
      "The corresponding MAP score of the I_ORE method on the PASCAL_VOC_2007 dataset for the Object Detection task is not explicitly found in the search results. Further investigation or specific sources may be required to obtain this information. | 76.2% => 0.0\n",
      "The BLEU score for the SliceNet method on the WMT2014 English-German dataset for the Machine Translation task is 26.1. | 26.1 => 1.0\n",
      "The Class_IOU score of the CoGAN method on the Cityscapes Photo-to-Labels dataset for the Image-to-Image Translation task is 45%. |  0.08 => 0.0\n",
      "The NLL_Test score for the Conv_DRAW method on the CIFAR-10 dataset for the Image Generation task could not be found in the available resources. | 3.58 => 0.0\n",
      "The Viewpoint_I_AEPE score of the FlowNet2 method on the HPatches dataset for the Dense Pixel Correspondence Estimation task is 5.99. | 5.99 => 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   3%|▎         | 2/60 [00:27<13:47, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Train Accuracy score of the 450D_DR-BiLSTM_Ensemble method on the SNLI dataset for the Natural Language Inference task is 94.8%. | 94.8 => 1.0\n",
      "The UAS score for the Arc-hybrid method on the Penn Treebank dataset for the Dependency Parsing task was not found in the search results. | 93.56 => 0.0\n",
      "The Percentage_correct score of the ResNet_ELU method on the CIFAR-100 dataset for the Image_Classification task could not be found using the available tools. | 73.5 => 0.0\n",
      "The TAR at FAR=0.01 score for the Triplet probabilistic embedding method on the IJB-A dataset for the Face Verification task is not explicitly available in the retrieved documents. Further detailed search or access to specific datasets or publications may be required to obtain this information. | 90% => 0.0\n",
      "The corresponding Mean IoU score of the ResNet-38_MS_COCO method on the PASCAL_VOC_2012 dataset for the Semantic Segmentation task is 84.9%. | 84.9% => 1.0\n",
      "I was unable to find the Percentage_correct score for the ACN method on the CIFAR-100 dataset for the Image Classification task. | 66.3 => 0.0\n",
      "Unable to find the specific F-Measure score for PSENet-1s on the IC17-MLT dataset for Scene Text Detection. Consider checking the original paper or related publications for detailed results. | 72.45% => 0.0\n",
      "Unable to find the specific accuracy score for the RetinaNet_Augmented_Autoencoders_ICP method on the T-LESS dataset for the 6D Pose Estimation task. Further detailed search or direct source consultation may be required. | 57.14 => 0.0\n",
      "The F1 score for the Neural-CRF_AE method on the CoNLL 2003 English dataset for the Named Entity Recognition (NER) task could not be found in the available resources. | 92.29 => 0.0\n",
      "The Percentage_correct score for the Deep Networks with Internal Selective Attention through Feedback Connections method on the CIFAR-100 dataset for the Image Classification task is not explicitly found in the available resources. However, the method is noted for outperforming previous state-of-the-art models on CIFAR-100. | 66.2 => 0.0\n",
      "Unable to find the specific F1 score for the Yang et al. method on the CoNLL 2003 English NER dataset. Consider checking the original paper or related publications for detailed results. | 91.26 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:   7%|▋         | 4/60 [00:42<09:25, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for the R2U-Net method on the LUNA dataset for the Lung Nodule Segmentation task could not be found in the available resources. | 0.9889 => 0.0\n",
      "The MAP score for the aNMM method on the TrecQA dataset for the Question Answering task is not explicitly found in the search results. Further detailed search or access to specific academic papers or datasets might be required to obtain this information. | 0.750 => 0.0\n",
      "The corresponding Aspect score of the LSTM-LOC method on the Sentihood dataset for the Aspect-Based Sentiment Analysis task is not explicitly found in the search results. However, the search results frequently mention LSTM-LOC in the context of the Sentihood dataset, suggesting it is a known method used in this area. Further detailed exploration of specific academic papers or datasets might be required to find the exact score. | 69.3 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 60/60 [00:45<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F-Measure score for the DeepFlux method on the SK-LARGE dataset for Object Skeleton Detection is not explicitly found in the search results. The search results consistently mention that DeepFlux outperforms other methods using the VGG16 backbone, but the exact F-Measure score is not provided in the available data. | 0.732 => 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teleprompter.thread_safe_evaluator(toolqa_test, optimized_actor_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy-74wouE_3-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
